<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - 陈汝丹</title>
    <description>陈汝丹 - 个人博客</description>
    <link>chenrudan.github.io</link>
    <atom:link href="chenrudan.github.io/page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sun, 19 Jun 2016 16:49:32 +0800</pubDate>
    <lastBuildDate>Sun, 19 Jun 2016 16:49:32 +0800</lastBuildDate>
    <generator>陈汝丹</generator>
    
      <item>
        <title>【David Silver强化学习公开课之三】动态规划解决MDP的Planning问题</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是David Silver强化学习公开课第三课的总结笔记。主要谈到了动态规划能够解决MDP的什么问题，如何通过Policy Iteration和Value Iteration来解决，以及这两者指的是什么，出于什么样的考虑提出这两种思路，具体解决步骤是什么。&lt;/p&gt;

&lt;p&gt;本课视频地址:&lt;a href=&quot;https://www.youtube.com/watch?v=lfHX2hHRMVQ&amp;amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa&amp;amp;index=2&quot;&gt;RL Course by David Silver - Lecture 3: Planning by Dynamic Programming&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;本课ppt地址:&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf&quot;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;文章的内容是课程的一个总结和讨论，会按照自己的理解来组织。个人知识不足再加上英语听力不是那么好可能会有一些理解不准的地方，欢迎一起讨论。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;1.内容回顾&lt;/h4&gt;

&lt;p&gt;前两节课我忽略了一些内容，这节课用到了，所以先回顾一下。首先是Planning的概念，在第一节课提到过强化学习是一种Sequential Decision Making问题，它是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的，因而agent需要从不断尝试的经验中发现一个好的policy。而Planning也属于Sequential Decision Making问题，不同的是它的environment是已知的，例如游戏的规则是已知的，所以agent不需要通过与environment的交互来获取下一个状态，而是知道自己执行某个action之后状态是什么，再优化自己的policy。因此这两者之间是有联系的，假如强化学习学习出来environment的模型，知道了environment是如何work的，强化学习要解决的问题就是Planning了。&lt;/p&gt;

&lt;p&gt;在第二课中，推导出了几个Bellman方程，回归一下。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Bellman方程&lt;/td&gt;
      &lt;td&gt;形式&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bellman Expectation Equation&lt;/td&gt;
      &lt;td&gt;$v_{\pi}(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s,a) =\sum_{a\in A}\pi(a|s)(R_s^a + \gamma \sum_{s’\in S}P_{ss’}^av_{\pi}(s’))$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bellman Expectation Equation&lt;/td&gt;
      &lt;td&gt;$q_{\pi}(s,a) = R_s^{a} + \gamma \sum_{s’ \in S}P_{ss’}^av_\pi(s’)=R_s^a + \gamma \sum_{s’\in S}P_{ss’}^a\sum_{a’\in A}\pi(a’|s’)q_{\pi}(s’,a’)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bellman Optimality Equation&lt;/td&gt;
      &lt;td&gt;$v_*(s) = \underset{a}{max}q_*(s,a) = \underset{a}{max} R_s^{a} + \gamma \sum_{s’ \in S}P_{ss’}^av_*(s’)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Bellman Optimality Equation&lt;/td&gt;
      &lt;td&gt;$q_*(s,a) = R_s^{a} + \gamma \sum_{s’ \in S}P_{ss’}^av_*(s’) = R_s^a + \gamma \sum_{s’\in S}P_{ss’}^a \underset{a’}    {max}q_*(s’, a’)$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;planning&quot;&gt;2. 动态规划与Planning&lt;/h4&gt;

&lt;p&gt;动态规划是这样一种方法，它将一个复杂的问题切分成一系列简单的子问题，一旦解决了这些简单的子问题，再将这些子问题的解结合起来变成复杂问题的解，同时将它们的解保存起来，如果下一次遇到了相同的子问题那么就不用再重新计算子问题的解[1]。其中“动态”是指某个问题是由序列化状态组成，状态是step-by-step的改变，从而可以step-by-step的来解这个问题，“规划”即优化Policy。而MDP有Bellman方程能够被递归的切分成子问题，同时它有值函数，保存了每一个子问题的解，因此它能通过动态规划来求解。针对MDP，切分成的子问题就是在每个状态下应该选择的action是什么，MDP的子问题是以一种递归的方式存在，这一时刻的子问题取决于上一时刻的子问题选择了哪个action。&lt;/p&gt;

&lt;p&gt;当已知MDP的状态转移矩阵时，environment的模型就已知了，此时可以看成Planning问题，动态规划则是用来解决MDP的Planning问题，主要解决途径有两种，Policy Iteration和Value Iteration。&lt;/p&gt;

&lt;h4 id=&quot;policy-iteration&quot;&gt;3. Policy Iteration&lt;/h4&gt;

&lt;p&gt;这个解决途径主要分为两步，示意图见图1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Policy Evaluation:基于当前的Policy计算出每个状态的value function&lt;/li&gt;
  &lt;li&gt;Policy Improvment:基于当前的value function，采用贪心算法来找到当前最优的Policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl3_1.png&quot; alt=&quot;1&quot; height=&quot;15%&quot; width=&quot;15%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 Policy Iteration过程图(图片来源[2])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;这里的Policy Evaluation要求的$v(s)$是通过计算第一个Bellman Expectation Equation得到的。下图是一个叫Small Gridworld的例子，左上角和右下角是终点，$\gamma = 1$，移动一步reward减少1，起始的random policy是朝每个能走的方向概率相同，先单独看左边一列，它表示在第k次迭代每个state上value function的值，这一列始终采用了random policy，这里的value function就是通过Bellman Expectation Equation得到的，考虑k=2的情况，-1.7 = -1.0 + 2*(1/3.0)*(-1)，-2.0 = -1.0 + 4*(1/4.0)*(-1)。而右边一列就是在当前的value function情况下通过greedy算法找到当前朝哪个方向走比较好。&lt;/p&gt;

&lt;p&gt;Policy Iteration会一直迭代到收敛，具体证明过程可以去看视频(46:09起)。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl3_2.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 Policy Iteration实例(图片来源[2])&lt;/center&gt;&lt;/font&gt;

&lt;h4 id=&quot;value-iteration&quot;&gt;3. Value Iteration&lt;/h4&gt;

&lt;p&gt;最优化原理:当且仅当任何从s能达到的$s’$能在当前policy下获取最优的value即$v_{\pi}(s’) = v_{*}(s’)$，那么状态s也能在当前policy下获得最优value$v_{\pi}(s) = v_{*}(s)$。&lt;/p&gt;

&lt;p&gt;从上面原理出发，如果已知子问题的最优值$v_{*}s’$，那么就能通过第一个Bellman Optimality Equation将$v_{*}(s)$也推出来。因此从终点开始向起点推就能把全部状态最优值推出来。Value Iteration通过迭代的方法，通过这一步的$v_k(s’)$更新下一步的$v_{k+1}(s)$不断迭代，最终收敛到最优的$v_{*}$，需要注意的是中间生成的value function的值不对应着任何policy。&lt;/p&gt;

&lt;p&gt;考虑下面这个Shortest Path例子，左上角是终点，要求的是剩下每一个格子距离终点的最短距离，每走一步，reward减少1，根据Value Iteration计算Bellman Optimality Equation就能得到后面的每一个正方形格子取值。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl3_3.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图3 Value Iteration实例(图片来源[2])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;以上是本课主要内容，后面还提了一下异步动态规划，并且提到了以上动态规划的解法适合状态个数百万级别的问题。总之，这节课需要搞懂的问题是动态规划能解决什么样的MDP问题以及具体怎样通过Policy Iteration和Value Iteration做到的。&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming&quot;&gt;Dynamic programming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf&quot;&gt;Planning by Dynamic Programming&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Fri, 17 Jun 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/06/17/reinforcementlearninglesssion3.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/06/17/reinforcementlearninglesssion3.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【David Silver强化学习公开课之二】马尔可夫决策过程MDP</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是David Silver强化学习公开课第二课的总结笔记。主要介绍了马尔可夫过程(MP)、马尔可夫奖赏过程(MRP)、马尔可夫决策过程(MDP)是什么，以及它们涉及到的一些概念，结合了课程ppt给出的例子对概念有了一些直观的了解。&lt;/p&gt;

&lt;p&gt;本课视频地址:&lt;a href=&quot;https://www.youtube.com/watch?v=lfHX2hHRMVQ&amp;amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa&amp;amp;index=2&quot;&gt;RL Course by David Silver - Lecture 2: Markov Decision Process&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;本课ppt地址:&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf&quot;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;文章的内容是课程的一个总结和讨论，会按照自己的理解来组织。个人知识不足再加上英语听力不是那么好可能会有一些理解不准的地方，欢迎一起讨论。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;1.基础概念&lt;/h4&gt;

&lt;p&gt;状态集合S: 有限状态state集合，s表示某个特定状态&lt;/p&gt;

&lt;p&gt;动作集合A: 有限动作action集合，a表示某个特定动作&lt;/p&gt;

&lt;p&gt;状态转移矩阵P: 矩阵每一项是从S中一个状态s转移到另一个状态{s’}的概率$P_{ss’} = P[S_{t+1}=s’|S_t=s]$以及执行动作a后从一个状态转移到另一个概率为$P_{ss’}^a = P[S_{t+1}=s’ | S_t=s, A_t=a]$。这里的状态转移矩阵决定了马尔可夫性质，即未来状态只与当前状态有关而与过去状态无关。矩阵一行之和为1。&lt;/p&gt;

&lt;p&gt;策略$\pi$: 状态s下执行动作a的概率，$\pi(a|s) = P[A_t=a|S_t=s]$&lt;/p&gt;

&lt;p&gt;reward函数ER: 这个函数是immediate reward的期望，即在时刻t的时候，agent执行某个action后下一个时刻立即能得到的reward $R_{t+1}$的期望，它由当前的状态决定。状态s下immediate reward期望为$ER_s = E[R_{t+1}|S_t = s]$，状态s下执行动作a后immediate reward期望为$ER_s^a = E[R_{t+1}|S_t=s, A_t=a]$&lt;/p&gt;

&lt;p&gt;Return $G_t$与discount $\gamma$: $G_t$是t时刻之后未来执行一组action能够获得的reward，即t+1、t+2、t+3…未来所有时刻reward之和，是未来时刻reward在当前时刻的体现，但是越往后的时刻它能反馈回来的reward需要乘以一个discount系数，系数$\gamma \in [0,1]$会产生一个打折的效果，这是因为并没有一个完美的模型能拟合出未来会发生什么，未来具有不确定性，同时这样计算会方便，避免了产生状态的无限循环，在某些情况下，即时产生的reward即$R_{t+1}$会比未来时刻更值得关注，符合人的直觉。因此$G_t = R_{t+1}+\gamma R_{t+2}+…=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}$&lt;/p&gt;

&lt;p&gt;状态值函数$v(s)$: 即基于t时刻的状态s能获得的return的期望，$v(s)=E[G_t|S_t=s]$，这里是仅按照状态转移矩阵选择执行何种动作，如果加入动作选择策略，那么函数就变成了$v_{\pi}(s) = E_{\pi}[G_t|S_t=s]$&lt;/p&gt;

&lt;p&gt;动作值函数$q_{\pi}(s,a)$: 基于t时刻的状态s，选择特定的一个action后能获得的return期望，这里的选择过程就隐含加入了策略。$q_{\pi}(s,a)= E_{\pi}[G_t|S_t=s, A_t=a]$&lt;/p&gt;

&lt;h4 id=&quot;mdp&quot;&gt;2. MDP与实例分析&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;马尔可夫链/过程(Markov Chain/Process)&lt;/strong&gt;，是具有markov性质的随机状态s1,s2,…序列。由$[S,P]$组成。如下图1圆圈内是状态，箭头上的值是状态之间的转移概率。class是指上第几堂课，facebook指看facebook网页，pub指去酒吧，pass指通过考试，sleep指睡觉。例如处于class1有0.5的概率转移到class2，或者0.5的概率转移到facebook。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl2_1.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 Markov Process Example(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;从而可以产生非常多的随机序列，例如C1 C2 C3 Pass Sleep或者C1 FB FB C1 C2 C3 Pub C1 FB FB FB C1 C2 C3 Pub C2 Sleep等。这些随机状态的序列就是马尔可夫过程。这里可以看到有一些状态发生了循环。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;马尔可夫奖赏过程(Markov Reward Process)&lt;/strong&gt;，即马尔可夫过程加上value judgement，value judegment即判断一个像上面一个特定的随机序列有多少累积reward，也就是计算出$v(s)$。它由$[S,P,R,\gamma]$组成，示意图如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl2_2.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 Markov Reward Process Example(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;可以看出比图1多了红色部分即R，但是R的取值只决定了immediate reward，在实际过程中肯定是需要考虑到后面步骤的reward才能确定当前的选择是否正确。而实际上$v(s)$由两部分组成，一个是immediate reward，一个是后续状态产生的discounted reward，推导如下(这里我觉得视频里似乎把$ER_s$与$R_{t+1}$的取值当成一样的了)，推导出来的这个式子称为Bellman方程。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{split}
v(s) = &amp;E[G_t|S_t=s] \\
       = &amp;E[R_{t+1} + \gamma (R_{t+2}+\gamma R_{t+3}+...) | S_t=s] \\
        = &amp;E[R_{t+1} + \gamma G_{t+1} | S_t=s] \\
        = &amp;ER_s + \gamma \sum_{s&#39;\in S}P_{ss&#39;}v(s&#39;)
\end{split} %]]&gt;&lt;/script&gt;

&lt;p&gt;那么每一个状态下能得到的状态值函数取值或者说累积reward如下所示，即原来写着class、sleep状态的地方替换成了数字(这里假设$\gamma = 1$)。可以从sleep状态出发，推导出每个状态的状态值函数取值，如右上角红色公式所示。最右的-23与-13，列出二元一次方程组即可求出。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl2_3.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图3 State Value Function Example(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;将Bellman方程表达成矩阵形式，变成了$v = ER + \gamma Pv$，是个线性等式，直接求解得到$v = (I - \gamma P)^{-1}ER$。而这样求解的话计算复杂度是$O(n^3)$，所以一般通过动态规划、蒙特卡洛估计与Temporal-Difference learning这些迭代的方式求解。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;马尔可夫决策过程(Markov Decision Process)&lt;/strong&gt;，它是拥有决策能力的马尔可夫奖赏过程，个人理解是MRP是将所有情况都遍历，而MDP则是选择性的遍历某些情况。它由$[S,A,P,R_s^a,\gamma,\pi(a|s)]$组成，并且拥有两个值函数$v_{\pi(s)}$和$q_{\pi}(s,a)$。根据这两个值函数的定义，它们之间的关系表示为$v_{\pi}(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s,a)$以及$q_{\pi}(s,a) = R_s^a + \gamma \sum_{s’\in S}P_{ss’}^av_{\pi}(s’)$。第二个式子是说当选择一个action之后，转移到不同状态下之后获取的reward之和是多少。将两个式子互相代入，可以得到如下的Bellman期望方程。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_{\pi}(s) = \sum_{a\in A}\pi(a|s)(R_s^a + \gamma \sum_{s&#39;\in S}P_{ss&#39;}^av_{\pi}(s&#39;))\\
        q_{\pi}(s,a) = R_s^a + \gamma \sum_{s&#39;\in S}P_{ss&#39;}^a\sum_{a&#39;\in A}\pi(a&#39;|s&#39;)q_{\pi}(s&#39;,a&#39;)&lt;/script&gt;

&lt;p&gt;下图是一个MDP的例子，箭头上的单词表示action，与MRP不同的是，这里给出的immediate reward是同时在某个状态s和某个动作a条件下，所以图中R不是只取决于s，而是取决于s和a。右上角的等式表达出了这一个状态的状态值函数求解过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl2_4.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图4 Markov Decision Processes Example(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;由于策略$\pi(a|s)$是可以改变的，因此两个值函数的取值不像MRP一样是固定的，那么就能从不同的取值中找到一个最大值即最优值函数(这节课没有讲如何求解)。例如下面两个图就是上面的例子能找到的最优状态值函数$v_*(s)$与最优动作值函数$q_*(s,a)$。如果知道了$q_*(s,a)$，那么也就知道了在每一步选择过程中应该选择什么样的动作。也就是说MDP需要解决的问题并不是每一步到底会获得多少累积reward，而是找到一个最优的解决方案。这两个最优值函数同样存在着一定关系，$v_*(s) = \underset{a}{max}q_*(s,a)$，从而可以推出$q_*(s,a) = R_s^a + \gamma \sum_{s’\in S}P_{ss’}^a \underset{a’}{max}q_*(s’, a’)$，这个等式称为Bellman优化方程，它不是一个线性等式，没有闭式解。通常通过值迭代、策略迭代、Q-learning、Sarsa等方法求解。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl2_5.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图5 Optimal State-Value Function Example(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl2_6.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图6 Optimal Action-Value Function Example(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;实际上一定存在这样的一个最优策略$\pi_*$。可以通过最大化$q_*(s,a)$获得。针对任意的MDP问题，总是存在一个最优的deterministic policy。&lt;/p&gt;

&lt;p&gt;以上是第二课的主要内容，从MP开始到MRP再到MDP，了解值函数的具体概念与reward有什么联系，重点是介绍MDP面对的问题，暂时没有提到如何解决。结合课程的例子对各个概念有比较直观的了解。但是这一课有个概念是“MDP描述了强化学习的environment，且是fully Observable的”，这个意思我暂时没明白。如果MDP是environment，那么它为什么有policy，为什么需要最大化policy，个人觉得MDP应该是fully Observable的强化学习，也就是说MDP本身描述的就是一个强化学习问题，它的状态转移矩阵和reward function就是environment，它的目标就是找到最优的动作值函数或者最优Policy。&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf&quot;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MDP.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Jun 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/06/12/reinforcementlearninglesssion2.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/06/12/reinforcementlearninglesssion2.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【David Silver强化学习公开课之一】强化学习入门</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是David Silver强化学习公开课第一课的总结笔记。第一课主要解释了强化学习在多领域的体现，主要解决什么问题，与监督学习算法的区别，完整的算法流程由哪几部分组成，其中的agent又包含什么内容，以及解释了强化学习涉及到的一些概念。&lt;/p&gt;

&lt;p&gt;本课视频地址:&lt;a href=&quot;https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;index=1&amp;amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa&quot;&gt;RL Course by David Silver - Lecture 1: Introduction to Reinforcement Learning&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;本课ppt地址:&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf&quot;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;文章的内容是课程的一个总结和讨论，会按照自己的理解来组织。个人知识不足再加上英语听力不是那么好可能会有一些理解不准的地方，欢迎一起讨论。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;1. 强化学习是什么&lt;/h4&gt;

&lt;p&gt;强化学习是多学科多领域交叉的一个产物，它的本质就是解决“decision making”问题，即学会自动进行决策。在computer science领域体现为机器学习算法。在Engineering领域体现在决定the sequence of actions来得到最好的结果。在Neuroscience领域体现在理解人类大脑如何做出决策，主要的研究是reward system。在Psychology领域，研究动物如何做出决策，动物的行为是由什么导致的。在Economics领域体现在博弈论的研究。这所有的问题最终都归结为一个问题，人为什么能够并且如何做出最优决策。&lt;/p&gt;

&lt;p&gt;强化学习是一个Sequential Decision Making问题，它需要连续选择一些行为，从而这些行为完成后得到最大的收益最好的结果。它在没有任何label告诉算法应该怎么做的情况下，通过先尝试做出一些行为得到一个结果，通过判断这个结果是对还是错来对之前的行为进行反馈，然后由这个反馈来调整之前的行为，通过不断的调整，算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果。&lt;/p&gt;

&lt;p&gt;强化学习与监督学习有着不少区别，首先监督学习是有一个label的，这个label告诉算法什么样的输入对应着什么样的输出，而强化学习没有label告诉它在某种情况下应该做出什么样的行为，只有一个做出一系列行为后最终反馈回来的reward signal，这个signal能判断当前选择的行为是好是坏。其次强化学习的结果反馈有延时，有时候可能需要走了很多步以后才知道以前的某一步的选择是好还是坏，而监督学习做了比较坏的选择会立刻反馈给算法。强化学习面对的输入总是在变化，输入不像监督学习是独立同分布的。而每当算法做出一个行为，它影响了下一次决策的输入。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;2. 强化学习组成&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl_1_1.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 强化学习组成部分(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;强化学习决策流程见上图。需要构造出一个agent(图中的大脑部分)，agent能够执行某个action，例如决定机器人超哪个方向走，围棋棋子下在哪个位置。agent能够接收当前环境的一个observation，例如当前机器人的摄像头拍摄到场景。agent还能接收当它执行某个action后的reward，即在第t步agent的工作流程是执行一个动作$A_t$，获得该动作之后的环境观测状况$O_t$，以及获得这个动作的反馈奖赏$R_t$。而环境environment则是agent交互的对象，它是一个行为不可控制的对象，agent一开始不知道环境会对不同action做出什么样的反应，而环境会通过observation告诉agent当前的环境状态，同时环境能够根据可能的最终结果反馈给agent一个reward，例如围棋棋面就是一个environment，它可以根据当前的棋面状况估计一下黑白双方输赢的比例。因而在第t步，environment的工作流程是接收一个$A_t$，对这个动作做出反应之后传递环境状况和评估的reward给agent。reward奖赏$R_t$，是一个反馈标量值，它表明了在第t步agent做出的决策有多好或者有多不好，整个强化学习优化的目标就是最大化累积reward。例如在射击游戏中，击中敌方的一架飞机，最后的得分会增加，那么这一步的reward就是正值。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;3. 一些变量&lt;/h4&gt;

&lt;p&gt;history是所有动作、状态、奖赏的序列，$H_t=A_1, O_1, R_1,…,A_t, O_t,R_t$&lt;/p&gt;

&lt;p&gt;environment state，$S_t^e$，环境当前的状态，它反应了环境发生什么改变。这里需要明白的一点是环境自身的状态和环境反馈给agent的状态并不一定是相同的，例如机器人在走路时，当前的environment状态是一个确定的位置，但是它的camera只能拍到周围的景象，无法告诉agent具体的位置，而拍摄到的照片可以认为是对环境的一个observation，也就是说agent并不是总能知道环境是如何发生改变的，只能看到改变后的一个结果展示。&lt;/p&gt;

&lt;p&gt;agent state，$S_t^a$，是agent的现在所处状态的表示，它可以是history的任何函数。&lt;/p&gt;

&lt;p&gt;information(Markov) state，它包含了history的所有有用信息。一个状态$S_t$有马尔可夫性质是指下一个时刻的状态仅由当前状态决定，与过去状态无关。这里定义可以看出environment state是有马尔可夫性质的(这个概念不明白可以暂时不管)。&lt;/p&gt;

&lt;p&gt;如果说environment是Fully Observable的，那么就是说agent能够直接看到环境当前的状态，在这种情况下agent state与environment state是相等的。而如果说environment是Partially Observable Environments，那么就是上面机器人的那个例子，agent能获取到的不是直接的环境状态。&lt;/p&gt;

&lt;h4 id=&quot;agent&quot;&gt;4. Agent的组成&lt;/h4&gt;

&lt;p&gt;一个agent由三部分组成Policy、Value function、Model，但这三部分不是必须同时存在的。&lt;/p&gt;

&lt;p&gt;Policy，它根据当前看到的observation来决定action，是从state到action的映射。有两种表达形式，一种是Deterministic policy即$a=\pi(s)$，在某种状态s下，一定会执行某个动作a。一种是Stochastic policy即$\pi(a|s)=p[A_t = a | S_t=s]$，它是在某种状态下执行某个动作的概率。&lt;/p&gt;

&lt;p&gt;Value function，它预测了当前状态下未来可能获得的reward的期望。$V_{\pi}(s)=E_{\pi}[R_{t+1}+rR_{t+2}+…|S_t=s]$。用于衡量当前状态的好坏。&lt;/p&gt;

&lt;p&gt;Model，预测environment下一步会做出什么样的改变，从而预测agent接收到的状态或者reward是什么。因而有两种类型的model，一种是预测下一个state的transition model即$P_{ss’}^a=p[S_{t+1}=s’|S_t = s,A_t=a]$，一种是预测下一次reward的reward model即$R_s^a=E[R_{t+1}|S_t=s, A_t=a]$&lt;/p&gt;

&lt;p&gt;因而根据是否选取这三个部分agent可分为下图中红色字体标出来的五种类型(这里有一个迷宫的例子很好，建议看原视频1:08:10起)。Model Free是指不需要去猜测environment的工作方式，而Model based则是需要学习environment的工作方式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/rl_1_2.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 Agent的分类(图片来源[1])&lt;/center&gt;&lt;/font&gt;

&lt;h4 id=&quot;section-3&quot;&gt;5. 探索和利用&lt;/h4&gt;

&lt;p&gt;强化学习是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的。因而agent需要从不断尝试的经验中发现一个好的policy，从而在这个过程中获取更多的reward。&lt;/p&gt;

&lt;p&gt;在这样的学习过程中，就会有一个在Exploration和Exploitation之间的权衡，前者是说会放弃一些已知的reward信息，而去尝试一些新的选择，即在某种状态下，算法也许已经学习到选择什么action让reward比较大，但是并不能每次都做出同样的选择，也许另外一个没有尝试过的选择会让reward更大，即Exploration希望能够探索更多关于environment的信息。而后者是指根据已知的信息最大化reward。例如，在选择一个餐馆时，Exploitation会选择你最喜欢的餐馆，而Exploration会尝试选择一个新的餐馆。&lt;/p&gt;

&lt;p&gt;以上是第一课的一些相关内容，主要是介绍了一些基础概念，从而对强化学习有一个基础的认识。&lt;/p&gt;

&lt;h4 id=&quot;section-4&quot;&gt;6. 引用&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf&quot;&gt;http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 06 Jun 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/06/06/reinforcementlearninglesssion1.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>[Python]内存管理</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文主要为了解释清楚python的内存管理机制，首先介绍了一下python关于内存使用的一些基本概念，然后介绍了引用计数和垃圾回收gc模块，并且解释了分代回收和“标记-清除”法，然后分析了一下各种操作会导致python变量和对象的变化，最后做了一下小结。本来是为了解决前几天遇到把服务器内存耗光的问题，结果后来检查发现并不是因为内存管理的问题…&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. Python变量、对象、引用、存储&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. Python内存管理机制和操作对变量的影响&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#2.1&quot;&gt;2.1 内存管理机制&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2.2&quot;&gt;2.2 各种操作对变量地址的改变&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. 小结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;4. 引用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;1&quot;&gt;1. Python变量、对象、引用、存储&lt;/h4&gt;

&lt;p&gt;python语言是一种解释性的编程语言，它不同于一些传统的编译语言，不是先编译成汇编再编程机器码，而是在运行的过程中，逐句将指令解释成机器码，所以造就了python语言一些特别的地方。例如a=1，其中a是变量，1是对象。这里所谓的变量，它的意义类似一个指针，它本身是没有类型的，只有它指向的那个对象是什么类型，它才是什么类型，一旦把它指到别的地方，它的类型就变了，现在指向的是1，它的类型可以认为是int，假如接下来执行a=2.5，那么变量的类型就变了。甚至当先给a=1，a=a+1时，a的地址也会改变。而这里的1,2.5或者一个list一个dict就是一个被实例化的对象，对象拥有真正的资源与取值，当一个变量指向某个对象，被称为这个对象的产生了一个引用，一个对象可以有多个变量指向它，有多个引用。而一个变量可以随时指向另外的对象。同时一个变量可以指向另外一个变量，那么它们指向的那个对象的引用就增加了一个。&lt;/p&gt;

&lt;p&gt;Python有个特别的机制，它会在解释器启动的时候事先分配好一些缓冲区，这些缓冲区部分是固定好取值，例如整数[-5,256]的内存地址是固定的(这里的固定指这一次程序启动之后，这些数字在这个程序中的内存地址就不变了，但是启动新的python程序，两次的内存地址不一样)。有的缓冲区就可以重复利用。这样的机制就使得不需要python频繁的调用内存malloc和free。下面的id是取内存地址，hex是转成16进制表示。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#第一次启动解释器
&amp;gt;&amp;gt;&amp;gt; hex(id(1))
&#39;0x14c5158&#39;
#第二次启动解释器
&amp;gt;&amp;gt;&amp;gt; hex(id(1))
&#39;0xe17158&#39;

#缓冲区被重复利用
&amp;gt;&amp;gt;&amp;gt; hex(id(100000))
&#39;0xe5be00&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(1000000))
&#39;0xe5be00&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(10000000))
&#39;0xe5be00&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(100000000))
&#39;0xe5be00&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;针对整数对象，它的内存区域似乎是一个单独的区域，跟string、dict等的内存空间都不一样，从实验结果来看，它的地址大小只有’0xe5be00’，其他的是’0x7fe7e03c7698’。而存储整数对象的这块区域，有一块内存区域是事先分配好的，即[-5,256]范围内的整数。这块称为小整数缓冲池，静态分配，对某个变量赋值就是直接从里面取就行了，在python初始化时被创建。而另外的整数缓冲池称为大整数缓冲池，这块内存也是已经分配好了，只是要用的时候再赋值。可以从下面的例子中看到，针对257这个数字，虽然给a和b赋了相同的值，但是解释器实际上是先分配了不同的地址，再把这个地址给两个变量。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = 1
&amp;gt;&amp;gt;&amp;gt; b = 1
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe17158&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0xe17158&#39;
&amp;gt;&amp;gt;&amp;gt; b = 257
&amp;gt;&amp;gt;&amp;gt; a = 257
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe5be00&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0xe5bdd0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;针对string类型，它也有自己的缓冲区，也是分为固定缓冲区和可重复缓冲区，固定的是256个ASCII码字符。还发现一个有意思的现象，string中只要不出现除了字母和数字其他字符，那么对a和b赋同样的值，它们的内存地址都相同。但是如果string对象中有其他字符，那么对两个变量赋相同的string值，它们的内存地址还是不一样的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; b = &#39;aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0x7fe7e03af848&#39;
&amp;gt;&amp;gt;&amp;gt; a = &#39;aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e03af848&#39;

&amp;gt;&amp;gt;&amp;gt; a = &#39;abcd%&#39;
&amp;gt;&amp;gt;&amp;gt; b = &#39;abcd%&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02d4900&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0x7fe7e02d48d0&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;而另外的dict和list的缓冲区也是事先分配好，大小为80个对象。&lt;/p&gt;

&lt;p&gt;因此变量的存储有三个区域，事先分配的静态内存、事先分配的可重复利用内存以及需要通过malloc和free来控制的自由内存。&lt;/p&gt;

&lt;h4 id=&quot;2&quot;&gt;2. Python内存管理机制和操作对变量的影响&lt;/h4&gt;

&lt;h5 id=&quot;2.1&quot;&gt;2.1 内存管理机制&lt;/h5&gt;

&lt;p&gt;python的内存在底层也是由malloc和free的方式来分配和释放，只是它代替程序员决定什么时候分配什么时候释放，同时也提供接口让用户手动释放，因此它有自己的一套内存管理体系，主要通过两种机制来实现，一个是引用计数，一个是垃圾回收。前者负责确定当前变量是否需要释放，后者解决前者解决不了的循环引用问题以及提供手动释放的接口[2]。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;引用计数(reference counting)&lt;/em&gt;&lt;/strong&gt;，针对可以重复利用的内存缓冲区和内存，python使用了一种引用计数的方式来控制和判断某快内存是否已经没有再被使用。即每个对象都有一个计数器count，记住了有多少个变量指向这个对象，当这个对象的引用计数器为0时，假如这个对象在缓冲区内，那么它地址空间不会被释放，而是等待下一次被使用，而非缓冲区的该释放就释放。&lt;/p&gt;

&lt;p&gt;这里通过sys包中的getrefcount()来获取当前对象有多少个引用。这里返回的引用个数分别是2和3，比预计的1和2多了一个，这是因为传递参数给getrefcount的时候产生了一个临时引用[1]。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = []
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
2
&amp;gt;&amp;gt;&amp;gt; b = a
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当一个变量通过另外一个变量赋值，那么它们的对象引用计数就增加1，当其中一个变量指向另外的地方，之前的对象计数就减少1。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = []
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
2
&amp;gt;&amp;gt;&amp;gt; b = a
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
3
&amp;gt;&amp;gt;&amp;gt; getrefcount(b)
3
&amp;gt;&amp;gt;&amp;gt; b = []
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
2
&amp;gt;&amp;gt;&amp;gt; getrefcount(b)
2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;垃圾回收(Garbage Collection)&lt;/em&gt;&lt;/strong&gt;python提供了del方法来删除某个变量，它的作用是让某个对象引用数减少1。当某个对象引用数变为0时并不是直接将它从内存空间中清除掉，而是采用垃圾回收机制gc模块，当这些引用数为0的变量规模达到一定规模，就自动启动垃圾回收，将那些引用数为0的对象所占的内存空间释放。这里gc模块采用了分代回收方法，将对象根据存活的时间分为三“代”，所有新建的对象都是0代，当0代对象经过一次自动垃圾回收，没有被释放的对象会被归入1代，同理1代归入2代。每次当0代对象中引用数为0的对象超过700个时，启动一次0代对象扫描垃圾回收，经过10次的0代回收，就进行一次0代和1代回收，1代回收次数超过10次，就会进行一次0代、1代和2代回收。而这里的几个值是通过查询get_threshold()返回(700,10,10)得到的。此外，gc模块还提供了手动回收的函数，即gc.collect()。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = []
&amp;gt;&amp;gt;&amp;gt; b = a
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
3
&amp;gt;&amp;gt;&amp;gt; del b
&amp;gt;&amp;gt;&amp;gt; getrefcount(a)
2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;而垃圾回收还有一个重要功能是，解决循环引用的问题，通常发生在某个变量a引用了自己或者变量a与b互相引用。考虑引用自己的情况，可以从下面的例子中看到，a所指向的内存对象有3个引用，但是实际上只有两个变量，假如把这两个变量都del掉，对象引用个数还是1，没有变成0，这种情况下，如果只有引用计数的机制，那么这块没有用的内存会一直无法释放掉。因此python的gc模块利用了“标记-清除”法，即认为有效的对象之间能通过有向图连接起来，其中图的节点是对象，而边是引用，下图中obj代表对象，ref代表引用，从一些不能被释放的对象节点出发(称为root object，&lt;strong&gt;&lt;em&gt;一些全局引用或者函数栈中的引用&lt;/em&gt;&lt;/strong&gt;[5]，例如下图的obj_1，箭头表示obj_1引用了obj_2)遍历各代引用数不为0的对象。在python源码中，每个变量不仅有一个引用计数，还有一个有效引用计数gc_ref，后者一开始等于前者，但是启动标记清除法开始遍历对象时，从root object出发(初始图中的gc_ref为(1,1,1,1,1,1,1))，当对象i引用了对象j时，将对象j的有效引用个数减去1，这样下图中各个对象有效引用个数变为了(1, 0, 0, 0, 0, 0, 0)，接着将所有对象分配到两个表中，一个是reachable对象表，一个是unreachable对象表，root object和在图中能够直接或者间接与它们相连的对象就放入reachable，而不能通过root object访问到且有效引用个数变为0的对象作为放入unreachable，从而通过这种方式来消去循环引用的影响。&lt;/p&gt;

&lt;p&gt;在人工调用gc.collect()的时候会有一个返回值，这个返回值就是这一次扫描unreachable的对象个数。在上面谈到的每一代的回收过程中，都会启用“标记-清除”法。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = []
&amp;gt;&amp;gt;&amp;gt; b = a
&amp;gt;&amp;gt;&amp;gt; getrefcount(b)
3
&amp;gt;&amp;gt;&amp;gt; a.append(a)
&amp;gt;&amp;gt;&amp;gt; getrefcount(b)
4
&amp;gt;&amp;gt;&amp;gt; del a
&amp;gt;&amp;gt;&amp;gt; getrefcount(b)
3
&amp;gt;&amp;gt;&amp;gt; del b
&amp;gt;&amp;gt;&amp;gt; unreachable = gc.collect()
&amp;gt;&amp;gt;&amp;gt; unreachable
1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/gc_graph.png&quot; alt=&quot;1&quot; height=&quot;40%&quot; width=&quot;40%&quot; hspace=&quot;280&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 变量形成的有向图&lt;/center&gt;&lt;/font&gt;

&lt;h5 id=&quot;2.2&quot;&gt;2.2 各种操作对变量地址的改变&lt;/h5&gt;

&lt;p&gt;当处理赋值、加减乘除时，这些操作实际上导致变量指向的对象发生了改变，已经不是原来的那个对象了，并不是通过这个变量来改变它指向的对象的值。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = 10
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe17080&#39;
&amp;gt;&amp;gt;&amp;gt; a = a - 1
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe17098&#39;
&amp;gt;&amp;gt;&amp;gt; a = a + 1
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe17080&#39;
&amp;gt;&amp;gt;&amp;gt; a = a * 10
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe177a0&#39;
&amp;gt;&amp;gt;&amp;gt; a = a / 2
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0xe17488&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;增加减少list、dict对象内容是在对对象本身进行操作，此时变量的指向并没有改变，它作为对象的一个别名/引用，通过操纵变量来改变对应的对象内容。但是一旦将变量赋值到别的地方去，那么变量地址就改变了。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = []
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02caef0&#39;
&amp;gt;&amp;gt;&amp;gt; a.append(1)
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02caef0&#39;
&amp;gt;&amp;gt;&amp;gt; a = [1]
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02caea8&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当把一个list变量赋值给另外一个变量时，这两个变量是等价的，它们都是原来对象的一个引用。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = []
&amp;gt;&amp;gt;&amp;gt; b = a
&amp;gt;&amp;gt;&amp;gt; a.append(1)
&amp;gt;&amp;gt;&amp;gt; b
[1]
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02caea8&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0x7fe7e02caea8&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;但是实际使用中，可能需要的是将里面的内容给复制出来到一个新的地址空间，这里可以使用python的copy模块，copy模块分为两种拷贝，一种是浅拷贝，一种是深拷贝。假设处理一个list对象，浅拷贝调用函数copy.copy()，产生了一块新的内存来存放list中的每个元素引用，也就是说每个元素的跟原来list中元素地址是一样的。所以从下面例子中可看出当原list中要是包含list对象，分别在a和b对list元素做操作时，两边都受到了影响。此外，通过b=list(a)来对变量b赋值时，也跟浅拷贝的效果一样。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = [1, 1000, [&#39;a&#39;, &#39;b&#39;]]
&amp;gt;&amp;gt;&amp;gt; b = copy.copy(a)
&amp;gt;&amp;gt;&amp;gt; b
[1, 1000, [&#39;a&#39;, &#39;b&#39;]]
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02e1368&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0x7fe7e02e1518&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(a[2]))
&#39;0x7fe7e02caea8&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b[2]))
&#39;0x7fe7e02caea8&#39;
&amp;gt;&amp;gt;&amp;gt; a[2].append(&#39;a+&#39;)
&amp;gt;&amp;gt;&amp;gt; a
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;]]
&amp;gt;&amp;gt;&amp;gt; b
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;]]
&amp;gt;&amp;gt;&amp;gt; b[2].append(&#39;b+&#39;)
&amp;gt;&amp;gt;&amp;gt; a
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;, &#39;b+&#39;]]
&amp;gt;&amp;gt;&amp;gt; b
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;, &#39;b+&#39;]]
&amp;gt;&amp;gt;&amp;gt; a[0] = 2
&amp;gt;&amp;gt;&amp;gt; a
[2, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;, &#39;b+&#39;]]
&amp;gt;&amp;gt;&amp;gt; b
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;, &#39;b+&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;而深拷贝则调用copy.deepcopy()，它将原list中每个元素都复制了值到新的内存中去了，因此跟原来的元素地址不相同，那么再对a和b的元素做操作，就是互相不影响了。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; a = [1, 1000, [&#39;a&#39;, &#39;b&#39;]]
&amp;gt;&amp;gt;&amp;gt; b = copy.deepcopy(a)
&amp;gt;&amp;gt;&amp;gt; hex(id(a))
&#39;0x7fe7e02cae18&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b))
&#39;0x7fe7e02e1368&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(a[2]))
&#39;0x7fe7e02e14d0&#39;
&amp;gt;&amp;gt;&amp;gt; hex(id(b[2]))
&#39;0x7fe7e02e1320&#39;
&amp;gt;&amp;gt;&amp;gt; a[2].append(&#39;a+&#39;)
&amp;gt;&amp;gt;&amp;gt; a
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;]]
&amp;gt;&amp;gt;&amp;gt; b
[1, 1000, [&#39;a&#39;, &#39;b&#39;]]
&amp;gt;&amp;gt;&amp;gt; b[2].append(&#39;b+&#39;)
&amp;gt;&amp;gt;&amp;gt; a
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;a+&#39;]]
&amp;gt;&amp;gt;&amp;gt; b
[1, 1000, [&#39;a&#39;, &#39;b&#39;, &#39;b+&#39;]]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当把一个变量传入一个函数时，它对应的对象引用个数增加2。&lt;/p&gt;

&lt;h4 id=&quot;3&quot;&gt;3. 小结&lt;/h4&gt;

&lt;p&gt;本来是因为前天把128g的服务器用死机了，想搞清楚为什么会导致那个问题，写完这篇去检查了一下，发现并不是因为对内存的使用有误导致的，而是因为我用到了多次hist函数，这个函数占了内存，换成了numpy的histgram函数就好了。不过写完也觉得很有意思，特别是垃圾回收其实是一个比较重要的不仅局限于python语言的一个东西，看了不少博客直接拿源码过来分析也是好牛的感觉。而对于标记清除法，个人不是特别理解为什么要加有效引用计数，那些循环引用的一个对象或者两个三个对象不应该跟有效的对象本来就是隔离开的，既然在遍历的时候，就能知道哪些对象是访问不到的，那么这些对象不就应该形成了环么。&lt;/p&gt;

&lt;h4 id=&quot;4&quot;&gt;4. 引用&lt;/h4&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://www.cnblogs.com/vamei/p/3232088.html&quot;&gt;Python深入06 Python的内存管理&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://www.digi.com/wiki/developer/index.php/Python_Garbage_Collection&quot;&gt;Python Garbage Collection&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://blog.csdn.net/zhzhl202/article/details/7547445&quot;&gt;Python内存池管理与缓冲池设计&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://www.cnblogs.com/kaituorensheng/p/4449457.html&quot;&gt;Python垃圾回收机制:gc模块&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] 《Python源码剖析》，陈儒著，2008&lt;/p&gt;

</description>
        <pubDate>Sat, 23 Apr 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/04/23/pythonmemorycontrol.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/04/23/pythonmemorycontrol.html</guid>
        
        <category>programming languages</category>
        
      </item>
    
      <item>
        <title>【机器学习算法系列之三】简述多种降维算法</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;最近看了一些关于降维算法的东西，本文首先给出了七种算法的一个信息表，归纳了关于每个算法可以调节的(超)参数、算法主要目的等等，然后介绍了降维的一些基本概念，包括降维是什么、为什么要降维、降维可以解决维数灾难等，然后分析可以从什么样的角度来降维，接着整理了这些算法的具体流程。主要目录如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. 降维基本概念&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. 从什么角度出发降维&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. 降维算法&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#3.1&quot;&gt;3.1 主成分分析PCA&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.2&quot;&gt;3.2 多维缩放(MDS)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.3&quot;&gt;3.3 线性判别分析(LDA)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.4&quot;&gt;3.4 等度量映射(Isomap)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.5&quot;&gt;3.5 局部线性嵌入(LLE)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.6&quot;&gt;3.6 t-SNE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.7&quot;&gt;3.7 Deep Autoencoder Networks&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;4. 小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;老规矩，先上一个各个算法信息表，$X$表示高维输入矩阵大小是高维数D乘以样本个数N，$C=XX^T$，$Z$表示降维输出矩阵大小低维数d乘以N，$E=ZZ^T$，线性映射是$Z=W^TX$，高维空间中两两之间的距离矩阵为A，$S_w, S_b$分别是LDA的类内散度矩阵和类间散度矩阵，k表示流形学习中一个点与k个点是邻近关系，$F$表示高维空间中一个点由周围几个点的线性组合矩阵，$M=(I-F)(I-F)^T$。$-$表示不确定。$P$是高维空间中两点距离占所有距离比重的概率矩阵，$Q$低维空间中两点距离占所有距离比重的概率矩阵。$l$表示全连接神经网络的层数，$D_l$表示每一层的节点个数。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/reduction.png&quot; alt=&quot;1&quot; height=&quot;100%&quot; width=&quot;100%&quot; hspace=&quot;0&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 不同降维算法对比&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;这里autoencoder是否去中心化个人觉得还是有点疑问，在处理图像数据的时候，会对输入图片做一个变到0均值的预处理，但是这个操作是针对一张样本内减均值[1]，这里的去中心化指的是针对某一维数据减均值，并不是一个概念。下面开始具体谈谈降维相关的内容。&lt;/p&gt;

&lt;h3 id=&quot;1&quot;&gt;1. 降维基本概念&lt;/h3&gt;

&lt;p&gt;降维的意思是能够用一组个数为d的向量$\mathbf{z_i}$来代表个数为D的向量$\mathbf{x_i}$所包含的有用信息，其中$d&amp;lt;D$。假设对一张512*512大小的图片，用svm来做分类，最直接的做法是将图按照行或者列展开变成长度为512*512的输入向量$\mathbf{x_i}$，跟svm的参数相乘。假如能够将512*512的向量在保留有用信息的情况下降维到100，那么存储输入和参数的空间会减少很多，计算向量乘法的时间也会减少很多。所以降维能够有效的减少计算时间。而高维空间的数据很有可能出现分布稀疏的情况，即100个样本在100维空间分布肯定是非常稀疏的，每增加一维所需的样本个数呈指数级增长，这种在高维空间中样本稀疏的问题被称为维数灾难。降维可以缓解这种问题。&lt;/p&gt;

&lt;p&gt;而为什么可以降维，这是因为数据有冗余，要么是一些没有用的信息，要么是一些重复表达的信息，例如一张512*512的图只有中心100*100的区域内有非0值，剩下的区域就是没有用的信息，又或者一张图是成中心对称的，那么对称的部分信息就重复了。正确降维后的数据一般保留了原始数据的大部分的重要信息，它完全可以替代输入去做一些其他的工作，从而很大程度上可以减少计算量。例如降到二维或者三维来可视化。&lt;/p&gt;

&lt;h3 id=&quot;2&quot;&gt;2. 从什么角度出发来降维&lt;/h3&gt;

&lt;p&gt;一般来说可以从两个角度来考虑做数据降维，一种是直接提取特征子集做特征抽取，例如从512*512图中只取中心部分，一种是通过线性/非线性的方式将原来高维空间变换到一个新的空间，这里主要讨论后面一种。后面一种的角度一般有两种思路来实现[2]，一种是基于从高维空间映射到低维空间的projection方法，其中代表算法就是PCA，而其他的LDA、Autoencoder也算是这种，主要目的就是学习或者算出一个矩阵变换W，用这个矩阵与高维数据相乘得到低维数据。另一种是基于流形学习的方法，流形学习的目的是找到高维空间样本的低维描述，它假设在高维空间中数据会呈现一种有规律的低维流形排列，但是这种规律排列不能直接通过高维空间的欧式距离来衡量，如下左图所示，某两点实际上的距离应该是下右图展开后的距离。如果能够有方法将高维空间中流形描述出来，那么在降维的过程中就能够保留这种空间关系，为了解决这个问题，流形学习假设高维空间的局部区域仍然具有欧式空间的性质，即它们的距离可以通过欧式距离算出(Isomap)，或者某点坐标能够由临近的节点线性组合算出(LLE)，从而可以获得高维空间的一种关系，而这种关系能够在低维空间中保留下来，从而基于这种关系表示来进行降维，因此流形学习可以用来压缩数据、可视化、获取有效的距离矩阵等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/manifold_learning.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 流形学习&lt;/center&gt;&lt;/font&gt;

&lt;h3 id=&quot;3&quot;&gt;3. 几种降维方法流程&lt;/h3&gt;

&lt;h4 id=&quot;3.1&quot;&gt;3.1 主成分分析PCA&lt;/h4&gt;

&lt;p&gt;PCA由Karl Pearson在1901年发明，是一种线性降维方法，高维空间(维数为D)的某个点$\mathbf{x_i} = (x_1, x_2, …, x_D)$通过与矩阵W相乘映射到低维空间(维数为d，$d&amp;lt;D$)中的某个点$\mathbf{z_i} = W^T\mathbf{x_i}$，其中W的大小是$D*d$，i对应的是第i个样本点。从而可以得到N个从D维空间映射到d维空间的点，PCA的目标是让映射得到的点$\mathbf{z_i}$尽可能的分开，即让N个$\mathbf{z_i}$的方差尽可能大。假如D维空间中的数据每一维均值为0，即$\sum_i \mathbf{x_i} = \mathbf{0}$，那么两边乘上$W^T$得到的降维后的数据每一维均值也是0，考虑一个矩阵$C=\frac{1}{N}X*X^T$，这个矩阵是这组D维数据的协方差矩阵，可以看出对角线上的值是D维中的某一维内的方差，非对角线元素是D维中两维之间的协方差。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{1}{N}X*X^T=\begin{pmatrix}
\frac{1}{N}\sum_{i=1}^N x_{1i}^2&amp; ... &amp; \frac{1}{N}\sum_{i=1}^N x_{1i}^Tx_{Di} \\ 
... &amp;  &amp;... \\ 
\frac{1}{N}\sum_{i=1}^N x_{Di}^Tx_{1i} &amp; ... &amp;\frac{1}{N}\sum_{i=1}^N x_{Di}^2  
\end{pmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;那么针对降维后d维数据的协方差矩阵$B=\frac{1}{N}Z*Z^T$，如果希望降维后的点尽可能分开，那么就希望B对角线上值即每一维的方差尽可能大，方差大说明这些维上数据具有非常好的区分性，同时希望d的每一维都是正交的，它们正交就会使得两个维是无关的，那么它们就不会包含重叠的信息，这样就能最好的表现数据，每一维都具有足够的区分性，同时还具有不同的信息。这种情况下B非对角线上值全部为0。又由于可以推导得出$B=\frac{1}{N}Z*Z^T = W^T*(\frac{1}{N}X*X^T)W = W^T*C*W$，这个式子实际上就是表示了线性变换矩阵W在PCA算法中的作用是让原始协方差矩阵C对角化。又由于线性代数中对角化是通过求解特征值与对应的特征向量得到，因此可以推出PCA算法流程(流程主要摘自周志华老师的《机器学习》一书，其中加入了目标和假设用于对比后面的算法。周老师书中是基于拉格朗日乘子法推导出来，本质上而言与[3]都是一样的，这里很推荐这篇讲PCA数学原理的博客[3])。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入:N个D维向量${\mathbf{x_1},…,\mathbf{x_N}}$，降维到d维&lt;/li&gt;
  &lt;li&gt;输出:投影矩阵$W=(w_1, …, w_d)$，其中每一个$w_i$都是D维列向量&lt;/li&gt;
  &lt;li&gt;目标:投影降维后数据尽可能分开，$\underset{w}{max} \; tr(W^TXX^TW)$(这里的迹是因为上面提到的B的非对角线元素都是0，而对角线上的元素恰好都是每一维的方差)&lt;/li&gt;
  &lt;li&gt;假设:降维后数据每一维方差尽可能大，并且每一维都正交&lt;/li&gt;
  &lt;li&gt;1.将输入的每一维均值都变为0，去中心化&lt;/li&gt;
  &lt;li&gt;2.计算输入的协方差矩阵$C=X*X^T$&lt;/li&gt;
  &lt;li&gt;3.对协方差矩阵C做特征值分解&lt;/li&gt;
  &lt;li&gt;4.取最大的前d个特征值对应的特征向量$w_1,…,w_d$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此外，PCA还有很多变种kernel PCA, probabilistic PCA等等，本文暂时只考虑最简单的PCA版本。&lt;/p&gt;

&lt;h4 id=&quot;3.2&quot;&gt;3.2 多维缩放(MDS)&lt;/h4&gt;

&lt;p&gt;MDS的目标是在降维的过程中将数据的dissimilarity(差异性)保持下来，也可以理解降维让高维空间中的距离关系与低维空间中距离关系保持不变。这里的距离用矩阵表示，N个样本的两两距离用矩阵A的每一项$a_{ij}$表示，并且假设在低维空间中的距离是欧式距离。而降维后的数据表示为$\mathbf{z_i}$，那么就有$a_{ij} = \left | \mathbf{z_i-z_j} \right |^2 = \left | \mathbf{z_i} \right |^2 + \left | \mathbf{z_j} \right |^2 - 2\mathbf{z_i}\mathbf{z_j}^T$，右边的三项统一用内积矩阵E来表示$e_{ij} = \mathbf{z_i}\mathbf{z_j}^T$。去中心化之后，E的每一行每一列之和都是0，从而可以推导得出&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e_{ij} = -\frac{1}{2}(a_{ij}^2-a_{i\cdot} - a_{\cdot j}-a_{\cdot\cdot}^2)=-\frac{1}{2}(e_{ii}+e_{jj}-2e_{ij}-\frac{1}{N}(tr(E)+Ne_{jj})-\frac{1}{N}(tr(E)+Ne_{ii})+\frac{1}{N^2}(2Ne_{jj})) \\ 
= e_{ij} = -\frac{1}{2}(PAP)_{ij}&lt;/script&gt;

&lt;p&gt;其中$P=I-\frac{1}{N}\mathbf 1$单位矩阵I减去全1矩阵的$\frac{1}{N}$，$i \cdot$与$\cdot j$是指某列或者某列总和，从而建立了距离矩阵A与内积矩阵E之间的关系。因而在知道A情况下就能够求解出E，进而通过对E做特征值分解，令$E=V\Lambda V^T$，其中$\Lambda$是对角矩阵，每一项都是E的特征值${\lambda_1\geq … \geq \lambda_d }$，那么在所有特征值下的数据就能表示成$Z=\Lambda^{\frac{1}{2}}V^T$，当选取d个最大特征值就能让在d维空间的距离矩阵近似高维空间D的距离矩阵，从MDS流程如下[2]:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入:距离矩阵$A^{N*N} = {a_{ij}}$，上标表示矩阵大小，原始数据是D维，降维到d维&lt;/li&gt;
  &lt;li&gt;输出:降维后矩阵$Z^{d*N}=\widetilde{\Lambda}^{\frac{1}{2}}\widetilde{V}$&lt;/li&gt;
  &lt;li&gt;目标:降维的同时保证数据之间的相对关系不变&lt;/li&gt;
  &lt;li&gt;假设:已知N个样本的距离矩阵&lt;/li&gt;
  &lt;li&gt;1.算出$a_{i\cdot}、a_{\cdot j}、a_{\cdot \cdot}$&lt;/li&gt;
  &lt;li&gt;2.计算内积矩阵E&lt;/li&gt;
  &lt;li&gt;3.对E做特征值分解&lt;/li&gt;
  &lt;li&gt;4.取d个最大特征值构成$\widetilde{V}$，对应的特征向量按序排列构成$\widetilde{\Lambda}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3.3&quot;&gt;3.3 线性判别分析(LDA)&lt;/h4&gt;

&lt;p&gt;LDA最开始是作为解决二分类问题由Fisher在1936年提出，由于计算过程实际上对数据做了降维处理，因此也可用作监督线性降维。它通过将高维空间数据投影到低维空间，在低维空间中确定每个样本所属的类，这里考虑K个类的情况。它的目标是将样本能尽可能正确的分成K类，体现为同类样本投影点尽可能近，不同类样本点尽可能远，这点跟PCA就不一样，PCA是希望所有样本在某一个维数上尽可能分开，LDA的低维投影可能会重叠，但是PCA就不希望投影点重叠。它采用的降维思路跟PCA是一样的，都是通过矩阵乘法来进行线性降维，投影点是$\mathbf{z_i}=W^T*\mathbf{x_i}$。假设按下图中的方向来投影，投影中心对应的原来高维点分别是$\mu1，\mu2$。由于希望属于不同类的样本尽可能离的远，那么就希望投影过后的投影中心点离的尽可能远，即目标是$\underset{W}{max}\; || W^T\mathbf{\mu_1}-W^T\mathbf{\mu_2} | |^2$，但是仅仅有中心离的远还不够，例如下图中垂直于x1轴投影，两个中心离的足够远，但是却有样本在投影空间重叠，因此还需要额外的优化目标，即同类样本投影点尽可能近。那么同类样本的类间协方差就应该尽可能小，同类样本的协方差矩阵如下。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lda.jpg&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图3 LDA进行投影(图来源[4])&lt;/center&gt;&lt;/font&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{w}{min}\;
= \underset{X\in X_1}{\sum}(W^TX-W^T\mu_1)(W^TX-W^T\mu_1)^T = W^T(\underset{X\in X_1}{\sum}(X-\mu_1)(X-\mu_1))W = W^T\sum_{X_1} W&lt;/script&gt;

&lt;p&gt;其中$\mu_1 = (u_1, …, u_N), W = (w_1, …, w_d)$，$X_1$表示样本属于第1类的集合，中间的矩阵$\sum_{X_1}$是属于第$X_1$的样本协方差矩阵，将K个类的原始数据协方差矩阵加起来称为类内散度矩阵，$S_w=\sum_{k=1}^{K}S_k = \sum_{k=1}^{K}\sum_{X\in X_k}(X-\mu_k)(X-\mu_k)^T$。而上面两个类的中心距离是中心直接相减，K个类投影中心距离需要先计算出全部样本的中心$\mu = \frac{1}{N} \sum_{k=1}^{K}N_k\mu_k$($N_k$表示属于第k类的样本个数)，通过类间散度矩阵来衡量，即$S_b=\sum_{k=1}^{K} N_k (\mathbf{\mu_k} - \mathbf{\mu})(\mathbf{\mu_k} - \mathbf{\mu})^T$。整合一下，LDA算法的优化目标是最大化下面的cost function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{W}{max} \; J(W) = \frac{tr(W^T(\sum_{k=1}^{K} N_k (\mathbf{\mu_k} - \mathbf{\mu})(\mathbf{\mu_k} - \mathbf{\mu})^T)W)} {tr(W^T\sum_{k=1}^{K}\sum_{X\in X_k}(X-\mu_k)(X-\mu_k)^T)W)}
=\frac{tr(W^TS_bW)}{tr(W^TS_wW)}&lt;/script&gt;

&lt;p&gt;二分类情况下，W的大小是$D*1$，即$J(W)$本身是个标量，针对K类的情况，W的大小是$D*d-1$，优化的目标是对上下的矩阵求它的迹。这里可以发现在LDA中没有对数据去中心化，如果要去中心化每个类的中心就会重叠了，所以这个算法没有去中心化。然后$J(W)$对W求导，这个式子就表明了W的解是$S_w^{-1}S_b$的d-1个最大特征值对应的特征向量组成的矩阵。那么就可以通过W来对X进行降维。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial J(W)}{\partial W} = (W^TS_wW)2S_bW-(W^TS_bW)2S_wW = 0 \\
\Rightarrow S_bW - J(W)S_wW = 0 \Rightarrow S_w^{-1}S_bW = J(W)W&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;输入:N个D维向量${\mathbf{x_1},…,\mathbf{x_N}}$，数据能够被分成d个类&lt;/li&gt;
  &lt;li&gt;输出:投影矩阵$W=(w_1, …, w_{d-1})$，其中每一个$w_i$都是D维列向量&lt;/li&gt;
  &lt;li&gt;目标:投影降维后同一类的样本之间协方差尽可能小，不同类之间中心距离尽可能远&lt;/li&gt;
  &lt;li&gt;假设:优化目标是最大化$\frac{tr(W^TS_bW)}{tr(W^TS_wW)}$&lt;/li&gt;
  &lt;li&gt;1.求出类内散度矩阵$S_w$和类间散度矩阵$S_b$&lt;/li&gt;
  &lt;li&gt;2.对$S_w = U\sum V^T$做奇异值分解，求得$S_w^{-1}=V\sum^{-1}U^T$&lt;/li&gt;
  &lt;li&gt;3.对矩阵$S_w^{-1}S_b$做特征分解&lt;/li&gt;
  &lt;li&gt;4.取最大的前d-1个特征值对应的特征向量$w_1,…,w_{d-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;个人觉得这里的优化目标实际上体现了一个假设，即假设优化目标上下的表达式都是对角矩阵，W的变换使得$S_d$与$S_w$都变成了对角矩阵。&lt;/p&gt;

&lt;h4 id=&quot;3.4&quot;&gt;3.4 等度量映射(Isomap)&lt;/h4&gt;

&lt;p&gt;上面提到的MDS只是对数据降维，它需要已知高维空间中的距离关系，它并不能反应出高维数据本身潜在的流形，但是可以结合流形学习的基本思想和MDS来进行降维[5]。也就是高维空间的局部空间的距离可以用欧式距离算出，针对MDS的距离矩阵A，某两个相邻的点之间距离$A_{ij} = \left | x_i -x_j \right |$也就是它们的欧式距离，距离比较近的点则通过最短路径算法来确定，而离的比较远的两点之间$A_{ij} = \infty$，把矩阵A确定下来，那么这里就涉及到判断什么样的点相邻，Isomap是通过KNN来确定相邻的点，整体算法流程如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入:N个D维向量${\mathbf{x_1},…,\mathbf{x_N}}$，一个点有K个近邻点，映射到d维&lt;/li&gt;
  &lt;li&gt;输出:降维后矩阵$Z^{d*N}=\widetilde{\Lambda}^{\frac{1}{2}}\widetilde{V}$&lt;/li&gt;
  &lt;li&gt;目标:降维的同时保证高维数据的流形不变&lt;/li&gt;
  &lt;li&gt;假设:高维空间的局部区域上某两点距离可以由欧式距离算出&lt;/li&gt;
  &lt;li&gt;1.由KNN先构造A的一部分，即求出相邻的点并取它们的欧式距离填入$A_{ij}$，其他的位置全部初始化为无穷大&lt;/li&gt;
  &lt;li&gt;2.根据最短路径算法(Dijkstra算法)找到距离比较近的点之间的路径并填入距离&lt;/li&gt;
  &lt;li&gt;3.将距离矩阵A作为MDS的输入，得到输出&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3.5&quot;&gt;3.5 局部线性嵌入(LLE)&lt;/h4&gt;

&lt;p&gt;如之前提到过的，流形学习的局部区域具有欧式空间的性质，那么在LLE中就假设某个点$x_i$坐标可以由它周围的一些点的坐标线性组合求出，即$\mathbf{x_i} = \sum_{j\in X_i}f_{ij} \mathbf{x_j}$(其中$X_i$表示$\mathbf{x_i}$的邻域上点的集合)，这也是在高维空间的一种表示。由于这种关系在低维空间中也被保留，因此$\mathbf{z_i} = \sum_{j\in Z_i}f_{ij} \mathbf{z_j}$，两个式子里面权重取值是一样的。&lt;/p&gt;

&lt;p&gt;基于上面的假设，首先想办法来求解这个权重，假设每个样本点由周围K个样本求出来，那么一个样本的线性组合权重大小应该是$1*K$，通过最小化reconstruct error重构误差来求解，然后目标函数对f求导得到解。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{f_1,...,f_K}{min} \; \sum_{k=1}^K\left \| \mathbf{x_i}- \sum_{j\in X_i}f_{ij}\mathbf{x_j} \right \|   \;\;\;\;\;\;\;\; s.t.\sum_{j\in X_i}f_{ij}=1 \\&lt;/script&gt;

&lt;p&gt;求出权重之后，代入低维空间的优化目标$\underset{z_1,…,z_K}{min}  \sum_{k=1}^K\left | \mathbf{z_i}- \sum_{j\in Z_i}f_{ij}\mathbf{z_j} \right | = tr((Z-Z*F)(Z-Z*F)^T) = tr(ZMZ)  \;\;\;\;\;\;\;\; s.t. Z*Z^T=I$来求解Z，这里将F按照$N*K$排列起来，且加入了对Z的限制。这里用拉格朗日乘子法可以得到$MZ=\lambda Y$的形式，从而通过对M进行特征值分解求得Z。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入:N个D维向量${\mathbf{x_1},…,\mathbf{x_N}}$，一个点有K个近邻点，映射到d维&lt;/li&gt;
  &lt;li&gt;输出:降维后矩阵Z&lt;/li&gt;
  &lt;li&gt;目标:降维的同时保证高维数据的流形不变&lt;/li&gt;
  &lt;li&gt;假设:高维空间的局部区域上某一点是相邻K个点的线性组合，低维空间各维正交&lt;/li&gt;
  &lt;li&gt;1.由KNN先构造A的一部分，即求出K个相邻的点，然后求出矩阵F和M&lt;/li&gt;
  &lt;li&gt;2.对M进行特征值分解&lt;/li&gt;
  &lt;li&gt;3.取前d个非0最小的特征值对应的特征向量构成Z(这里因为最小化目标，所以取小的特征值)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3.6&quot;&gt;3.6 t-SNE&lt;/h4&gt;

&lt;p&gt;t-SNE也是一种将高维数据降维到二维或者三维空间的方法，它是2008年由Maaten提出[6]，基于2002年Hinton提出的随机近邻嵌入(Stochastic Neighbor Embedding, SNE)方法的改进。主要的思想是假设高维空间中的任意两个点，$x_j$的取值服从以$x_i$为中心方差为$\sigma_i$的高斯分布，同样$x_i$服从以$x_j$为中心方差为$\sigma_j$的高斯分布,这样$x_j$与$x_i$相似的条件概率就为$p_{j|i} = \frac{exp(-\left || \mathbf{x_i}-\mathbf{x_j} \right ||^2 / 2\sigma_i^2)}{\sum_{k\neq i}exp(-\left || \mathbf{x_i}-\mathbf{x_k} \right ||^2 / 2\sigma_i^2)}$，即$x_j$在$x_i$高斯分布下的概率占全部样本在$x_i$高斯分布下概率的多少，说明了从$x_i$角度来看两者的相似程度。接着令$p_{ij} = (p_{i|j} + p_{j | i})/2n$用这个概率来作为两个点相似度在全部样本两两相似度的联合概率$p_{ij}$。公式如下，论文没有解释$\sigma$是标量还是矢量，但是因为在后续的求解中$p_{ij}$不是直接由下面这个联合概率公式求出，而是通过前面的条件概率来求，前面的式子针对每一个样本i都会计算一个$\sigma_i$，具体给定一个确定值$Prep(P_i) = 2^{H(P_i)}$，其中$H(P_i)=-\sum_jp_{j|i}logp_{j|i}$。接着通过二分查找来确定$x_i$对应的$\sigma_i$，使得代入上面的两个式子后等于$Prep$的值，因此这里的$\sigma$应该是个矢量。不太可能所有样本都共用一个高斯分布参数。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{ij}=\frac{exp(-\frac{\left \| \mathbf{x_i}-\mathbf{x_j} \right \|^2}{2\sigma^2})}{\sum_{k\neq l}exp(-\frac{\left \| \mathbf{x_k}-\mathbf{x_l} \right \|^2}{2\sigma^2})}&lt;/script&gt;

&lt;p&gt;同时将低维空间两个点的相互关系或者说相似程度也用联合概率来表示，假设在低维空间中两点间欧式距离服从一个自由度的学生t分布，那么在低维空间中两个点的距离概率在所有的两个点距离概率之中的比重作为它们的联合概率。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;q_{ij}=\frac{(1+{\left \| \mathbf{z_i}-\mathbf{z_j} \right \|^2})^{-1}}{\sum_{k\neq l}(1+\left \| \mathbf{z_k}-\mathbf{z_l} \right \|^2)^{-1}}&lt;/script&gt;

&lt;p&gt;假如在高维空间的$x_i, x_j$与对应在低维空间中的$z_i, z_j$算出来的相似性值$p_{ij}, q_{ij}$相等，那么就说明低维空间的点能够正确的反应高维空间中的相对位置关系。所以tsne的目的就是找到一组降维表示能够最小化$p_{ij}$和$q_{ij}$之间的差值。因此，tsne采用了KullbackLeibler divergence即KL散度来构建目标函数$ J = KL(P || Q) = \sum_i\sum_jp_{ij}log\frac{p_{ij}}{q_{ij}}$，KL散度能够用来衡量两个概率分布的差别。它通过梯度下降的方法来求输入数据对应的低维表达$z_i$，即用目标函数对$z_i$求导，把$z_i$作为可优化变量，求得每次对$z_i$的梯度为$\frac{\partial J}{\partial \mathbf{z_i}} = 4\sum_j(p_{ij}-q_{ij})(1+||\mathbf{z_i}-\mathbf{z_j}||^2)^{-1}$，然后更新迭代$z_i$，在实际更新的过程中则像神经网络的更新一样加入了momentum项为了加速优化，大概的算法流程如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入:N个D维向量${\mathbf{x_1},…,\mathbf{x_N}}$，映射到二维或者三维，定值$Perp$，迭代次数T，学习率$\eta$，momentum项系数$\alpha(t)$&lt;/li&gt;
  &lt;li&gt;输出:降维后数据表示${\mathbf{z_1},…,\mathbf{z_N}}$&lt;/li&gt;
  &lt;li&gt;目标:降维到二维或者三维可视化(重点是可视化)&lt;/li&gt;
  &lt;li&gt;假设:在高维空间中，一个点$x_j$的取值服从以另外一个点$x_i$为中心的高斯分布。在低维空间中，两个点之间的欧式距离服从自由度为1的t分布&lt;/li&gt;
  &lt;li&gt;1.先由二分查找确定$x_i$的$\sigma_i$&lt;/li&gt;
  &lt;li&gt;2.计算成对的$P_{j|i}$，得到$p_{ij} = (p_{j|i}+p_{i|j})/2$&lt;/li&gt;
  &lt;li&gt;3.初始化${\mathbf{z_1},…,\mathbf{z_N}}$&lt;/li&gt;
  &lt;li&gt;4.计算$q_{ij}$&lt;/li&gt;
  &lt;li&gt;5.计算梯度$\frac{\partial J}{\partial \mathbf{z_i}}$&lt;/li&gt;
  &lt;li&gt;6.更新$\mathbf{z_{i}}^{(t)} = \mathbf{z_{i}}^{(t-1)} + \eta \frac{\partial J}{\partial \mathbf{z_i}} + \alpha(t)(\mathbf{z_{i}}^{(t-1)} - \mathbf{z_{i}}^{(t-2)})$&lt;/li&gt;
  &lt;li&gt;7.重复4~6至收敛或者完成迭代次数T&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要注意的是，这个算法将低维数据作为变量进行迭代，所以如果需要加入插入新的数据，是没有办法直接对新数据进行操作，而是要把新数据加到原始数据中再重新算一遍，因此T-sne主要的功能还是可视化。&lt;/p&gt;

&lt;h4 id=&quot;3.7&quot;&gt;3.7 Deep Autoencoder Networks&lt;/h4&gt;

&lt;p&gt;Autoencoder是神经网络的一种，它是一种无监督算法，可以用来降维也能用来从数据中自动学习某种特征，这个神经网络的原理是输入一组值，经过网络之后能够获得一组输出，这组输出的值尽可能的跟输入的值大小一致。网络由全连接层组成，每层每个节点都跟上一层的所有节点连接。Autoencoder的结构如下图4所示，encoder网络是正常的神经网络前向传播$\mathbf{z} = W\mathbf{x}+\mathbf{b}$，decoder网络的传播参数是跟它成对称结构的层参数的转置，经过这个网络的值为$\mathbf{\mathbf{x}’ = W^T\mathbf{z}+\mathbf{b}^T}$，最后传播到跟网络的输入层个数相等的层时，得到一组值$\mathbf{x}’$，网络希望这两个值相等$\mathbf{x}’=\mathbf{x}$，这个值与真实输入$x$值通过交叉熵或者均方误差得到重构误差的cost function，再通过最小化这个cost和梯度下降的方法使网络学到正确的参数。因此可以通过这个网络先经过”encoder”网络将高维数据投影到低维空间，再经过”decoder”网络反向将低维数据还原到高维空间。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/autoencoder.png&quot; alt=&quot;1&quot; height=&quot;15%&quot; width=&quot;15%&quot; hspace=&quot;490&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图4 Autoencoder网络结构图&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;然而在实际的实现网络过程中，整个网络实际上层数只是图4中的一半，即4层网络，2000-1000-500-30的全连接结构。因为权重参数实际上在encoder和decoder中是相同的，enocoder过程是上一层的节点值乘以权重得到这一层的节点值，而decoder是这一层节点值与权重矩阵的转置相乘得到上一层的节点值。下图[7]更加清晰的展示了每一层实际的结构，包括一次前向传播和后向传播，因此可以拿最顶层的值作为网络的降维输出来进行其他的分析，例如可视化，或者作为压缩特征使用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/autoencoder2.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图5 Autoencoder层间结构&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;06年的时候Hinton在science上发了一篇文章讲如何用深度学习中的autoencoder网络来做降维[8]，主要是提出了先通过多层RBM来预训练权重参数，用来解决autoencoder降维后的质量依赖初始化网络权重的问题，即主要目的是提出一种有效的初始化权重的方式。上面的表达式中没有加入非线性变换，真实网络中每一层跟权重做矩阵乘法之后还需要加上非线性变换。此外，autoencoder的模型中可以加入sparsity的性质[9]，即针对N个D维输入，某一层的某一个节点输出值之和$\widehat{\rho_j}^{(l)}$趋近于0，即$\widehat{\rho_j}^{(l)} = \frac{1}{N}\sum_{i=1}^N[a_j^{(l)}(\mathbf{x^{(i)}})]$，其中l代表是哪一层，i代表是第几个输入。也能加对权重有要求的正则项。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入:N个D维向量${\mathbf{x_1},…,\mathbf{x_N}}$，网络结构即每层节点数，迭代次数T，学习率$\eta$&lt;/li&gt;
  &lt;li&gt;输出:降维后数据表示${\mathbf{z_1},…,\mathbf{z_N}}$&lt;/li&gt;
  &lt;li&gt;目标:网络能够学习到数据内部的一些性质或者结构，从而能够重构输入数据&lt;/li&gt;
  &lt;li&gt;假设:神经网络就是特牛逼，就是能学到特征，科科&lt;/li&gt;
  &lt;li&gt;1.设置层数和每一层节点数&lt;/li&gt;
  &lt;li&gt;2.初始化权重参数&lt;/li&gt;
  &lt;li&gt;3.前向传播计算下一层的节点值$\mathbf{z} = W\mathbf{x}+\mathbf{b}$&lt;/li&gt;
  &lt;li&gt;4.反向传播计算上一层反向节点值$\mathbf{\mathbf{x}’ = W^T\mathbf{z}+\mathbf{b}^T}$&lt;/li&gt;
  &lt;li&gt;5.计算每一层对输入和对这层参数W的梯度，利用反向传播将error传递到整个网络&lt;/li&gt;
  &lt;li&gt;6.将分别对$W$和$W^T$的梯度求和然后更新$W$&lt;/li&gt;
  &lt;li&gt;7.重复3~6至收敛或者完成迭代次数T&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4&quot;&gt;4. 小结&lt;/h3&gt;

&lt;p&gt;本文主要重点放在算法流程是什么，每一步具体做了什么，有的地方可能理论阐述还不够清晰。但是有意思的是除了t-sne和autoencoder之外，其他的几种降维算法都是基于构造某个矩阵，然后对矩阵进行特征值分解，得到相关的$Z$或者$W$。Laplacian Eigenmaps拉普拉斯特征映射没有完整研究，但是看算法最后也是选择前d个最小非零特征值，这个很有意思，就是数学功底不好，暂时想不通为什么基于特征值的效果这么好。而比较一层的autoencoder和PCA，假设autoencoder的目标函数是最小化均方误差，虽然autoencoder没有PCA那么强的约束(要求每一维正交)，但是autoencoder也许可以学到，因为本身基于最大化协方差的迹与最小均方差估计是等价的。几种方法总是让人感觉有着某些潜在的关联，不知道是不是能够提取出一种统一的模型能够把降维这件事情给解决掉。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing#Per-example_mean_subtraction&quot;&gt;Data Preprocessing&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://research.microsoft.com/pubs/150728/FnT_dimensionReduction.pdf&quot;&gt;Dimension Reduction: A Guided Tour&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://blog.codinglabs.org/articles/pca-tutorial.html&quot;&gt;PCA的数学原理&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.di.univr.it/documenti/OccorrenzaIns/matdid/matdid437773.pdf&quot;&gt;A Tutorial on Data Reduction Linear Discriminant Analysis (LDA)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.eecs.berkeley.edu/~efros/courses/AP06/presentations/ThompsonDimensionalityReduction.pdf&quot;&gt;manifold learning with applications to object recognition&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf&quot;&gt;Visualizing Data using t-SNE&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://cs.stanford.edu/~quocle/tutorial2.pdf&quot;&gt;A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~hinton/science.pdf&quot;&gt;Reducing the Dimensionality of Data with Neural Networks&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf&quot;&gt;CS294A Lecture notes Sparse autoencoder&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Fri, 01 Apr 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/04/01/dimensionalityreduction.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【GPU编程系列之二】CUDA的软件层面和NVCC编译流程</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是gpu编程系列第二篇，主要谈谈CUDA软件层面的一些内容，这里的软件层面是指用户写程序涉及到的一些内容，包括介绍CUDA、区分Runtime API与Driver API以及介绍一些常用的库，最后谈一谈NVCC的编译流程。&lt;/p&gt;

&lt;p&gt;####1. CUDA是什么？&lt;/p&gt;

&lt;p&gt;接触过gpu编程的人肯定都会看到过一个词CUDA，全称是Compute Unified Device Architecture，英伟达在2007年推出这个统一计算架构，为了让gpu有可用的编程环境，从而能通过程序控制底层的硬件进行计算。CUDA提供host-device的编程模式以及非常多的接口函数和科学计算库，通过同时执行大量的线程而达到并行的目的。在上一篇讲gpu硬件的文章中提到过流处理器(SM)的概念，并且说过SM是gpu的计算单元，而线程是执行在SM上的并行代码。CUDA也有不同的版本，从1.0开始到现在的7.5，每个版本都会有一些新特性。CUDA是基于C语言的扩展，例如扩展了一些限定符__device__、__shared__等，从3.0开始也支持c++编程，从7.0开始支持c++11。&lt;/p&gt;

&lt;p&gt;在安装CUDA的时候，会安装三个大的组件[1]，分别是NVIDIA驱动、toolkit和samples。驱动用来控制gpu硬件，toolkit里面包括nvcc编译器、Nsight调试工具(支持Eclipse和VS，linux用cuda-gdb)、分析和调试工具和函数库。samples或者说SDK，里面包括很多样例程序包括查询设备、带宽测试等等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/install_ubuntu_3.png&quot; alt=&quot;1&quot; height=&quot;20%&quot; width=&quot;20%&quot; hspace=&quot;350&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 CUDA安装完成截面(点击查看大图)&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;####2. Runtime API vs Driver API&lt;/p&gt;

&lt;p&gt;在写cuda程序时，除了自己写的kernel函数，常常会调用cuda接口函数，最常见的就是全局内存分配函数cudaMalloc()，这里分配的内存暂且理解为gpu硬件参数上的显存。然而在某种情况下可能会看到另外一个函数cuMemAlloc()，这两个函数本质上完成的功能是一样的，都是在分配全局内存，但却属于两套接口，分别为Runtime API和Driver API。下图是cuda软件层的一些组件，实际上在cuda的软件层面，Runtime比Driver API更高级，封装的更好，在Runtime之上就是封装的更好的cuFFT等库。这两个库的函数都是能直接调用的，但Driver API相对于Runtime对底层硬件驱动的控制会更直接更方便，比如对context的控制[2]，Driver API调用硬件速度实际上比Runtime也快不了多少。不过Driver API向后兼容支持老版本的，这点Runtime就做不到，7.0的版本代码可能在6.5上就跑不了。大部分的功能两组API都有对应的实现，一般基于Driver API的开头会是cu，而基于Runtime API的开头是cuda，但基于Driver API来写程序会比Runtime API要复杂，虽然功能上差别不大，但是使用Runtime API和更高级的库函数就已经足够了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/cuda_software.png&quot; alt=&quot;1&quot; height=&quot;20%&quot; width=&quot;20%&quot; hspace=&quot;350&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 CUDA软件层(图片来源《The cuda handbook》，点击查看大图)&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;####3. 常用函数库&lt;/p&gt;

&lt;p&gt;NVIDIA针对cuda的使用开发了很多好用的库，包括实现c++ STL的thrust、实现gpu版本blas的cublas、实现快速傅里叶变换的cuFFT、实现稀疏矩阵运算操作的cuSparse以及实现深度学习网络加速的cuDNN等等[3]。在操作这些库时有一个通用的规范，即调用者进行设备内存的分配与释放，内存分配好后将指针传递给这些库接口，就可以进行计算了。&lt;/p&gt;

&lt;p&gt;关于thrust，它最基本的数据类型是两个向量容器，host_vetcor和device_vector，分别对应了内存分配在cpu内存和cpu内存，并且提供了非常多的函数模板，例如归约、归并、排序、二分查找等等。此外支持很多STL容器，例如下面的例子(代码来源[4])中即可以从c++容器中将数据复制给thrust的vector，也能将thrust的数据复制给c++ stl。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;std::list&amp;lt;int&amp;gt; stl_list;
stl_list.push_back(10);
stl_list.push_back(20);
stl_list.push_back(30);
stl_list.push_back(40);

// 从c++ stl的list来初始化device_vector 
thrust::device_vector&amp;lt;int&amp;gt; D(stl_list.begin(), stl_list.end()); 

// 将device_vector的内容复制到stl的vector中 
std::vector&amp;lt;int&amp;gt; stl_vector(D.size()); 
thrust::copy(D.begin(), D.end(), stl_vector.begin());
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;而cublas的接口设计基本上跟blas库是一致的，也是有lever1~3的接口，不同点在cuda的库风格是在使用相关函数之前要创建一个句柄，这个句柄也要传递到相关函数中去，在程序完成之后要销毁对应的句柄。因此cublas库函数原型比blas的参数要多。句柄的设置是为了方便执行多gpu，即在通过调用cudaSetDevice()来启用不同的gpu设备，在每个设备上又可以初始化一个独立的句柄，那么就能同时在多个gpu上执行。虽然cuda允许不同的线程调用同一个句柄，但是最好还是不要这样做。&lt;/p&gt;

&lt;p&gt;至于cudnn，它是专门针对深度学习实现的一个库，目前主要还是实现卷积神经网络，加速效果很好，现在的一些深度学习框架基本上都支持它。&lt;/p&gt;

&lt;p&gt;####4. NVCC编译流程&lt;/p&gt;

&lt;p&gt;由于程序是要经过编译器编程成可执行的二进制文件，而cuda程序有两种代码，一种是运行在cpu上的host代码，一种是运行在gpu上的device代码，所以NVCC编译器要保证两部分代码能够编译成二进制文件在不同的机器上执行。nvcc涉及到的文件后缀及相关意义如下表[5]。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;文件后缀&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;意义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cu&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cuda源文件，包括host和device代码&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cup&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;经过预处理的cuda源文件，编译选项--preprocess/-E&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.c&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c源文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cc/.cxx/.cpp&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++源文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.gpu&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gpu中间文件，编译选项--gpu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.ptx&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;类似汇编代码，编译选项--ptx&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.o/.obj&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;目标文件，编译选项--compile/-c&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.a/.lib&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;库文件，编译选项--lib/-lib&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.res&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;资源文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.so&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;共享目标文件，编译选项--shared/-shared&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cubin&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cuda的二进制文件，编译选项-cubin&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;此外编译的常用选项还有--link，编译并链接所有的输入文件，--run编译链接文件后直接执行，-m指定32位还是64位机器。&lt;/p&gt;

&lt;p&gt;有时还会见到--gpu-architecture/-arch和--gpu-code/-code，它们的目标是为了让gpu代码能够兼容多种架构的gpu，它们的取值范围是一样的包括compute_10/compute_11/…/compute_30/compute_35/sm_10/sm_11/…/sm_30/sm_35，这些取值范围的意义实际上是指gpu的计算能力，或者是SM的版本，一般表示的是1.0/2.0/3.5这样的，具体体现在之前讲过的kepler/fermi的不同架构，所以两者实际含义是一样的(这里看[5]上表达应该是两者等价，但是为什么要搞两个，统一叫计算能力不行吗…)。而这里两个编译选项都要选择计算能力/sm版本，这是因为nvcc最后生成的可执行文件可以同时存在多个版本(对应compute_10/…/compute_35等)的kernel函数，这多个版本就通过这两个编译选项确定。这两个编译选项使得中间会生成的ptx文本文件和cubin二进制文件，它们指定了最后生成的可执行文件中可以满足的版本要求，即通过-arch指定ptx将来可以生成怎么样的版本(可以看成针对一个虚拟的gpu)，而-code参数是指当前就要生成的二进制的版本(可以想象成一个真实的GPU)，当前和将来的意思是指，最后生成的可执行文件中一个部分是马上就能在gpu上执行，而如果gpu硬件版本不支持这个能够马上执行的部分，那么显卡驱动会重新根据ptx指定的版本再生成一个能够执行的可执行版本，来满足这个gpu的硬件需求。它们的版本信息会先嵌入fatbin文件中，再通过fatbin与host代码编译生成的中间结果链接成最后的目标文件，这是针对一个.cu源文件生成一个.o文件，再将不同.o链接成可执行文件，那么这个可执行文件中就包含了多个版本的信息。假如取–arch=compute_10 –code=sm_13，在最后的可执行文件中，就有一个可以直接执行1.3的版本，假如此时gpu计算能力只有1.0，那么驱动会再次编译生成1.0版本的可执行文件，这个新的可执行文件就能在计算能力只有1.0的机器上运行了，从而通过这样的方式可以兼容不同计算能力的gpu。需要注意的是code的版本要高于arch。&lt;/p&gt;

&lt;p&gt;显然这里有一个问题，一般而言生成的c++可执行程序就直接执行了，不会再有编译的过程，但是cuda不一样，它有一个机制叫做just-in-time(JIT，运行时编译)，为了满足两种执行cuda程序的方式。第一种就是直接执行cubin版本，第二种就是显卡驱动通过JIT在运行的时候根据ptx版本再次编译生成可执行文件。&lt;/p&gt;

&lt;p&gt;NVCC实际上调用了很多工具来完成编译步骤(建议先看看[5]中完善的流程图)。在编译一个.cu源文件时，当输入下面的指令后，执行程序就会将整个编译过程都打印出来。cuda的整个编译流程分成两个分支，分支1预处理device代码并进行编译生成cubin或者ptx，然后整合到二进制文件fatbin中，分支2预处理host代码，再和fatbin一起生成目标文件。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvcc --cuda test.cu -keep --dryrun
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;针对打印出来的每个过程中生成的文件做一个简单的分析，test.cu初始化一个runtime的硬件查询对象cudaDeviceProp，然后打印共享内存的大小。这里打印出来的内容只截取了每条命令的部分，主要给出中间的生成文件。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//打印信息
gcc -D__CUDA_ARCH__=200 -E -x c++ ... -o &quot;test.cpp1.ii&quot; &quot;test.cu&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面这一步是device代码预处理，它将一些定义好的枚举变量(例如cudaError)、struct(例如cuda的数据类型float4)、静态内联函数、extern “c++”和extern的函数、还重新定义了std命名空间、函数模板等内容写在main函数之前。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cudafe ... --gen_c_file_name &quot;test.cudafe1.c&quot; --gen_device_file_name &quot;test.cudafe1.gpu&quot; ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这一步test.cpp1.ii被cudafe切分成了c/c++ host代码和.gpu结尾的device代码，其中main函数还是在.c结尾的文件中。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcc -D__CUDA_ARCH__=200 -E -x c ... -o &quot;test.cpp2.i&quot; &quot;test.cudafe1.gpu&quot;
cudafe ... --gen_c_file_name &quot;test.cudafe2.c&quot; ... --gen_device_file_name &quot;test.cudafe2.gpu&quot;
gcc -D__CUDA_ARCH__=200 -E -x c ... -o &quot;test.cpp3.i&quot; &quot;test.cudafe2.gpu&quot;
filehash -s &quot;test.cpp3.i&quot; &amp;gt; &quot;test.hash&quot;
gcc -E -x c++ ... -o &quot;test.cpp4.ii&quot; &quot;test.cu&quot;
cudafe++ ... --gen_c_file_name &quot;test.cudafe1.cpp&quot; --stub_file_name &quot;test.cudafe1.stub.c&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面这段生成的test.cpp4.ii是在对host代码进行预处理，前面几行内容直接看文件有点看不出来，希望以后能够把这段补充起来。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cicc  -arch compute_20 ... --orig_src_file_name &quot;test.cu&quot; &quot;test.cpp3.i&quot; -o &quot;test.ptx&quot;
----------------
test.ptx:
	.version 4.1
	.target sm_20
	.address_size 64
----------------
test.cpp1.ii
	&quot;.../include/sm_11_atomic_functions.h&quot;
	...
	&quot;.../include/sm_12_atomic_functions.h&quot;
	...
	&quot;.../include/sm_35_atomic_functions.h&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;test.ptx文件中只记录了三行编译信息，可以看出对应了上面提到指定ptx的版本，以后可以根据这个版本再进行编译。实际上在host c++代码即每一个test.cpp*文件中，都包含了所有版本的SM头文件，从而可以调用每一种版本的函数进行编译。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ptxas -arch=sm_20 -m64  &quot;test.ptx&quot; -o &quot;test.sm_20.cubin&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这一步叫做PTX离线编译，主要的目的是为了将代码编译成一个确定的计算能力和SM版本，对应的版本信息保存在cubin中。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fatbinary --create=&quot;test.fatbin&quot; ... &quot;file=test.sm_20.cubin&quot; ... &quot;file=test.ptx&quot; --embedded-fatbin=&quot;test.fatbin.c&quot; --cuda
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这一步叫PTX在线编译，是将cubin和ptx中的版本信息保存在fatbin中。这里针对一个.cu源文件，调用系统的gcc/g++将host代码和fatbin编译成对应的目标文件。最后用c++编译器将目标文件链接起来生成可执行文件。&lt;/p&gt;

&lt;p&gt;####5. 小结&lt;/p&gt;

&lt;p&gt;上面谈到了关于cuda软件层面的一些东西，内容也不算多，主要是谈谈自己对这方面的理解，最后的编译流程，具体每一步到底做了什么还不够清晰，希望能有人共同探讨一下。&lt;/p&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://people.maths.ox.ac.uk/gilesm/cuda/lecs/lec1.pdf&quot;&gt;Lecture 1: an introduction to CUDA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://chenrudan.github.io/blog/2015/07/22/cudastream.html&quot;&gt;【GPU编程系列之三】cuda stream和event相关内容&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://developer.nvidia.com/gpu-accelerated-libraries&quot;&gt;GPU-Accelerated Libraries&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://docs.nvidia.com/cuda/thrust/#axzz42f5Uevuz&quot;&gt;Thrust&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#axzz43KUwL7GV&quot;&gt;The CUDA Compiler Driver NVCC&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Mar 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/03/06/introductionofgpusoftware.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/03/06/introductionofgpusoftware.html</guid>
        
        <category>programming languages</category>
        
      </item>
    
      <item>
        <title>#Yoshua Bengio#1月20日Yoshua Bengio在Quora上的问答记录</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2016年1月20日Bengio在Quora上做了一次面向网友的问答，回答了83个问题，这里记录一下感兴趣的几个问题。因为不是全部翻译过来而是选取了一些我觉得有价值的内容，所以有的地方可能会失去原来的感觉，每个问题都附上了原文链接可以点击查看。目录如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. 什么是深度学习？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. 深度学习在沿着什么样的方向发展？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. 2015年读过最好的机器学习paper？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;4. 距离了解为什么深度学习有效还有多远？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5&quot;&gt;5. 深度学习研究的领域有哪些？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6&quot;&gt;6. 为什么bengio认为当前机器学习算法限制在于它们需要足够多的数据来学习？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7&quot;&gt;7. 为什么无监督重要？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#8&quot;&gt;8. 怎么看待Are ML and Statistics Complementary?这篇论文，由于深度学习机器学习是不是离统计学变远了？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#9&quot;&gt;9. 神经网络是否有概率解释？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#10&quot;&gt;10. 除了重构输入，其他的无监督学习目标还有什么?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#11&quot;&gt;11. 算法是否可能从噪声中提取有用信息？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quora原文地址: &lt;a href=&quot;https://www.quora.com/profile/Yoshua-Bengio/session/37/?__snids__=1517801181&amp;amp;__nsrc__=1&amp;amp;__filter__=all&quot;&gt;Session with Yoshua Bengio&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;1&quot;&gt;1. 什么是深度学习？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-is-Deep-Learning-3&quot;&gt;Yoshua Bengio: What is Deep Learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;深度学习是在学习多层表达来帮助学习者完成感兴趣的任务，较高级别的表达会通过深层组合计算来获取更抽象的概念。&lt;/p&gt;

&lt;h4 id=&quot;2&quot;&gt;2. 深度学习在沿着什么样的方向发展？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Where-is-deep-learning-research-headed&quot;&gt;Yoshua Bengio: Where is deep learning research headed?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;研究即探索，并不知道什么能够成功而是需要探索很多条路，因此以下是一些比较有挑战的方向。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无监督学习非常重要，而我们现在做的并不正确&lt;/li&gt;
  &lt;li&gt;深度学习会继续从传统模式识别的任务扩增到全面的人工智能任务，包括symbolic manipulation, memory, planning and reasoning，从而能够更好的理解人类的自然语言和对话（通过图灵测试）。此外还扩增到了强化学习、控制学、机器人学等领域。&lt;/li&gt;
  &lt;li&gt;人工智能方面，需要更加深入的理解人类大脑并尝试找到通过机器学习来解释大脑运作的方法&lt;/li&gt;
  &lt;li&gt;改进极大似然方法，在复杂的高维空间中，并不是绝对需要学习最优的目标&lt;/li&gt;
  &lt;li&gt;计算能力（特别是硬件）的提升会让基于深度学习的AI获利，因为AI需要特别多关于这个世界的数据和知识，并基于这些来进行推理，然后需要大型网络来训练大量数据集。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3&quot;&gt;3. 2015年读过最好的机器学习paper？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-most-exciting-machine-learning-research-paper-you-read-in-2015&quot;&gt;Yoshua Bengio: What is the most exciting machine learning research paper you read in 2015?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1502.03167v3.pdf&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;，因为对训练大型结构有效，并且被人们认为是标准方法&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1507.02672.pdf&quot;&gt;Semi-Supervised Learning with Ladder Networks&lt;/a&gt;,让半监督重回人们视野，特别是去噪的自编码网络很有意思&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.2661v1.pdf&quot;&gt;Generative Adversarial Nets&lt;/a&gt;、&lt;a href=&quot;http://arxiv.org/pdf/1511.06434v2.pdf&quot;&gt;nsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt;等关于generative adversarial networks (GAN)，LAPGAN，DCGAN网络的论文，因为它们提出了图片生成模型，使得无监督在去年有快速的发展&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1506.03134.pdf&quot;&gt;Pointer Networks&lt;/a&gt;、&lt;a href=&quot;http://arxiv.org/pdf/1506.07503.pdf&quot;&gt;Attention-Based Models for Speech Recognition&lt;/a&gt;等基于内容的关注机制content-based attention mechanisms的文章，研究了机器翻译、神经图灵机和端到端的记忆网络等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4&quot;&gt;4. 距离了解为什么深度学习有效还有多远？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/How-far-along-are-we-in-understanding-why-deep-learning-works&quot;&gt;Yoshua Bengio: How far along are we in understanding why deep learning works?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;bengio认为我们已经有一定的了解基础了。我们知道表示学习、深层结构、卷积结构和递归结构的每个元素对应到某个函数的参数选择（或者说某种先验信息）。我们已经有理论解释为什么这些参数选择能够起到一个重要(指数)统计优势（即能够用更少的数据获得更高的正确性）&lt;a href=&quot;http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf&quot;&gt;On the Number of Linear Regions of Deep Neural Networks&lt;/a&gt;。我们知道为什么在训练深度网络中的优化问题并不像以前认为的那样难以解决，即绝大多数的局部最小值也是很好的解。&lt;a href=&quot;https://ganguli-gang.stanford.edu/pdf/14.SaddlePoint.NIPS.pdf&quot;&gt;Identifying and attacking the saddle point problem in high-dimensional non-convex optimization&lt;/a&gt;、&lt;a href=&quot;http://cims.nyu.edu/~achoroma/NonFlash/Papers/PAPER_AMMGY.pdf&quot;&gt;The Loss Surfaces of Multilayer Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;此外，在另外一个问题中，bengio认为当前的深度学习算法还有东西无法学到无法解决，但随着深度学习的逐渐发展，以后都能学到。&lt;/p&gt;

&lt;h4 id=&quot;5&quot;&gt;5. 深度学习研究的领域有哪些？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-are-the-open-research-areas-in-Deep-Learning&quot;&gt;Yoshua Bengio: What are the open  research areas in Deep Learning?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无监督学习会有很了不起的发展，其中包括
    &lt;ul&gt;
      &lt;li&gt;从自然图片和声音中生成清晰图像和语音的生成模型&lt;/li&gt;
      &lt;li&gt;当有label的数据集不干净时半监督学习能够发挥作用&lt;/li&gt;
      &lt;li&gt;学习从数据到空间独立变量的双向变换&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在模型中引入推理能力&lt;/li&gt;
  &lt;li&gt;大规模的自然语言理解和知识表达&lt;/li&gt;
  &lt;li&gt;多时间尺度的分层表达的模型&lt;/li&gt;
  &lt;li&gt;更好的理解某些优化问题，例如无监督学习或者有长期依赖的递归网络中产生的一些问题&lt;/li&gt;
  &lt;li&gt;训练模型将planning（能够执行what-if的情景）纳入学习过程，并且能够做决策&lt;/li&gt;
  &lt;li&gt;提升强化学习的规模&lt;/li&gt;
  &lt;li&gt;最大似然有一些缺点需要解决，例如在训练和测试条件下有错误匹配的问题&lt;/li&gt;
  &lt;li&gt;连接深度学习与生物学&lt;/li&gt;
  &lt;li&gt;加大对深度学习的理论理解（优化问题，表达和统计理论）&lt;/li&gt;
  &lt;li&gt;制造特殊的硬件，不仅仅能够离线训练模型，而且能训练更大的模型，使得模型能容纳更多信息&lt;/li&gt;
  &lt;li&gt;健康领域，存在的特殊问题是缺失数据，通过迁移学习来从其他小任务中采集数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6&quot;&gt;6. 为什么bengio认为当前机器学习算法限制在于它们需要足够多的数据来学习？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/You-have-said-that-the-main-limitation-of-current-machine-learning-algorithms-is-that-they-need-too-much-data-to-learn-Can-you-elaborate-on-that&quot;&gt;Yoshua Bengio: You have said that the main limitation of current machine learning algorithms is that they need too much data to learn. Can you elaborate on that?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;人一生下来的前两年，孩子们能看到的数据实际上是没有label的，在幼年时代孩子们所接触到的自然语言比我们用来训练系统的要少的多。这是因为人类能够更好的利用少量的数据，bengio认为人类建造了一个关于这个世界的内在模型并且能够获取一些构成因果关系因子。这样能够让我们在某种假设条件下预测会发生什么，即使这些假设条件跟我们经历过的完全不一样。我们可能从来没有经历过一次车祸，但是我们能够在脑子里将它模拟出来。&lt;/p&gt;

&lt;p&gt;(笔者：这个问题我保持怀疑态度，这里面举出来的例子个人认为并不合适，比如我们没有经历车祸但是我们看到过，所以我觉得模拟出来的也差不多是我们记忆中看到的车祸。而孩子们接触到的自然语言，也没有一个量化的标准说明它比网络用来训练的少)&lt;/p&gt;

&lt;h4 id=&quot;7&quot;&gt;7. 为什么无监督重要？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Why-is-unsupervised-learning-important-What-role-does-deep-learning-have-in-solving-it&quot;&gt;Yoshua Bengio: Why is unsupervised learning important? What role does deep learning have in solving it?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;深度学习是在学习表示，获取中间概念，特征和隐藏变量的统计依赖性。这些依赖是指监督学习中的输入到输出的依赖或者无监督学习中的观测变量之间的关系。监督学习会给电脑展示非常多的例子，这些例子中会出现某些概念，然后“教”电脑知道哪些概念对我们来说很重要。但是这并不是人们学习的方式，人们在接受新概念时不一定同时有label来告诉他们，例如成年人不会告诉孩子一张图中每个像素点是什么或者每张图中每个物体是什么，也不会告诉他们听到的句子中每个词的意思和语法结构。而从简单的观察中提取大量的信息是无监督正在做的。我们希望无监督能从少量的有lable数据发现所有的概念。&lt;/p&gt;

&lt;p&gt;而科学家们也会进行无监督学习，比如他们在观察这个世界，想出一些有解释能力的模型，通过观察现象来测试它们，然后持续尝试改进围绕着我们的世界的因果模型。&lt;/p&gt;

&lt;h4 id=&quot;8&quot;&gt;8. 怎么看待Are ML and Statistics Complementary?这篇论文，由于深度学习机器学习是不是拉开了与统计学的距离？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Whats-your-opinion-on-Max-Wellings-position-paper-Are-ML-and-Statistics-Complementary-Is-ML-is-moving-away-from-statistics-due-to-deep-learning&quot;&gt;Yoshua Bengio: What’s your opinion on Max Welling’s position paper “Are ML and Statistics Complementary”? Is ML is moving away from statistics due to deep learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Max总结了三个深度学习成功的原因：计算能力,大数据集,大模型。bengio加了第四条，powerful biases，它是指函数的参数选择，也可以认为是贝叶斯理论中的先验信息）。因为在深度学习中有很多的假设：假设有很多隐藏因子，假设有很多因子的组合，equivariance(?)和时间相干性假设（卷积网络），时间平稳的假设（递归网络）等。bengio同意max认为的解释数以亿计参数的意义是不现实的，但是能理解这些隐藏的或者显在的引入网络的先验信息。因此，仍然有很多关于深度学习的理论需要被挖掘，其中统计学会占有重要地位。&lt;/p&gt;

&lt;h4 id=&quot;9&quot;&gt;9. 神经网络有效是否有概率解释？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Can-neural-networks-have-a-purely-probabilistic-interpretation-for-why-they-work&quot;&gt;Yoshua Bengio: Can neural networks have a purely probabilistic interpretation for why they work?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;可以这样理解，有监督深度学习在学习条件概率，无监督学习方法在估计数据生成的联合分布。但是由于它既包括统计问题又包括优化问题，所以为什么深度学习有效不是一个概率问题。&lt;/p&gt;

&lt;h4 id=&quot;10&quot;&gt;10. 除了重构输入，其他的无监督学习目标还有什么?&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Apart-from-trying-to-reconstruct-the-input-as-in-Autoencoder-what-other-tasks-could-prove-useful-for-unsupervised-learning-of-deep-networks&quot;&gt;Yoshua Bengio: Apart from trying to reconstruct the input (as in Autoencoder), what other tasks could prove useful for unsupervised learning of deep networks?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在给定其他变量情况下预测一个变量（pseudolikelihood）&lt;/li&gt;
  &lt;li&gt;在给定其他变量情况下预测一小组变量(generalized pseudolikelihood)&lt;/li&gt;
  &lt;li&gt;在给定前一组变量情况下预测这一组中的某个变量(fully-visible Bayes nets, autoregressive nets, NADE, generative RNNs)&lt;/li&gt;
  &lt;li&gt;在给定一个被损坏的观测点情况下将原始干净的点还原出来(denoising)&lt;/li&gt;
  &lt;li&gt;预测输入是否来自数据产生的分布还是其他分布，类似概率分类器(Noise-Constrastive Estimation)&lt;/li&gt;
  &lt;li&gt;学习一个逆转函数&lt;/li&gt;
  &lt;li&gt;学习一个能够多次使用的复杂变换并收敛到接近数据产生的分布(Generative Stochastic Networks, generative denoising autoencoders, diffusion inversion = nonequilibrium thermodynamics)&lt;/li&gt;
  &lt;li&gt;学习产生不能被分类器区分的样本(GAN = generative adversarial networks)&lt;/li&gt;
  &lt;li&gt;极大化某个概率模型的似然函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;11&quot;&gt;11. 算法是否可能从噪声中提取有用信息？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Do-you-think-that-its-possible-for-algorithms-to-extract-useful-information-from-what-is-generally-disacarded-as-noise&quot;&gt;Yoshua Bengio: Do you think that it’s possible for algorithms to extract useful information from what is generally disacarded as noise?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;只有当噪声中真正存在某种潜在的统计结构时才可以。例如猫在听人讲话觉得是噪声，人听外语也会觉得是噪声。因此当构建合适的模型让噪声也变得结构化才能提取有用信息。&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Jan 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/01/20/bengiosession.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/01/20/bengiosession.html</guid>
        
        <category>resource</category>
        
      </item>
    
      <item>
        <title>【机器学习算法系列之二】浅析Logistic Regression</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是受rickjin老师的启发，谈谈关于logistic regression的一些内容，虽然已经有珠玉在前，但还是做一下自己的总结。在查找资料的过程中，越看越觉得lr实在是博大精深，囊括的内容太多太多了，本文只能浅显的提到某些方面。文章的内容如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. 起源&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. 模型介绍与公式推导&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#2.1&quot;&gt;2.1 Logistic Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2.2&quot;&gt;2.2 Binomial logistic regression model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. 解法&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#3.1&quot;&gt;3.1 梯度下降法&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.2&quot;&gt;3.2 牛顿法&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.3&quot;&gt;3.3 BFGS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;4. 正则化&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#4.1&quot;&gt;4.1 过拟合&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4.2&quot;&gt;4.2 正则化的两种方法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5&quot;&gt;5. 逻辑回归与其他模型关系&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#5.1&quot;&gt;5.1 逻辑回归与线性回归&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.2&quot;&gt;5.2 逻辑回归与最大熵&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.3&quot;&gt;5.3 逻辑回归与svm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.4&quot;&gt;5.4 逻辑回归与朴素贝叶斯&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.5&quot;&gt;5.5 逻辑回归与能量函数&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6&quot;&gt;6. 并行化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7&quot;&gt;7. 小结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#8&quot;&gt;8. 引用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1&quot;&gt;1. 起源&lt;/h3&gt;

&lt;p&gt;logistic regression的起源主要分为几个阶段，从开始想到logistic这个词，到发现logistic function，再推导出logit function，最后才命名logistic regression。这些过程都是大量的研究者们共同努力发现的，只是在历史的长河中，很多人被渐渐遗忘了。&lt;/p&gt;

&lt;p&gt;logistic起源于对人口数量增长情况的研究，最重要的工作是Pierre François Verhulst在1838年提出了对人口增长的公式描述(这人是个比利时人，写的文章是法语的，一个字都看不懂，下面的内容都是看了一篇将研究人口数量增长发展历程的书[1]才知道的…)，他博士毕业于根特大学的数学系，是个数学教授和人口学家。在1835年Verhulst的同乡人Adolphe Quetelet发表了一篇关于讨论人口增长的文章，文中认为人口不可能一直是几何(指数)增长，而会被与增长速度平方成比例的一种阻力而影响，但是这篇论文只有猜想没有数学理论基础，却极大的启发了Verhulst。因此在1838年Verhulst发表了关于人口数量增长的论文，就是在这篇论文里面他推导出了logistic equation，文章中谈到一个重要观点，随着时间的增加，一个国家的大小（我理解为资源）和这个国家人们的生育能力限制了人口的增长，人口数量会渐渐趋近一个稳定值。厉害的是他将这个过程用公式给描述出来了，他从人口数量增长的速度公式入手，即人口数量$P(t)$对时间t的导数:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial P}{\partial t} = rP(1-\frac{P}{K})&lt;/script&gt;

&lt;p&gt;其中$K$就是他认为人口数量稳定的值，当$P(t)$远小于$K$时，求导公式后一项约等于0，那么就变成了$\frac{\partial P}{\partial t} \simeq  rP$，这个阶段人口增长速度与人口数量和一个常数的乘积成正比，并且在渐渐变大。然后对这个式子求解一阶线性微分方程得到$P(t)\simeq P(0)e^{rt}$。当$P(t)$接近$K$时，人口增长速度开始渐渐变小，同样求解二阶微分方程(论文中是将二阶转化成一阶求解)，然后将二者整合在一起得到最初的形式。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(t) = \frac{P(0)e^{rt}}{1+P(0)(e^{rt}-1)/K}&lt;/script&gt;

&lt;p&gt;他将法国英国等过十几年的人口实际数据拿来跟这个公式对比之后发现确实拟合的很不错。但他当时并没有那么多年的数据，下图1是在他过世以后人们总结的300年来的人口增长分布，可以看到非常漂亮的拟合了logisitc分布的累积分布函数走势。但是当时这个公式并没有名字，直到1845年他发表了另外一篇重要文章[2]，他给这个公式起了一个名字——”logistic”，此外在这篇文章中，他发现在$P(t)&amp;lt;K/2$的时候，$P(t)$呈凸增长趋势，在$P(t)&amp;gt;K/2$时$P(t)$呈凹增长(通过求二阶导来分析，这里略)。这个增长的趋势类似logistic分布的概率密度函数。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr1.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 比利时的人口增长数量图&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;然而在后来的几十年内人们都没有意识到这个工作的重要性，很多人都独立的研究出了这个增长现象，直到1922年一个叫做Raymond Pearl的人口学家注意到Verhulst在1838年就已经提出了这个现象和公式，并在他的文章中也使用了logistic function来称呼它，并且沿用至今。在1920年Pearl[3]在研究美国人口增长规律时提出了另外一种表示logistic function的方法。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \frac{be^{ax}}{1+ce^{ax}}&lt;/script&gt;

&lt;p&gt;基于这个表达式，Joseph Berkson在1944年提出了logit function，$logit = In(\frac{1-Q}{Q})$，假如$Q = \frac{1}{1+e^{a-bx}}$，结果就是$logit=a-bx$。&lt;/p&gt;

&lt;p&gt;后来，在1958年David Cox提出了logistic regression[4]。他的文章是为了解决这样一个问题，有一组取值为0，1的观测值，它们的取值$Y_i$依赖于一些独立变量$x_i$， 当$Y_i=1$时对应的概率为$\theta_i=pr(Y_i = 1)$。由于$\theta_i$限制在[0,1]之间，因此假设$\theta_i$与$x_i$的关系符合logit function，即$logit\theta_i \equiv log{\frac{\theta_i}{1-\theta_i}} = \alpha + \beta x_i$，文章主要在分析如何求解里面的参数$\beta$，这里就不提了。由于用到了logistic function，而这个问题本身又个回归问题(建立观测值与独立变量之间的关系)，因而它被称呼为logistic regression。&lt;/p&gt;

&lt;p&gt;貌似Cox在这篇文章中并不是刻意提出logistic regression，但确实这个词第一次出现就是在这篇文章中，虽然Cox之前已经有很多人做过这方面的研究了，但是他们没给个名字，因此Cox成了提出logistic regression的人。这个故事告诉我们一个道理，无论是发文章还是写软件一定要取一个言简意赅又好听又好记的名字…&lt;/p&gt;

&lt;p&gt;以上是逻辑回归的历史发展中比较有代表性的几件事(我认为的…还有好多论文没时间细看…)，J.S Cramer[5]在他的文章中有更加详细的讨论。它是由数学家对人口发展规律研究得出，后来又被应用到了微生物生长情况的研究，后来又被应用解决经济学相关问题，直到发展到今天作为一个非常重要的算法而存在于各行各业。逻辑回归作为Regression Analysis的一个分支，它实际上还受到很多Regression Analysis相关技术的启发，例如Berkson就是基于probit function提出的logit function。光它的起源到应用就能写一本书出来了，难怪rickjin老师说lr其实非常非常复杂…&lt;/p&gt;

&lt;h3 id=&quot;2&quot;&gt;2.模型介绍与公式推导&lt;/h3&gt;

&lt;p&gt;上面说过了逻辑斯蒂回归的起源，下面讨论一下完整的模型，首先介绍一下何为逻辑斯蒂分布，再由逻辑斯蒂分布推出逻辑回归。&lt;/p&gt;

&lt;h4 id=&quot;2.1&quot;&gt;2.1 Logistic Distribution&lt;/h4&gt;

&lt;p&gt;随机变量X服从逻辑斯蒂分布，即X的累积分布函数为上文提到过的logistic function。对分布函数求导得到了概率密度函数。公式如下，参数影响参考图2(图来自维基百科，它的参数s就是统计学习方法上的$\gamma$)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(x) = P(X \leqslant x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = F&#39;(x) = \frac{e^{-(x-\mu)/\gamma}} { \gamma (1+e^{-(x-\mu)/\gamma})^2 }&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr3.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 不同参数对logistic分布的影响&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;可以看到$\mu$影响的是中心对称点的位置，$\gamma$越小中心点附近增长的速度越快。而常常在深度学习中用到的非线性变换sigmoid函数是逻辑斯蒂分布的$\gamma=1,\mu=0$的特殊形式。&lt;/p&gt;

&lt;h4 id=&quot;2.2&quot;&gt;2.2 Binomial logistic regression model&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr2.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图3 数据示例&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。如图3所示，有一些属于两个类的数据，目标是判断圆圈属于哪一类。也就是说逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。假设已经存在这样一个边界，针对于图中这种线性可分的情况，这条边界是
输入特征向量的线性组合，假设输入的特征向量为$x\in R^n$(图中输入向量为二维)，$Y$取值为0，1。那么决策边界可以表示为$w_1x_1+w_2x_2+b=0$，假如存在一个例子使得$h_w(x) = w_1x_1+w_2x_2+b &amp;gt; 0$，那么可以判断它类别为1，这个过程实际上是感知机，即只通过决策函数的符号来判断属于哪一类。而逻辑回归需要再进一步，它要找到分类概率$P(Y=1)$与输入向量$x$的直接关系，通过比较概率值来判断类别，也就是上文中的logit function，因此产生了逻辑回归，将决策函数的输出值映射到概率值上。最基础的二分类问题，对逻辑回归而言就是二项逻辑斯蒂回归，从而某组输入向量$x$下导致产生不同类的概率为:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y=1|x)=\frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}} \:\:\:\:\:\:\:\:\:(1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y=0|x)=\frac{1}{1+e^{w\cdot x+b}} \:\:\:\:\:\:\:\:\:(2)&lt;/script&gt;

&lt;p&gt;其中$w$称为权重，$b$称为偏置，其中的$w\cdot x+b$看成对$x$的线性函数。然后对比上面两个概率值，概率值大的就是x对应的类。有时候为了书写方便，会将$b$写入$w$，即$w=(w_0, w_1, …, w_n)$其中$w_0=b$，并取$x_0 = 1$。又已知一个事件发生的几率odds是指该事件发生与不发生的概率比值，二分类情况下即$\frac {P(Y=1|x)}{P(Y=0|x)} = \frac {P(Y=1|x)}{1-P(Y=1|x)}$。取odds的对数就是上面提到的logit function，$logit(P(Y=1|x)) = log\frac {P(Y=1|x)}{1-P(Y=1|x)} = w\cdot x$。从而可以得到一种对逻辑回归的定义，&lt;strong&gt;&lt;em&gt;输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型&lt;/em&gt;&lt;/strong&gt;(李航.《统计机器学习》)。而直接考察公式1可以得到另一种对逻辑回归的定义，&lt;strong&gt;&lt;em&gt;线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型&lt;/em&gt;&lt;/strong&gt;(李航.《统计机器学习》)。因此逻辑回归的思路是，先拟合决策边界(这里的决策边界不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。这里有个非常棒的博文[6]推荐，阐述了逻辑回归的思路。&lt;/p&gt;

&lt;p&gt;有了上面的分类概率，就可以建立似然函数，通过极大似然估计法来确定模型的参数。设$P(Y=1|x)=h_w (x)$，似然函数为$\prod [h_w(x_i)]^{y_i}[1-h_w(x_i)]^{(1-y_i)}$，对数似然函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(w) = \sum _{i=1}^{N}logP(y_i|x_i;w) = \sum_{i=1}^{N}[y_ilog h_w (x_i) +(1-y_i)log(1-h_w(x_i))] \:\:\:\:\:\:\:\:\:(3)&lt;/script&gt;

&lt;h3 id=&quot;3&quot;&gt;3.解法&lt;/h3&gt;

&lt;p&gt;优化逻辑回归的方法有非常多[7]，有python的不同实现[8]，这里只谈谈梯度下降，牛顿法和BFGS。优化的主要目标是找到一个方向，参数朝这个方向移动之后使得似然函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min J(w) = min \{-\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))]\} \:\:\:\:\:\:\:\:\:(4)&lt;/script&gt;

&lt;p&gt;先把$J(w)$对$w_j$的一阶二阶偏导求出来，且分别用$g$和$H$表示。$g$是梯度向量，$H$是海森矩阵。这里只考虑一个实例$y_i$产生的似然函数对一个参数$w_j$的偏导。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_j = \frac{\partial J(w)} {\partial w_j} = \frac{y^{(i)}}{h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))(-x_{j}^{(i)})+(1-y^{(i)})\frac {1}{1-h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}=(y^{(i)}-h_w(x^{(i)}))x^{(i)}   \:\:\:\:\:\:\:\:\:(5)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H_{mn} = \frac {\partial^2 J(w)} {\partial w_m \partial w_n} =h_w(x^{(i)})(1-h_w(x^{(i)}))x^{(i)}_mx^{(i)}_n \:\:\:\:\:\:\:\:\:(6)&lt;/script&gt;

&lt;p&gt;这几种方法一般都是采用迭代的方式来逐步逼近极小值，需要给定参数$w_0$作为起点，并且需要一个阈值$\epsilon$来判断迭代何时停止。&lt;/p&gt;

&lt;h4 id=&quot;3.1&quot;&gt;3.1 梯度下降法&lt;/h4&gt;

&lt;p&gt;梯度下降是通过$J(w)$对$w$的一阶导数来找下降方向，并且以迭代的方式来更新参数，更新方式为$w_j^{k+1} = w_j^k + \alpha g_j$，$k$为迭代次数。每次更新参数后，可以通过比较$||J(w^{k+1})-J(w^k)||$或者$||w^{k+1}-w^k||$与某个阈值$\epsilon $大小的方式来停止迭代，即比阈值小就停止。&lt;/p&gt;

&lt;h4 id=&quot;3.2&quot;&gt;3.2 牛顿法&lt;/h4&gt;

&lt;p&gt;牛顿法的基本思路是，&lt;strong&gt;&lt;em&gt;在现有极小点估计值的附近对f(x)做二阶泰勒展开，进而找到极小点的下一个估计值&lt;/em&gt;&lt;/strong&gt;[9]。假设$w^k$为当前的极小值估计值，那么有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varphi (w) = J(w^k) + J&#39;(w^k)(w-w^k)+\frac{1}{2}J&#39;&#39;(w^k)(w-w^k)^2  \:\:\:\:\:\:\:\:\:(7)&lt;/script&gt;

&lt;p&gt;然后令$\varphi’(w)=0$，得到了$w=w^k-\frac{J’(w^k)}{J&#39;&#39;(w^k)}$。因此有迭代更新式，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^{k+1} = w^k - \frac{J&#39;(w^k)}{J&#39;&#39;(w^k)} = w^k - H_k^{-1}\cdot g_k \:\:\:\:\:\:\:\:\:(8)&lt;/script&gt;

&lt;p&gt;此方法中也需要一个阈值$\epsilon$，当$||g_k|| &amp;lt; epsilon$时停止迭代。此外，这个方法需要目标函数是二阶连续可微的，本文中的$J(w)$是符合要求的。&lt;/p&gt;

&lt;h4 id=&quot;3.3&quot;&gt;3.3 BFGS&lt;/h4&gt;

&lt;p&gt;由于牛顿法中需要求解二阶偏导，这个计算量会比较大，而且有时目标函数求出的海森矩阵无法保持正定，因此提出了拟牛顿法。拟牛顿法是一些算法的总称，它们的目标是通过某种方式来近似表示森海矩阵(或者它的逆矩阵)。例如BFGS就是一种拟牛顿法，它是由四个发明人的首字母组合命名，是求解无约束非线性优化问题最常用的方法之一。目标是用迭代的方式逼近海森矩阵$H$，假设这个逼近值为$B^k\approx H^k$，那么希望通过计算$B^{k+1} = B^k + \Delta B^k$能够达到目的。并且假设$\Delta B^k = \alpha uu^T + \beta vv^T$，而由3.2可知，$\Delta w = w^{k+1}-w^k = (H^{-1})^{k+1}(g^{k+1}-g^k) = (H^{-1})^k\Delta g$，将$B^{k+1}$的更新式代入，可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta g = B^k\Delta g + (\alpha u^T\Delta w)u + (\beta v^T\Delta w)v  \:\:\:\:\:\:\:\:\:(9)&lt;/script&gt;

&lt;p&gt;此处，直接令$\alpha u^T\Delta w=1$、$\beta v^T\Delta w=-1$、$u=\Delta g$和$v=B^k\Delta w$，那么可以求得$\alpha = \frac {1}{(\Delta g)^T\Delta w}$和$\beta = -\frac{1}{(\Delta w)^TB^k\Delta w}$。从而再代入求$\Delta B^k$的式子就可以得到更新的式子&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta B^k = \frac {\Delta g(\Delta g)^T}{(\Delta g)^T\Delta w} - \frac {B^k\Delta w(\Delta w)^TB^k}{(\Delta w)^TB^k\Delta w}  \:\:\:\:\:\:\:\:\:(10)&lt;/script&gt;

&lt;p&gt;这里还会对(10)进行变换，通过Sherman-Morrison公式直接求出$(B^{-1})^{k+1}$与$(B^{-1})^k$，用$D^{k+1}$和$D^k$来表。更新公式变成了&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D^{k+1} = (I-\frac {\Delta w (\Delta g)^T}{(\Delta g)^T\Delta w}) D^k (I - \frac {\Delta g (\Delta w)^T}{(\Delta g)^T\Delta w}) + \frac {\Delta w (\Delta w)^T}{(\Delta g)^T\Delta w} \:\:\:\:\:\:\:\:\:(11)&lt;/script&gt;

&lt;p&gt;用BFGS来更新参数的流程如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;确定改变量，$(\Delta w)^k= -D^k\cdot g^k$&lt;/li&gt;
  &lt;li&gt;更新参数，$w^{k+1} = w^k + \lambda (\Delta w)^k$&lt;/li&gt;
  &lt;li&gt;求出$\Delta g = g^{k+1} -g^k$&lt;/li&gt;
  &lt;li&gt;由(11)求出$D^{k+1}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;式子的系数$\lambda=argmin J(w^k + \lambda (\Delta w)^k)$，即在求得下降方向上来从很多值中搜索最优的下降大小，这里我觉得可以用学习率替代。因此，这个更新方法跟牛顿法的区别是，它是在更新参数$w$之后更新一下近似森海矩阵的值，而牛顿法是在更新$w$之前完全的计算一遍森海矩阵。还有一种从计算上改进BFGS的方法称为L-BFGS，不直接存储森海矩阵，而是通过存储计算过程中产生的部分$\Delta w(g)_{k-m+1,k-m+2,…,k}$，从而减少了参数存储所需空间。&lt;/p&gt;

&lt;h3 id=&quot;4&quot;&gt;4.正则化&lt;/h3&gt;

&lt;p&gt;正则化不是只有逻辑回归存在，它是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合，在谈正则化之前先聊聊什么是过拟合。&lt;/p&gt;

&lt;h4 id=&quot;4.1&quot;&gt;4.1 过拟合&lt;/h4&gt;

&lt;p&gt;之前的模型介绍和算法求解可以通过训练数据集(图2中的三角形和星形)将分类模型训练好，从而可以预测一个新数据(例如图2中的粉色圆圈)的分类，这种对新数据进行预测的能力称为泛化能力。而对新数据预测的结果不好就是泛化能力差，一般来说泛化能力差都是由于发生了过拟合现象。过拟合现象是指对训练数据预测很好但是对未知数据预测不行的现象，通常都是因为模型过于复杂，或者训练数据太少。即当$\frac{complexity\: of \:the \:model}{training \:set \:size}$比值太大的情况下会发生过拟合。模型复杂体现在两个方面，一是参数过多，二是参数值过大。参数值过大会导致导数非常大，那么拟合的函数波动就会非常大，即下图所示，从左到右分别是欠拟合、拟合和过拟合。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr4.jpg&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图4 同样数据下欠拟合，拟合和过拟合&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;在模型过于复杂的情况下，模型会学习到很多特征，从而导致可能把所有训练样本都拟合到，就像上图中一样，拟合的曲线将每一个点都正确的分类了。举个例子，假如要预测一个房子是贵还是便宜，房子的面积和所属的地区是有用的特征，但假如训练集中刚好所有贵的房子都是开发商A开发，便宜的都是开发商B开发，那么当模型变复杂能学习到的特征变多之后，房子是哪个开发商的会被模型认为是个有用特征，但是实际上这点不能成为判断的标准，这个现象就是过拟合。因此在这个例子中可以看到，解决的方法有两个，一个是减少学习的特征不让模型学到开发商的特征，一是增加训练集，让训练集有贵房子是B开发的样本。&lt;/p&gt;

&lt;p&gt;从而，解决过拟合可以从两个方面入手，一是减少模型复杂度，一是增加训练集个数。而正则化就是减少模型复杂度的一个方法。&lt;/p&gt;

&lt;h4 id=&quot;4.2&quot;&gt;4.2 正则化的两种方法&lt;/h4&gt;

&lt;p&gt;由于模型的参数个数一般是由人为指定和调节的，所以正则化常常是用来限制模型参数值不要过大，也被称为惩罚项。一般是在目标函数(经验风险)中加上一个正则化项$\Phi(w)$即&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(w) = -\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))] + \lambda \Phi(w) \:\:\:\:\:\:\:\:\:(12)&lt;/script&gt;

&lt;p&gt;而这个正则化项一般会采用L1范数或者L2范数。其形式分别为$\Phi (w)=||x||_1$和$\Phi (w)=||x||_2 $。&lt;/p&gt;

&lt;p&gt;首先针对L1范数$\phi (w) = |w|$，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \Phi(w)}{\partial w_j} = \left\{\begin{matrix} 1 &amp; w_j&gt;0\\  -1 &amp; w_j&lt;0  \end{matrix}\right. \:\:\:\:\:\:\:\:\:(13) %]]&gt;&lt;/script&gt;

&lt;p&gt;从而导致的参数$w_j$减去了学习率与(13)式的乘积，因此当$w_j$大于0的时候，$w_j$会减去一个正数，导致$w_j$减小，而当$w_j$小于0的时候，$w_j$会减去一个负数，导致$w_j$又变大，因此这个正则项会导致参数$w_j$取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为 Lasso regularization。&lt;/p&gt;

&lt;p&gt;然后针对L2范数$\phi(w) = \sum_{j=1}^{n}w_j^2$，同样对它求导，得到梯度变化为$\frac{\partial \Phi(w)}{\partial w_j} = 2w_j$(一般会用$\frac{\lambda}{2}$来把这个系数2给消掉)。同样的更新之后使得$w_j$的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。&lt;/p&gt;

&lt;p&gt;需要注意的是，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而(12)式中的$\lambda$也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，$\lambda$越大，对参数值惩罚越大，泛化能力越好。&lt;/p&gt;

&lt;p&gt;此外，从贝叶斯的角度而言，正则化项实际上是给了模型一个先验知识，L2正则相当于添加了一个均值为0协方差为$1/\lambda$的高斯分布先验(将L2正则表示为$\frac{\lambda}{2}w^Tw$)，当$\lambda$为0，即不添加正则项，那么可以看成协方差是无穷大，$w$可以不受控制变成任意大。当$\lambda$越大，即协方差越小，那么参数值的取值方差会变小，模型会趋向于稳定(参考[10]最高票答案)。&lt;/p&gt;

&lt;h3 id=&quot;5&quot;&gt;5. 逻辑回归与其他模型的关系&lt;/h3&gt;

&lt;h4 id=&quot;5.1&quot;&gt;5.1 逻辑回归与线性回归&lt;/h4&gt;

&lt;p&gt;在谈两者关系之前，需要讨论的是，逻辑回归中使用到的sigmoid函数到底起到了什么作用。下图的例子中，需要判断肿瘤是恶性还是良性，其中横轴是肿瘤大小，纵轴是线性函数$h_w(x)=w^Tx+b$的取值，因此在左图中可以根据训练集(图中的红叉)找到一条决策边界，并且以0.5作为阈值，将$h_w(x) \geqslant 0.5$情况预测为恶性肿瘤，这种方式在这种数据比较集中的情况下好用，但是一旦出现如右图中的离群点，它会导致学习到的线性函数偏离(它产生的权重改变量会比较大)，从而原先设定的0.5阈值就不好用了，此时要么调整阈值要么调整线性函数。如果我们调节阈值，在这个图里线性函数取值看起来是0～1，但是在其他情况下可能就是从$-\infty$到$\infty$，所以阈值的大小很难确定，假如能够把$w^Tx+b$的值变换到一个能控制的范围那么阈值就好确定了，所以找到了sigmoid函数，将$w^Tx+b$值映射到了(0,1)，并且解释成概率。而如果调节线性函数，那么最需要的是减少离群点的影响，离群点往往会导致比较大的$|w^Tx+b|$值，通过sigmoid函数刚好能够削弱这种类型值的影响，这种值经过sigmoid之后接近0或者1，从而对$w_j$的偏导数为$h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}$，无论接近0还是1这个导数都是非常小的。因此可以说sigmoid在逻辑回归中起到了两个作用，一是将线性函数的结果映射到了(0,1)，一是减少了离群点的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr5.jpg&quot; alt=&quot;1&quot; height=&quot;55%&quot; width=&quot;55%&quot; hspace=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图5 良性恶性肿瘤分类&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;有了上面的分析基础，再来看看逻辑回归和线性回归的关系(线性回归我这里就不展开说了，不清楚的可以看看[11])，有的人觉得逻辑回归本质上就是线性回归，它们俩都要学习一个线性函数，逻辑回归无非是多加了一层函数映射，但是我对线性回归的理解是在拟合输入向量x的分布，而逻辑回归中的线性函数是在拟合决策边界，它们的目标是不一样的。所以我不觉得逻辑回归比线性回归好，它们俩要解决的问题不一样。但它们都可以用一个东西来概括，那就是广义线性模型GLM(Generalized linear models)[12]。先介绍何为指数簇(exponential family)，当某个随机变量的概率分布可以表示为$p(y;\eta )=b(y)exp(\eta^TT(y)-a(\eta))$时就可以说它属于指数簇，通过调整$\eta$可以获得不同的分布。对应于线性回归与逻辑回归的高斯分布与伯努利分布就是属于指数簇的，例如取$T(y)=y$、$a(\eta)=-log(1-\phi) = log(1+e^\eta)$以及$b(y)=1$代入上式就可以得到逻辑回归的损失函数$J(w) = \frac {1}{2} \sum_{i=1}^{m} (h_w(x^{(i)})-y^{(i)})^2$。&lt;/p&gt;

&lt;p&gt;GLM需要满足下面三个条件。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在给定观测值x和参数w情况下，输出y服从参数为$\eta$的指数簇分布&lt;/li&gt;
  &lt;li&gt;预测的值$h_w(x) = E[y|x]$&lt;/li&gt;
  &lt;li&gt;$\eta = w^Tx$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，选择合适的参数就能分析出线性回归和逻辑回归都是GLM的一种特例，有时会看到有的人会从GLM出发将逻辑回归的公式给推导出来。总之，线性回归和逻辑回归是属于同一种模型，但是它们要解决的问题不一样，前者解决的是regression问题，后者解决的是classification问题，前者的输出是连续值，后者的输出是离散值，而且前者的损失函数是输出y的高斯分布，后者损失函数是输出的伯努利分布。&lt;/p&gt;

&lt;h4 id=&quot;5.2&quot;&gt;5.2 逻辑回归与最大熵&lt;/h4&gt;

&lt;p&gt;最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。为了证明最大熵模型跟逻辑回归的关系，那么就要证明两者求出来的模型是一样的，即求出来的h(x)的形式应该是一致的。由于最大熵是通过将有约束条件的条件极值问题转变成拉格朗日对偶问题来求解，模型的熵为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\sum_{v=1}^k\sum_{i=1}^m h(x^{(i)})_v log(h(x^{(i)})_v) \:\:\:\:\:\:\:\:\:(14)&lt;/script&gt;

&lt;p&gt;并假设约束条件如下，其中$v,u$是输出类别的index，$j$是对应输入向量$x$的index，$A(u,y^{(i)})$是指示函数，两个值相等输出1，其他输出0[13]。而第三个约束是通过令公式(5)等于0得来的，它的意义是参数$w_{u,j}$最好的取值是让每一个样本i对应$h(x^{(i)})_u$的行为接近指示函数$A(u,y^{(i)})$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{cases}
h(x)_v\geqslant 0 &amp; \text{ always } \\ 
\sum_{v=1}^k h(x)_v = 1 &amp; \text{ always } \\ 
\sum_{i=1}^m h(x^{(i)})_u x^{(i)}_j = \sum_{i=1}^m A(u,y^{(i)})x^{(i)}_j &amp; \text{ for all } \: u, j
\end{cases} \:\:\:\:\:\:\:\:\:(15) %]]&gt;&lt;/script&gt;

&lt;p&gt;通过约束条件(15)可以直接推导出softmax的公式。基于这一点，再回过头来看《统计学习方法》上的约束条件，如果假设$P(y|x) = h(x)$，公式左边的$f(x,y)$实际上取值一直为1，那么这两个约束条件实际上是一样的。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{x,y} \widetilde{P(x)}P(y|x)f(x,y) = \sum_{x,y} \widetilde{P(x,y)}f(x,y) \:\:\:\:\:\:\:\:\:(16)&lt;/script&gt;

&lt;p&gt;因此，可以这样说，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。此外，最大熵与逻辑回归都称为对数线性模型(log linear model)。&lt;/p&gt;

&lt;h4 id=&quot;5.3&quot;&gt;5.3 逻辑回归与svm&lt;/h4&gt;

&lt;p&gt;逻辑回归和svm作为经典的分类算法，被放在一起讨论的次数特别多，知乎和Quora上每种意见都非常有意思都从不同角度有分析，建议都可以看看[14][15][16]。这里只讨论一些我赞同的观点。要是不清楚svm的由来，建议看JerryLead的系列博客[17]，我这里就不提了。&lt;/p&gt;

&lt;p&gt;相同点:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;都是分类算法&lt;/li&gt;
  &lt;li&gt;都是监督学习算法&lt;/li&gt;
  &lt;li&gt;都是判别模型&lt;/li&gt;
  &lt;li&gt;都能通过核函数方法针对非线性情况分类&lt;/li&gt;
  &lt;li&gt;目标都是找一个分类超平面&lt;/li&gt;
  &lt;li&gt;都能减少离群点的影响&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不同点:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss&lt;/li&gt;
  &lt;li&gt;逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。&lt;/li&gt;
  &lt;li&gt;逻辑回归对概率建模，svm对分类超平面建模&lt;/li&gt;
  &lt;li&gt;逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有&lt;/li&gt;
  &lt;li&gt;逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响&lt;/li&gt;
  &lt;li&gt;逻辑回归是统计方法，svm是几何方法&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;5.4&quot;&gt;5.4 逻辑回归与朴素贝叶斯&lt;/h4&gt;

&lt;p&gt;这两个算法有一些相似之处，并且在对比判别模型和生成模型，它们作为典型的分类算法经常被提及，因此这里也做一个小小的总结。&lt;/p&gt;

&lt;p&gt;相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率$P(X|Y=c_k)$服从高斯分布时Gaussian Naive Bayes，它计算出来的$P(Y=1|X)$形式跟逻辑回归是一样的[18]。&lt;/p&gt;

&lt;p&gt;不同的地方在于，逻辑回归为判别模型求的是$p(y|x)$，朴素贝叶斯为生成模型求的是$p(x,y)$。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率$P(X|Y=c_k)$是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。&lt;/p&gt;

&lt;h4 id=&quot;5.5&quot;&gt;5.5 逻辑回归与能量模型&lt;/h4&gt;

&lt;p&gt;(3月3日补充)&lt;/p&gt;

&lt;p&gt;基于能量的模型不是一个具体的算法，而是一种框架思想，它认为输入输出变量之间的依赖关系用一个值表示$E(x,y)$，这个值称为能量，对关系建模的这个函数叫能量函数 。如果在保持输入变量不变的情况下，对应正确输出时能量低，对应错误输出时能量高，那么这个模型就是有用的。&lt;/p&gt;

&lt;p&gt;因此当给定了训练集S，能量模型的构造和训练由四部分组成[19]:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;有合适的能量函数$E(W,Y,X)$&lt;/li&gt;
  &lt;li&gt;inference算法，针对一个给定的输入变量X和能量函数形式，找到一个Y值使得能量最小，即$Y^* = argmin_{Y\in y}E(W,Y,X) $&lt;/li&gt;
  &lt;li&gt;有loss函数$L(W,S)$，用来衡量在训练集S下能量函数的好坏&lt;/li&gt;
  &lt;li&gt;learning算法，用来找合适的参数W，在一系列能量函数中选择让损失函数最小化的能量函数。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看出13步通过选择不同的能量函数和损失函数来构造不同的模型，24步是如何训练这样的一个模型。&lt;/p&gt;

&lt;p&gt;因此当我们假设要解决二分类问题时，y取值为-1和1，如果假设能量函数为$E(W,Y,X)=-YG_w(X)=W^TX$，损失函数采用negative log-likelihood loss，那么可以求得损失函数具体形式为$L_{nll}(W,S)=\frac{1}{P}\sum_{i=1}^{P}log(1+exp(-2Y^iW^TX))$，这个形式与把(1)(2)代入公式(3)后得到的损失函数公式是一致的，说明这种组合下，产生的算法就是逻辑回归。因此逻辑回归是能量模型的一种特例。&lt;/p&gt;

&lt;p&gt;而此处，同样针对二分类问题，假如能量函数保持不变，损失函数采用hinge loss，并加上一个正则化项，那么就能够推导出SVM的损失函数表达式(SVM的核函数体现在能量函数中，这里为了方便解释没有具体展开说)。这也是为什么说逻辑回归与svm最本质的区别就是损失函数不同。&lt;/p&gt;

&lt;h3 id=&quot;6&quot;&gt;6. 并行化&lt;/h3&gt;

&lt;p&gt;由于找不到特别多的并行化资料，这里就分析一下博主冯扬给出的实现[20]。实际上逻辑回归的并行化最主要的目标就是计算梯度。将目标的label变为-1和1，那么梯度公式可以整合在一起变成$\sum_{i=1}^M(\frac{1}{1+exp(y^{(i)}w^Tx^{(i)})}-1)y^{(i)}x^{(i)}$，梯度计算里面最主要的就是矩阵乘法，一般的做法都是想办法将矩阵切割成大小合适的块。针对二分类，现在有M个样本，N个特征，假如有m*n个计算节点，并且将计算节点排列成m行n列，那么每个节点分配M/m个样本，N/n个特征，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr6.jpg&quot; alt=&quot;1&quot; height=&quot;55%&quot; width=&quot;55%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图6 并行LR的数据分割&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;原文的标示我不太习惯，下面都改成了ij，并画出了矩阵运算的过程图。其中$X_{(i,j)}, i\in[1, m],j\in[1,n]$表示输入数据被分块后的第i行第j列的块。&lt;/p&gt;

&lt;p&gt;$X_{(i,j),k}$表示这个块中的第k行，$W_j$表示参数的第j块。&lt;/p&gt;

&lt;p&gt;第一步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_{(i,j),k}=W^T_jX_{(i,j),k}&lt;/script&gt;

&lt;p&gt;第二步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_{i,k} = \sum_{j=1}{n}d_{(i,j),k}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr7.png&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图7 并行LR的求梯度12步&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;第三步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{(i,j)} = \sum_{k=1}^{M/m}(\frac {1}{1+exp(y_{i,k} d_{i,k})} - 1) y_{i,k}X_{(i,j),k}&lt;/script&gt;

&lt;p&gt;第四步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{j} = \sum_{i=1}^mG_{(i,j)}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr8.png&quot; alt=&quot;1&quot; height=&quot;65%&quot; width=&quot;65%&quot; hspace=&quot;150&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图6 并行LR的求梯度34步&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;从而，经过上面的分解步骤可以将逻辑回归来做并行化计算。&lt;/p&gt;

&lt;h3 id=&quot;7&quot;&gt;7. 总结&lt;/h3&gt;

&lt;p&gt;这篇文章写了好几天，有时候写着写着就把自己绕进去了，因为可以展开说的地方太多了，写完这些内容，我又找了一些面试题看了看，理论部分基本上都能覆盖到了，但是涉及到真正的应用还是要再花时间去了解，最后的并行化理解还不够透彻，矩阵乘法我用gpu实现过，但是并没有接触过海量的数据，也不知道真正的问题会发生在什么地方。逻辑回归可以从很多方面来解释来理解，确实是个很美丽的算法。&lt;/p&gt;

&lt;h3 id=&quot;8&quot;&gt;8. 引用&lt;/h3&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://download.springer.com/static/pdf/925/chp%253A10.1007%252F978-0-85729-115-8_6.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-0-85729-115-8_6&amp;amp;token2=exp=1452256751~acl=%2Fstatic%2Fpdf%2F925%2Fchp%25253A10.1007%25252F978-0-85729-115-8_6.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Fchapter%252F10.1007%252F978-0-85729-115-8_6*~hmac=7d6eefedcc9f47275d89e6b094bf3900beea7c9a52e1ee42d99ea3d4d4a03064&quot;&gt;Verhulst and the logistic equation (1838)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN129323640_0018&amp;amp;DMDID=dmdlog7&quot;&gt;Mathematical enquiries on the law of population growth&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://math.bu.edu/people/mak/MA565/Pearl_Reed_PNAS_1920.pdf&quot;&gt;Proceedings of the national academy of sciences&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://www.jstor.org/stable/pdf/2983890.pdf?acceptTC=true&quot;&gt;The regression analysis of binary sequences&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;http://papers.tinbergen.nl/02119.pdf&quot;&gt;The Origins of Logistic Regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;http://blog.csdn.net/han_xiaoyang/article/details/49332321&quot;&gt;机器学习系列(2)用初等数学视角解读逻辑回归&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf&quot;&gt;A comparison of numerical optimizers for logistic regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&quot;http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/&quot;&gt;Numerical optimizers for Logistic Regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/21896453&quot;&gt;牛顿法与拟牛顿法学习笔记（一）牛顿法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] &lt;a href=&quot;https://www.zhihu.com/question/20700829&quot;&gt;知乎:机器学习中使用「正则化来防止过拟合」到底是一个什么原理&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[11] &lt;a href=&quot;http://blog.csdn.net/abcjennifer/article/details/7700772&quot;&gt;多变量线性回归 Linear Regression with multiple variable&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[12] &lt;a href=&quot;http://cs229.stanford.edu/notes/cs229-notes1.pdf&quot;&gt;CS229 Lecture notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[13] &lt;a href=&quot;http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf&quot;&gt;The equivalence of logistic regression and maximum entropy models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[14] &lt;a href=&quot;https://www.zhihu.com/question/26768865&quot;&gt;Linear SVM 和 LR 有什么异同？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[15] &lt;a href=&quot;https://www.zhihu.com/question/21704547&quot;&gt;SVM和logistic回归分别在什么情况下使用？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[16] &lt;a href=&quot;https://www.quora.com/Support-Vector-Machines/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression&quot;&gt;Support Vector Machines: What is the difference between Linear SVMs and Logistic Regression?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[17] &lt;a href=&quot;http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html&quot;&gt;支持向量机svm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[18] &lt;a href=&quot;https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf&quot;&gt;GENERATIVE AND DISCRIMINATIVE CLASSIFIERS: NAIVE BAYES AND LOGISTIC REGRESSION&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[19] &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf&quot;&gt;A Tutorial on Energy-Based Models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[20] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html&quot;&gt;并行逻辑回归&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 09 Jan 2016 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2016/01/09/logisticregression.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2016/01/09/logisticregression.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【GPU编程系列之一】从深度学习选择什么样的gpu来谈谈gpu的硬件架构</title>
        <description>&lt;p&gt;从深度学习在2012年大放异彩，gpu计算也走入了人们的视线之中，它使得大规模计算神经网络成为可能。人们可以通过07年推出的CUDA(Compute Unified Device Architecture)用代码来控制gpu进行并行计算。本文首先根据显卡一些参数来推荐何种情况下选择何种gpu显卡，然后谈谈跟cuda编程比较相关的硬件架构。&lt;/p&gt;

&lt;p&gt;####1.选择怎样的GPU型号&lt;/p&gt;

&lt;p&gt;这几年主要有AMD和NVIDIA在做显卡，到目前为止，NVIDIA公司推出过的GeForce系列卡就有几百张[1]，虽然不少都已经被淘汰了，但如何选择适合的卡来做算法也是一个值得思考的问题，Tim Dettmers[2]的文章给出了很多有用的建议，根据自己的理解和使用经历(其实只用过GTX 970…)我也给出一些建议。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/choose-gpu.png&quot; alt=&quot;1&quot; height=&quot;80%&quot; width=&quot;80%&quot; hspace=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 GPU选择&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;上面并没有考虑笔记本的显卡，做算法加速的话还是选台式机的比较好。性价比最高的我觉得是GTX 980ti，从参数或者一些用户测评来看，性能并没有输给TITAN X多少，但价格却便宜不少。从图1可以看出，价位差不多的显卡都会有自己擅长的地方，根据自己的需求选择即可。要处理的数据量比较小就选择频率高的，要处理的数据量大就选显存大core数比较多的，有double的精度要求就最好选择kepler架构的。tesla的M40是专门为深度学习制作的，如果只有深度学习的训练，这张卡虽然贵，企业或者机构购买还是比较合适的(百度的深度学习研究院就用的这一款[3])，相对于K40单精度浮点运算性能是4.29Tflops，M40可以达到7Tflops。QUADRO系列比较少被人提起，它的M6000价格比K80还贵，性能参数上也并没有好多少。&lt;/p&gt;

&lt;p&gt;在挑选的时候要注意的几个参数是处理器核心(core)、工作频率、显存位宽、单卡or双卡。有的人觉得位宽最重要，也有人觉得核心数量最重要，我觉得对深度学习计算而言处理器核心数和显存大小比较重要。这些参数越多越高是好，但是程序相应的也要写好，如果无法让所有的core都工作，资源就被浪费了。而且在购入显卡的时候，如果一台主机插多张显卡，要注意电源的选择。&lt;/p&gt;

&lt;p&gt;####2.一些常见的名称含义&lt;/p&gt;

&lt;p&gt;上面聊过了选择什么样的gpu，这一部分介绍一些常见名词。随着一代一代的显卡性能的更新，从硬件设计上或者命名方式上有很多的变化与更新，其中比较常见的有以下一些内容。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gpu架构：Tesla、Fermi、Kepler、Maxwell、Pascal&lt;/li&gt;
  &lt;li&gt;芯片型号：GT200、GK210、GM104、GF104等&lt;/li&gt;
  &lt;li&gt;显卡系列：GeForce、Quadro、Tesla&lt;/li&gt;
  &lt;li&gt;GeForce显卡型号：G/GS、GT、GTS、GTX&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;gpu架构指的是硬件的设计方式，例如流处理器簇中有多少个core、是否有L1 or L2缓存、是否有双精度计算单元等等。每一代的架构是一种思想，如何去更好完成并行的思想，而芯片就是对上述思想的实现，芯片型号GT200中第二个字母代表是哪一代架构，有时会有100和200代的芯片，它们基本设计思路是跟这一代的架构一致，只是在细节上做了一些改变，例如GK210比GK110的寄存器就多一倍。有时候一张显卡里面可能有两张芯片，Tesla k80用了两块GK210芯片。这里第一代的gpu架构的命名也是Tesla，但现在基本已经没有这种设计的卡了，下文如果提到了会用Tesla架构和Tesla系列来进行区分。&lt;/p&gt;

&lt;p&gt;而显卡系列在本质上并没有什么区别，只是NVIDIA希望区分成三种选择，GeFore用于家庭娱乐，Quadro用于工作站，而Tesla系列用于服务器。Tesla的k型号卡为了高性能科学计算而设计，比较突出的优点是双精度浮点运算能力高并且支持ECC内存，但是双精度能力好在深度学习训练上并没有什么卵用，所以Tesla系列又推出了M型号来做专门的训练深度学习网络的显卡。需要注意的是Tesla系列没有显示输出接口，它专注于数据计算而不是图形显示。&lt;/p&gt;

&lt;p&gt;最后一个GeForce的显卡型号是不同的硬件定制，越往后性能越好，时钟频率越高显存越大，即G/GS&amp;lt;GT&amp;lt;GTS&amp;lt;GTX。&lt;/p&gt;

&lt;p&gt;####3.gpu的部分硬件&lt;/p&gt;

&lt;p&gt;这一部分以下面的GM204硬件图做例子介绍一下GPU的几个主要硬件(图片可以点击查看大图，不想图片占太多篇幅)[4]。这块芯片它是随着GTX 980和970一起出现的。一般而言，gpu的架构的不同体现在流处理器簇的不同设计上(从Fermi架构开始加入了L1、L2缓存硬件)，其他的结构大体上相似。主要包括主机接口(host interface)、复制引擎(copy engine)、流处理器簇(Streaming Multiprocessors)、图形处理簇GPC(graphics processing clusters)、内存等等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/gm204hardware.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;390&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 GM204芯片结构&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;主机接口，它连接了gpu卡和PCI Express，它主要的功能是读取程序指令并分配到对应的硬件单元，例如某块程序如果在进行内存复制，那么主机接口会将任务分配到复制引擎上。&lt;/p&gt;

&lt;p&gt;复制引擎(图中没有表示出来)，它完成gpu内存和cpu内存之间的复制传递。当gpu上有复制引擎时，复制的过程是可以与核函数的计算同步进行的。随着gpu卡的性能变得强劲，现在深度学习的瓶颈已经不在计算速度慢，而是数据的读入，如何合理的调用复制引擎是一个值得思考的问题。&lt;/p&gt;

&lt;p&gt;流处理器簇SM是gpu最核心的部分，这个翻译参考的是GPU编程指南，SM由一系列硬件组成，包括warp调度器、寄存器、Core、共享内存等。它的设计和个数决定了gpu的计算能力，一个SM有多个core，每个core上执行线程，core是实现具体计算的处理器，如果core多同时能够执行的线程就多，但是并不是说core越多计算速度一定更快，最重要的是让core全部处于工作状态，而不是空闲。不同的架构可能对它命名不同，kepler叫SMX，maxwell叫SMM，实际上都是SM。而GPC只是将几个sm组合起来，在做图形显示时有调度，一般在写gpu程序不需要考虑这个东西，只要掌握SM的结构合理的分配SM的工作即可。&lt;/p&gt;

&lt;p&gt;图中的内存控制器控制的是L2内存，每个大小为512KB。&lt;/p&gt;

&lt;p&gt;####4.流处理器簇的结构&lt;/p&gt;

&lt;p&gt;上面介绍的是gpu的整个硬件结构，这一部分专门针对流处理器簇SM来分析它内部的构造是怎样的。首先要明白的是，gpu的设计是为了执行大量简单任务，不像cpu需要处理的是复杂的任务，gpu面对的问题能够分解成很多可同时独立解决的部分，在代码层面就是很多个线程同时执行相同的代码，所以它相应的设计了大量的简单处理器，也就是stream process，在这些处理器上进行整形、浮点型的运算。下图给出了GK110的SM结构图。它属于kepler架构，与之前的架构比较大的不同是加入了双精度浮点运算单元，即图中的DP Unit。所以用kepler架构的显卡进行双精度计算是比较好的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/keplersmx.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;390&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 GK110的SMX结构图&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;上面提到过的一个SM有多个core或者叫流处理器，它是gpu的运算单元，做整形、浮点型计算。可以认为在一个core上一次执行一个线程，GK110的一个SM有192个core，因此一次可以同时执行192个线程。core的内部结构可以查看[5]，实现算法一般不会深究到core的结构层面。SFU是特殊函数单元，用来计算log/exp/sin/cos等。DL/ST是指Load/Store，它在读写线程执行所需的全局内存、局部内存等。&lt;/p&gt;

&lt;p&gt;一个SM有192个core，8个SM有1536个core，这么多的线程并行执行需要有统一的管理，假如gpu每次在1536个core上执行相同的指令，而需要计算这一指令的线程不足1536个，那么就有core空闲，这对资源就是浪费，因此不能对所有的core做统一的调度，从而设计了warp(线程束)调度器。32个线程一组称为线程束，32个线程一组执行相同的指令，其中的每个thread称为lane。一个线程束接受同一个指令，里面的32个线程同时执行，不同的线程束可执行不同指令，那么就不会出现大量线程空闲的问题了。但是在线程束调度上还是存在一些问题，假如某段代码中有if…else…，在调度一整个线程束32个线程的时候不可能做到给thread0~15分配分支1的指令，给thread16~31分配分支2的指令(实际上gpu对分支的控制是，所有该执行分支1的线程执行完再轮到该执行分支2的线程执行)，它们获得的都是一样的指令，所以如果thread16~31是在分支2中它们就需要等待thread0~15一起完成分支1中的计算之后，再获得分支2的指令，而这个过程中，thread0～15又在等待thread16~31的工作完成，从而导致了线程空闲资源浪费。因此在真正的调度中，是半个warp执行相同指令，即16个线程执行相同指令，那么给thread0~15分配分支1的指令，给thread16~31分配分支2的指令，那么一个warp就能够同时执行两个分支。这就是图中Warp Scheduler下为什么会出现两个dispatch的原因。&lt;/p&gt;

&lt;p&gt;另外一个比较重要的结构是共享内存shared memory。它存储的内容在一个block(暂时认为是比线程束32还要大的一些线程个数集合)中共享，一个block中的线程都可以访问这块内存，它的读写速度比全局内存要快，所以线程之间需要通信或者重复访问的数据往往都会放在这个地方。在kepler架构中，一共有64kb的空间大小，供共享内存和L1缓存分配，共享内存实际上也可看成是L1缓存，只是它能够被用户控制。假如共享内存占48kb那么L1缓存就占16kb等。在maxwell架构中共享内存和L1缓存分开了，共享内存大小是96kb。而寄存器的读写速度又比共享内存要快，数量也非常多，像GK110有65536个。&lt;/p&gt;

&lt;p&gt;此外，每一个SM都设置了独立访问全局内存、常量内存的总线。常量内存并不是一块内存硬件，而是全局内存的一种虚拟形式，它跟全局内存不同的是能够高速缓存和在线程束中广播数据，因此在SM中有一块常量内存的缓存，用于缓存常量内存。&lt;/p&gt;

&lt;p&gt;####5.小结&lt;/p&gt;

&lt;p&gt;本文谈了谈gpu的一些重要的硬件组成，就深度学习而言，我觉得对内存的需求还是比较大的，core多也并不是能够全部用上，但现在开源的库实在完整，想做卷积运算有cudnn，想做卷积神经网络caffe、torch，想做rnn有mxnet、tensorflow等等，这些库内部对gpu的调用做的非常好并不需用户操心，但了解gpu的一些内部结构也是很有意思的。&lt;/p&gt;

&lt;p&gt;另，一开始接触GPU并不知道是做图形渲染的…所以有些地方可能理解有误，主要基于计算来讨论GPU的构造。&lt;/p&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units&quot;&gt;List of Nvidia graphics processing units&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://timdettmers.com/2014/08/14/which-gpu-for-deep-learning/&quot;&gt;Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://www.nextplatform.com/2015/12/11/inside-the-gpu-clusters-that-power-baidus-neural-networks/&quot;&gt;Inside the GPU Clusters that Power Baidu’s Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_980_Whitepaper_FINAL.PDF&quot;&gt;Whitepaper NVIDIA GeForce GTX 980&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline&quot;&gt;Life of a triangle - NVIDIA’s logical pipeline&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Dec 2015 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2015/12/20/introductionofgpuhardware.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2015/12/20/introductionofgpuhardware.html</guid>
        
        <category>programming languages</category>
        
      </item>
    
      <item>
        <title>【机器学习算法系列之一】EM算法实例分析</title>
        <description>&lt;p&gt;最近两天研究了一下EM算法，主要是基于《统计学习方法》和论文《What is the expectation maximization algorithm?》[1]，但是对两个文章里面给的实例求解过程都比较的困惑，搜索网上的一些博客也没有找到对应的求解过程，自己就仔细研究了一下，中间也遇到了一些坑，现在把解题思路给出来。因为书上和网上的博客[2]对EM算法的推导和证明解释的非常清楚，本文就不做解释了，如果对EM算法原理不清楚的建议先看看《统计学习方法》第9章或者博客[2][3]。本文只给出两个文章中的例子的求解过程。&lt;/p&gt;

&lt;p&gt;(题目我会列出来，如果不是看这个两个文章而了解EM算法的也不要紧，题目是通用的)&lt;/p&gt;

&lt;p&gt;本文中观测数据记为Y(因为两个例子都是输出是观测数据)，隐藏变量(未观测变量)记为z，模型参数记为$\theta$。&lt;/p&gt;

&lt;p&gt;####1.三硬币模型&lt;/p&gt;

&lt;p&gt;假设有三枚硬币A、B、C，每个硬币正面出现的概率是$\pi、p、q$。进行如下的掷硬币实验：先掷硬币A，正面向上选B，反面选C；然后掷选择的硬币，正面记1，反面记0。独立的进行10次实验，结果如下：1，1，0，1，0，0，1，0，1，1。假设只能观察最终的结果(0 or 1)，而不能观测掷硬币的过程(不知道选的是B or C)，问如何估计三硬币的正面出现的概率？&lt;/p&gt;

&lt;p&gt;首先针对某个输出y值，它在参数$\theta (\theta=(\pi, p, q))$下的概率分布为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y|\theta )=\sum_{z}P(y,z|\theta)=\sum_{z}P(z|\theta)P(y|z, \theta) = \pi p^y (1-p)^{1-y} + (1-\pi) q^y (1-q)^{1-y}&lt;/script&gt;

&lt;p&gt;从而针对观测数据$Y=(y_1, y_2, \cdot\cdot\cdot, y_n)^T$的似然函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(Y|\theta ) =\sum_{z}P(Y,z|\theta)=\sum_{z}P(z|\theta)P(Y|z, \theta) = \prod _{j=1} ^{n} \pi p^y_j (1-p)^{1-y_j} + (1-\pi) q^y_j (1-q)^{1-y_j}&lt;/script&gt;

&lt;p&gt;因此本题的目标是求解参数$\theta$的极大似然估计，即$\hat{\theta} = \underset{\theta }{argmax}logP(Y|\theta)$。直接对连乘的似然函数求导太复杂，所以一般用极大似然估计都会转化成对数似然函数，但是就算转化成了求和，如果这个式子对某个参数(例如$\pi$)求导，由于这个式子中有“和的对数”，求导非常复杂。因此这个问题需要用EM算法来求解。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E步&lt;/strong&gt;:根据EM算法，在这一步需要计算的是未观测数据的条件概率分布，也就是每一个$P(z|y_j, \theta)$，$\mu^{i+1}$表示在已知的模型参数$\theta^i$下观测数据$y_j$来自掷硬币B的概率，相应的来自掷C的概率就是$1-\mu^{i+1}$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu ^{i+1} = \frac {\pi^i ({p^i})^{y_j}(1-p^i)^{1-y_j}} {\pi^i ({p^i})^{y_j}(1-p^i)^{1-y_j} + (1-\pi^i) ({q^i})^{y_j} (1-q^i)^{1-y_j}}&lt;/script&gt;

&lt;p&gt;这里的分子就是z取掷硬币B和y的联合概率分布，需要注意的是，这里的$\mu^{i+1}$通过E步的计算就已经是一个常数了，后面的求导不需要把这个式子代入。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M步&lt;/strong&gt;:针对Q函数求导，Q函数的表达式是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\theta, \theta^i) = \sum_{j=1}^N \sum_{z} P(z|y_j, \theta^i)logP(y_j, z|\theta)=\sum_{j=1}^N \mu_jlog(\pi p^{y_j}(1-p)^{1-y_j}) + (1-\mu_j)log((1-\pi) q^{y_j} (1-q)^{1-y_j})]&lt;/script&gt;

&lt;p&gt;最开始求导犯了一个大错，没有将表达式展开来求，这样就直接默认$\mu_j$是一个系数，求导将它给约去了，这样就得不到最后的结果。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q}{\partial \pi} = (\frac{\mu_1}{\pi} - \frac{1-\mu_1}{1-\pi})+\cdot \cdot \cdot + (\frac{\mu_N}{\pi} - \frac{1-\mu_N}{1-\pi}) = \frac{\mu_1-\pi}{\pi(1-\pi)} + \cdot \cdot \cdot + \frac{\mu_N-\pi}{\pi(1-\pi)} = \frac{\sum _{j=1} ^N\mu_j-N\pi}{\pi(1-\pi)}&lt;/script&gt;

&lt;p&gt;再令这个结果等于0，即获得$\pi^{i+1} = \frac{1}{N}\sum_{j=1}^{N}\mu_j^{i+1}$，其他两个也同理。&lt;/p&gt;

&lt;p&gt;####2.两硬币模型&lt;/p&gt;

&lt;p&gt;假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H，H代表证明朝上。a是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/em1.png&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的$\hat{\theta_A}$，这其实也是极大似然求导求出来的。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\underset{\theta }{argmax}logP(Y|\theta) = log((\theta_B^5(1-\theta_B)^5) (\theta_A^9(1-\theta_A))(\theta_A^8(1-\theta_A)^2) (\theta_B^4(1-\theta_B)^6) (\theta_A^7(1-\theta_A)^3) ) = log(   (\theta_A^{24}(1-\theta_A)^6) (\theta_B^9(1-\theta_B)^{11})  )&lt;/script&gt;

&lt;p&gt;上面这个式子求导之后就能得出$\hat{\theta_A} = \frac{24}{24 + 6} = 0.80$以及$\hat{\theta_B} = \frac{9}{9 + 11} = 0.45$。&lt;/p&gt;

&lt;p&gt;针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E步&lt;/strong&gt;:计算在给定的$\hat{\theta_A^{(0)}}$和$\hat{\theta_B^{(0)}}$下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(z=A|y_1, \theta) = \frac {P(z=A, y_1|\theta)}{P(z=A,y_1|\theta) + P(z=B,y_1|\theta)} = \frac{(0.6)^5*(0.4)^5}{(0.6)^5*(0.4)^5+(0.5)^{10}} = 0.45&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;M步&lt;/strong&gt;:针对Q函数求导，在本题中Q函数形式如下，参数设置参照例1，只是这里的$y_j$代表的是每次正面朝上的个数。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(\theta, \theta^i) = \sum_{j=1}^N \sum_{z} P(z|y_j, \theta^i)logP(y_j, z|\theta)=\sum_{j=1}^N \mu_jlog(\theta_A^{y_j}(1-\theta_A)^{10-y_j}) + (1-\mu_j)log(\theta_B^{y_j}(1-\theta_B)^{10-y_j})]&lt;/script&gt;

&lt;p&gt;从而针对这个式子来对参数求导，例如对$\theta_A$求导&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q}{\partial \theta_A} = \mu_1(\frac{y_1}{\theta_A}-\frac{10-y_1}{1-\theta_A}) + \cdot \cdot \cdot  + \mu_5(\frac{y_5}{\theta_A}-\frac{10-y_5}{1-\theta_A}) 
= \mu_1(\frac{y_1 - 10\theta_A} {\theta_A(1-\theta_A)}) + \cdot \cdot \cdot +\mu_5(\frac{y_5 - 10\theta_A} {\theta_A(1-\theta_A)})  = \frac{\sum_{j=1}^5 \mu_jy_j - \sum_{j=1}^510\mu_j\theta_A} {\theta_A(1-\theta_A)}&lt;/script&gt;

&lt;p&gt;求导等于0之后就可得到图中的第一次迭代之后的参数值$\hat{\theta_A^{(1)}} = 0.71$和$\hat{\theta_B^{(1)}} = 0.58$。&lt;/p&gt;

&lt;p&gt;这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。&lt;/p&gt;

&lt;p&gt;####3.小结&lt;/p&gt;

&lt;p&gt;EM算法将不完全数据补全成完全数据，而E步并不是只取最可能补全的未观测数据，而是将未观测的数据的所有补全可能都计算出对应的概率值，从而对这些所有可能的补全计算出它们的期望值，作为下一步的未观测数据。至于为什么取期望，一是因为这个未观测数据本身就是基于一组不完全正确的参数估计出来的，例如三硬币例子假如每次在进行maximization之前都只取某一个值(极端一点，每次结果都是认为B是最可能的观测数据，而不算C)，那么在更新参数时，也只有B的参数在更新。二是这种情况下JENSEN不等式不成立，那么对$\theta$的似然函数变换形式就不成立，收敛也不成立。&lt;/p&gt;

&lt;p&gt;这两个例子想明白之后求解实际上非常简单，所以很多博主并没把它们列出来，但如果一开始思考的方向不对就会浪费很多时间，当我把上面的过程想清楚之后再去求解别的例子，发现很轻松就能解出来。当然EM算法的核心还是证明和推导，这点别的文章讲的非常清晰了我就不赘述了。这也是数学上常用的思路，当无法直接对某个含参式子求极大值时，考虑对它的下界求极大值，当确定下界取极大值的参数时也能让含参式子值变大，也就是&lt;strong&gt;&lt;em&gt;不断求解下界的极大值逼近求解对数似然函数极大化(李航.《统计学习方法》)&lt;/em&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果本文有错误，请一定要指出来，感谢～&lt;/p&gt;

&lt;p&gt;####4.参考：&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf&quot;&gt;What is the expectation maximization
algorithm?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html&quot;&gt;（EM算法）The EM Algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/8537620&quot;&gt;从最大似然到EM算法浅解&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Dec 2015 00:00:00 +0800</pubDate>
        <link>chenrudan.github.io/blog/2015/12/02/emexample.html</link>
        <guid isPermaLink="true">chenrudan.github.io/blog/2015/12/02/emexample.html</guid>
        
        <category>project experience</category>
        
      </item>
    
  </channel>
</rss>
