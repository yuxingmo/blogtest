<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
  <meta name="description" content="听见下雨的声音">
  <meta name="author" content="陈汝丹">
  <meta name="keywords" content="【David Silver强化学习公开课之三】动态规划解决MDP的Planning问题, 陈汝丹, 陈汝丹">
  <title>【David Silver强化学习公开课之三】动态规划解决MDP的Planning问题 - 陈汝丹</title>
  <link rel="canonical" href="chenrudan.github.io/blog/2016/06/17/reinforcementlearninglesssion3.html">
  <link rel="icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="shortcut icon" href="/res/img/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" href="/res/css/public.css">
  <link rel="stylesheet" href="/res/css/light.css">
  <script src="/res/js/light.js"></script>
  <script>
  var _hmt = _hmt || [];
  (function() {
    var hm = document.createElement("script");
    hm.src = "//hm.baidu.com/hm.js?";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
  })();
</script>
</head>
<body>
  <div id="blog">
    <div class="sidebar">
  <div class="profilepic">
    <a href="/"><img src="http://7xkmdr.com1.z0.glb.clouddn.com/90fba609961616fae2b927.jpg" alt="logo"></img></a>
  </div>
  <h1 class="title"><a href="/">陈汝丹</a></h1>
  <h2 class="sub-title">个人博客</h2>
  <nav id="nav">
    <ul>
    
      <li><a href="/page/timing.html"><i class="fa fa-clock-o"></i>&nbsp;时间轴</a></li>
    
      <li><a href="/page/category.html"><i class="fa fa-tags"></i>&nbsp;分类</a></li>
    
      <li><a href="/page/about.html"><i class="fa fa-paper-plane-o"></i>&nbsp;关于</a></li>
    
    </ul>
  </nav>  
  <nav id="sub-nav">
    <a class="weibo " href="http://weibo.com/u/3842815916" title="新浪微博" target="_blank"><i class="fa fa-weibo"></i></a>
    <a class="github" href="https://github.com/chenrudan" title="GitHub" target="_blank"><i class="fa fa-github fa-2x"></i></a>
    <a class="rss" href="/page/feed.xml" title="RSS订阅" target="_blank"><i class="fa fa-rss"></i></a>
  </nav>
  <div id="license">
    <a href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="本站所有作品采用：&#10;知识共享《署名 非商业性使用 相同方式共享 3.0》&#10;进行许可" >
    <img alt="License" height="31" width="88" src="data:image/png;base64,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" /></a>
  </div>
</div>
    <div class="main">
    <header class="post-header">
  <h1 class="post-title"><a href="/blog/2016/06/17/reinforcementlearninglesssion3.html">【David Silver强化学习公开课之三】动态规划解决MDP的Planning问题</a></h2>
  <p class="post-meta">
    <i class="fa fa-calendar"></i>
    2016年06月17日
    <i class="space"></i>
    <i class="fa fa-tags"></i>
    <a class="post-category" href="/page/category.html#project experience">project experience</a>
  </p>
</header>

<div class="post-main">
	<p>【转载请注明出处】<a href="http://chenrudan.github.io/">chenrudan.github.io</a></p>

<p>本文是David Silver强化学习公开课第三课的总结笔记。主要谈到了动态规划能够解决MDP的什么问题，如何通过Policy Iteration和Value Iteration来解决，以及这两者指的是什么，出于什么样的考虑提出这两种思路，具体解决步骤是什么。</p>

<p>本课视频地址:<a href="https://www.youtube.com/watch?v=lfHX2hHRMVQ&amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa&amp;index=2">RL Course by David Silver - Lecture 3: Planning by Dynamic Programming</a>。</p>

<p>本课ppt地址:<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf</a>。</p>

<p>文章的内容是课程的一个总结和讨论，会按照自己的理解来组织。个人知识不足再加上英语听力不是那么好可能会有一些理解不准的地方，欢迎一起讨论。</p>

<h4 id="section">1.内容回顾</h4>

<p>前两节课我忽略了一些内容，这节课用到了，所以先回顾一下。首先是Planning的概念，在第一节课提到过强化学习是一种Sequential Decision Making问题，它是一种试错(trial-and-error)的学习方式，一开始不清楚environment的工作方式，不清楚执行什么样的行为是对的，什么样是错的，因而agent需要从不断尝试的经验中发现一个好的policy。而Planning也属于Sequential Decision Making问题，不同的是它的environment是已知的，例如游戏的规则是已知的，所以agent不需要通过与environment的交互来获取下一个状态，而是知道自己执行某个action之后状态是什么，再优化自己的policy。因此这两者之间是有联系的，假如强化学习学习出来environment的模型，知道了environment是如何work的，强化学习要解决的问题就是Planning了。</p>

<p>在第二课中，推导出了几个Bellman方程，回归一下。</p>

<table>
  <tbody>
    <tr>
      <td>Bellman方程</td>
      <td>形式</td>
    </tr>
    <tr>
      <td>Bellman Expectation Equation</td>
      <td>$v_{\pi}(s) = \sum_{a\in A}\pi(a|s)q_{\pi}(s,a) =\sum_{a\in A}\pi(a|s)(R_s^a + \gamma \sum_{s’\in S}P_{ss’}^av_{\pi}(s’))$</td>
    </tr>
    <tr>
      <td>Bellman Expectation Equation</td>
      <td>$q_{\pi}(s,a) = R_s^{a} + \gamma \sum_{s’ \in S}P_{ss’}^av_\pi(s’)=R_s^a + \gamma \sum_{s’\in S}P_{ss’}^a\sum_{a’\in A}\pi(a’|s’)q_{\pi}(s’,a’)$</td>
    </tr>
    <tr>
      <td>Bellman Optimality Equation</td>
      <td>$v_*(s) = \underset{a}{max}q_*(s,a) = \underset{a}{max} R_s^{a} + \gamma \sum_{s’ \in S}P_{ss’}^av_*(s’)$</td>
    </tr>
    <tr>
      <td>Bellman Optimality Equation</td>
      <td>$q_*(s,a) = R_s^{a} + \gamma \sum_{s’ \in S}P_{ss’}^av_*(s’) = R_s^a + \gamma \sum_{s’\in S}P_{ss’}^a \underset{a’}    {max}q_*(s’, a’)$</td>
    </tr>
  </tbody>
</table>

<h4 id="planning">2. 动态规划与Planning</h4>

<p>动态规划是这样一种方法，它将一个复杂的问题切分成一系列简单的子问题，一旦解决了这些简单的子问题，再将这些子问题的解结合起来变成复杂问题的解，同时将它们的解保存起来，如果下一次遇到了相同的子问题那么就不用再重新计算子问题的解[1]。其中“动态”是指某个问题是由序列化状态组成，状态是step-by-step的改变，从而可以step-by-step的来解这个问题，“规划”即优化Policy。而MDP有Bellman方程能够被递归的切分成子问题，同时它有值函数，保存了每一个子问题的解，因此它能通过动态规划来求解。针对MDP，切分成的子问题就是在每个状态下应该选择的action是什么，MDP的子问题是以一种递归的方式存在，这一时刻的子问题取决于上一时刻的子问题选择了哪个action。</p>

<p>当已知MDP的状态转移矩阵时，environment的模型就已知了，此时可以看成Planning问题，动态规划则是用来解决MDP的Planning问题，主要解决途径有两种，Policy Iteration和Value Iteration。</p>

<h4 id="policy-iteration">3. Policy Iteration</h4>

<p>这个解决途径主要分为两步，示意图见图1:</p>

<ul>
  <li>Policy Evaluation:基于当前的Policy计算出每个状态的value function</li>
  <li>Policy Improvment:基于当前的value function，采用贪心算法来找到当前最优的Policy</li>
</ul>

<p><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl3_1.png" alt="1" height="15%" width="15%" hspace="370" /></p>

<font size="2"><center>图1 Policy Iteration过程图(图片来源[2])</center></font>

<p>这里的Policy Evaluation要求的$v(s)$是通过计算第一个Bellman Expectation Equation得到的。下图是一个叫Small Gridworld的例子，左上角和右下角是终点，$\gamma = 1$，移动一步reward减少1，起始的random policy是朝每个能走的方向概率相同，先单独看左边一列，它表示在第k次迭代每个state上value function的值，这一列始终采用了random policy，这里的value function就是通过Bellman Expectation Equation得到的，考虑k=2的情况，-1.7 = -1.0 + 2*(1/3.0)*(-1)，-2.0 = -1.0 + 4*(1/4.0)*(-1)。而右边一列就是在当前的value function情况下通过greedy算法找到当前朝哪个方向走比较好。</p>

<p>Policy Iteration会一直迭代到收敛，具体证明过程可以去看视频(46:09起)。</p>

<p><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl3_2.png" alt="1" height="30%" width="30%" hspace="300" /></p>

<font size="2"><center>图2 Policy Iteration实例(图片来源[2])</center></font>

<h4 id="value-iteration">3. Value Iteration</h4>

<p>最优化原理:当且仅当任何从s能达到的$s’$能在当前policy下获取最优的value即$v_{\pi}(s’) = v_{*}(s’)$，那么状态s也能在当前policy下获得最优value$v_{\pi}(s) = v_{*}(s)$。</p>

<p>从上面原理出发，如果已知子问题的最优值$v_{*}s’$，那么就能通过第一个Bellman Optimality Equation将$v_{*}(s)$也推出来。因此从终点开始向起点推就能把全部状态最优值推出来。Value Iteration通过迭代的方法，通过这一步的$v_k(s’)$更新下一步的$v_{k+1}(s)$不断迭代，最终收敛到最优的$v_{*}$，需要注意的是中间生成的value function的值不对应着任何policy。</p>

<p>考虑下面这个Shortest Path例子，左上角是终点，要求的是剩下每一个格子距离终点的最短距离，每走一步，reward减少1，根据Value Iteration计算Bellman Optimality Equation就能得到后面的每一个正方形格子取值。</p>

<p><img src="http://7xkmdr.com1.z0.glb.clouddn.com/rl3_3.png" alt="1" height="30%" width="30%" hspace="300" /></p>

<font size="2"><center>图3 Value Iteration实例(图片来源[2])</center></font>

<p>以上是本课主要内容，后面还提了一下异步动态规划，并且提到了以上动态规划的解法适合状态个数百万级别的问题。总之，这节课需要搞懂的问题是动态规划能解决什么样的MDP问题以及具体怎样通过Policy Iteration和Value Iteration做到的。</p>

<p>[1] <a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</a></p>

<p>[2] <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/DP.pdf">Planning by Dynamic Programming</a></p>


</div>
<div class="share">
  <div class="bdsharebuttonbox">
    <a href="#" class="bds_more" data-cmd="more"></a>
    <a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a>
    <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
  </div>
  <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":["qzone","tsina","weixin","sqq"],"bdPic":"","bdStyle":"0","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='/res/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>
</div>
<div class="pagination">
	
	<a class="pagination-item newer" href="/blog/2016/06/12/reinforcementlearninglesssion2.html"><i class="fa fa-arrow-left"></i>&nbsp;&nbsp;【David Silver强化学习公开课之二】马尔可夫决策过程MDP</a>
	

	<span class="pagination-item older">下一篇&nbsp;<i class="fa fa-chain-broken"></i></span>
	
</div>
    
<i class="space"></i>

<!-- 多说评论框 start -->
<div class="ds-thread" data-thread-key="/blog/2016/06/17/reinforcementlearninglesssion3.html" data-title="【David Silver强化学习公开课之三】动态规划解决MDP的Planning问题" data-url="/blog/2016/06/17/reinforcementlearninglesssion3.html"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"chenrudan"};
		(function() {
 			var ds = document.createElement('script');
			ds.type = 'text/javascript';ds.async = true;
			ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
			ds.charset = 'UTF-8';
			(document.getElementsByTagName('head')[0] 
			|| document.getElementsByTagName('body')[0]).appendChild(ds);
											
 		})();
		</script>
	<!-- 多说公共JS代码 end -->
	

    
    </div>
  </div>    
  <div id="top"><a id="rocket" href="javascript:;" title="返回顶部"><i></i></a></div>
  
</body>
</html>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)
 },i[r].l=1*new Date();a=s.createElement(o),
   m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
     })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-62560044-1', 'auto');
    ga('send', 'pageview');

</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	                  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
					                            
	});
</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
