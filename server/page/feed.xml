<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RSS - 陈汝丹</title>
    <description>陈汝丹 - 个人博客</description>
    <link>chenrudan.github.io</link>
    <atom:link href="chenrudan.github.io/page/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>2016-04-05 14:23:23 +0800</pubDate>
    <lastBuildDate>2016-04-05 14:23:23 +0800</lastBuildDate>
    <generator>陈汝丹</generator>
    
      <item>
        <title>【机器学习算法系列之三】</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. &lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#3.1&quot;&gt;3.1 主成分分析PCA&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3&quot;&gt;3. &lt;/h3&gt;

&lt;h4 id=&quot;3.1&quot;&gt;3.1 主成分分析PCA&lt;/h4&gt;

&lt;p&gt;PCA是一种线性降维方法，高维空间(维数为D)的某个点$\mathbf{x_i} = (x_1, x_2, …, x_D)$通过与矩阵W相乘映射到低维空间(维数为d，$d&amp;lt;D$)中的某个点$\mathbf{z_i} = W^T\mathbf{x_i}$，其中W的大小是$d&lt;em&gt;D$，i对应的是第i个样本点。从而可以得到N个从D维空间映射到d维空间的点，PCA的目标是让映射得到的点$\mathbf{z_i}$尽可能的分开，即让N个$\mathbf{z_i}$的方差尽可能大。假如D维空间中的数据每一维均值为0，即$\sum_i \matbf{x_i} = \mathbf{0}$，那么两边乘上$W^T$得到的降维后的数据每一维均值也是0，考虑一个矩阵$C=\frac{1}{N}X&lt;/em&gt;X^T$，这个矩阵是这组D维数据的协方差矩阵，可以看出对角线上的值是D维中的某一维内的方差，非对角线元素是D维中两维之间的协方差。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\frac{1}{N}X*X^T=\begin{pmatrix}
\frac{1}{N}\sum_{i=1}^N x_{1i}^2&amp; ... &amp; \frac{1}{N}\sum_{i=1}^N x_{1i}x_{Di} \\ 
... &amp;  &amp;... \\ 
\frac{1}{N}\sum_{i=1}^N x_{Di}x_{1i} &amp; ... &amp;\frac{1}{N}\sum_{i=1}^N x_{Di}^2  
\end{pmatrix}
 %]]&gt;&lt;/script&gt;

&lt;p&gt;那么针对降维后d维数据的协方差矩阵$B=\frac{1}{N}Z&lt;em&gt;Z^T$，如果希望降维后的点尽可能分开，那么就希望B对角线上值即每一维的方差尽可能大，方差大说明这些维上数据具有非常好的区分性，同时希望d的每一维都是正交的，它们正交就会使得两个维是无关的，那么它们就不会包含重叠的信息，这样就能最好的表现数据，每一维都具有足够的区分性，同时还具有不同的信息。这种情况下B非对角线上值全部为0。又由于可以推导得出$B=\frac{1}{N}Z&lt;/em&gt;Z^T = W&lt;em&gt;(\frac{1}{N}X&lt;/em&gt;X^T)W = W&lt;em&gt;C&lt;/em&gt;W^T$，这个式子实际上就是表示了线性变换矩阵W在PCA算法中的作用是让原始协方差矩阵C对角化。又由于线性代数中对角化是通过求解特征值与对应的特征向量得到，因此可以推出PCA算法流程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr1.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 比利时的人口增长数量图&lt;/center&gt;&lt;/font&gt;

</description>
        <pubDate>2016-04-01 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2016/04/01/dimensionalityreduction.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2016/04/01/dimensionalityreduction.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【GPU编程系列之二】CUDA的软件层面和NVCC编译流程</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是gpu编程系列第二篇，主要谈谈CUDA软件层面的一些内容，这里的软件层面是指用户写程序涉及到的一些内容，包括介绍CUDA、区分Runtime API与Driver API以及介绍一些常用的库，最后谈一谈NVCC的编译流程。&lt;/p&gt;

&lt;h4 id=&quot;cuda&quot;&gt;1. CUDA是什么？&lt;/h4&gt;

&lt;p&gt;接触过gpu编程的人肯定都会看到过一个词CUDA，全称是Compute Unified Device Architecture，英伟达在2007年推出这个统一计算架构，为了让gpu有可用的编程环境，从而能通过程序控制底层的硬件进行计算。CUDA提供host-device的编程模式以及非常多的接口函数和科学计算库，通过同时执行大量的线程而达到并行的目的。在上一篇讲gpu硬件的文章中提到过流处理器(SM)的概念，并且说过SM是gpu的计算单元，而线程是执行在SM上的并行代码。CUDA也有不同的版本，从1.0开始到现在的7.5，每个版本都会有一些新特性。CUDA是基于C语言的扩展，例如扩展了一些限定符__device__、__shared__等，从3.0开始也支持c++编程，从7.0开始支持c++11。&lt;/p&gt;

&lt;p&gt;在安装CUDA的时候，会安装三个大的组件[1]，分别是NVIDIA驱动、toolkit和samples。驱动用来控制gpu硬件，toolkit里面包括nvcc编译器、Nsight调试工具(支持Eclipse和VS，linux用cuda-gdb)、分析和调试工具和函数库。samples或者说SDK，里面包括很多样例程序包括查询设备、带宽测试等等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/install_ubuntu_3.png&quot; alt=&quot;1&quot; height=&quot;20%&quot; width=&quot;20%&quot; hspace=&quot;350&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 CUDA安装完成截面(点击查看大图)&lt;/center&gt;&lt;/font&gt;

&lt;h4 id=&quot;runtime-api-vs-driver-api&quot;&gt;2. Runtime API vs Driver API&lt;/h4&gt;

&lt;p&gt;在写cuda程序时，除了自己写的kernel函数，常常会调用cuda接口函数，最常见的就是全局内存分配函数cudaMalloc()，这里分配的内存暂且理解为gpu硬件参数上的显存。然而在某种情况下可能会看到另外一个函数cuMemAlloc()，这两个函数本质上完成的功能是一样的，都是在分配全局内存，但却属于两套接口，分别为Runtime API和Driver API。下图是cuda软件层的一些组件，实际上在cuda的软件层面，Runtime比Driver API更高级，封装的更好，在Runtime之上就是封装的更好的cuFFT等库。这两个库的函数都是能直接调用的，但Driver API相对于Runtime对底层硬件驱动的控制会更直接更方便，比如对context的控制[2]，Driver API调用硬件速度实际上比Runtime也快不了多少。不过Driver API向后兼容支持老版本的，这点Runtime就做不到，7.0的版本代码可能在6.5上就跑不了。大部分的功能两组API都有对应的实现，一般基于Driver API的开头会是cu，而基于Runtime API的开头是cuda，但基于Driver API来写程序会比Runtime API要复杂，虽然功能上差别不大，但是使用Runtime API和更高级的库函数就已经足够了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/cuda_software.png&quot; alt=&quot;1&quot; height=&quot;20%&quot; width=&quot;20%&quot; hspace=&quot;350&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 CUDA软件层(图片来源《The cuda handbook》，点击查看大图)&lt;/center&gt;&lt;/font&gt;

&lt;h4 id=&quot;section&quot;&gt;3. 常用函数库&lt;/h4&gt;

&lt;p&gt;NVIDIA针对cuda的使用开发了很多好用的库，包括实现c++ STL的thrust、实现gpu版本blas的cublas、实现快速傅里叶变换的cuFFT、实现稀疏矩阵运算操作的cuSparse以及实现深度学习网络加速的cuDNN等等[3]。在操作这些库时有一个通用的规范，即调用者进行设备内存的分配与释放，内存分配好后将指针传递给这些库接口，就可以进行计算了。&lt;/p&gt;

&lt;p&gt;关于thrust，它最基本的数据类型是两个向量容器，host_vetcor和device_vector，分别对应了内存分配在cpu内存和cpu内存，并且提供了非常多的函数模板，例如归约、归并、排序、二分查找等等。此外支持很多STL容器，例如下面的例子(代码来源[4])中即可以从c++容器中将数据复制给thrust的vector，也能将thrust的数据复制给c++ stl。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;std::list&amp;lt;int&amp;gt; stl_list;
stl_list.push_back(10);
stl_list.push_back(20);
stl_list.push_back(30);
stl_list.push_back(40);

// 从c++ stl的list来初始化device_vector 
thrust::device_vector&amp;lt;int&amp;gt; D(stl_list.begin(), stl_list.end()); 

// 将device_vector的内容复制到stl的vector中 
std::vector&amp;lt;int&amp;gt; stl_vector(D.size()); 
thrust::copy(D.begin(), D.end(), stl_vector.begin());
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而cublas的接口设计基本上跟blas库是一致的，也是有lever1~3的接口，不同点在cuda的库风格是在使用相关函数之前要创建一个句柄，这个句柄也要传递到相关函数中去，在程序完成之后要销毁对应的句柄。因此cublas库函数原型比blas的参数要多。句柄的设置是为了方便执行多gpu，即在通过调用cudaSetDevice()来启用不同的gpu设备，在每个设备上又可以初始化一个独立的句柄，那么就能同时在多个gpu上执行。虽然cuda允许不同的线程调用同一个句柄，但是最好还是不要这样做。&lt;/p&gt;

&lt;p&gt;至于cudnn，它是专门针对深度学习实现的一个库，目前主要还是实现卷积神经网络，加速效果很好，现在的一些深度学习框架基本上都支持它。&lt;/p&gt;

&lt;h4 id=&quot;nvcc&quot;&gt;4. NVCC编译流程&lt;/h4&gt;

&lt;p&gt;由于程序是要经过编译器编程成可执行的二进制文件，而cuda程序有两种代码，一种是运行在cpu上的host代码，一种是运行在gpu上的device代码，所以NVCC编译器要保证两部分代码能够编译成二进制文件在不同的机器上执行。nvcc涉及到的文件后缀及相关意义如下表[5]。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;文件后缀&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;意义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cu&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cuda源文件，包括host和device代码&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cup&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;经过预处理的cuda源文件，编译选项--preprocess/-E&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.c&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c源文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cc/.cxx/.cpp&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++源文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.gpu&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;gpu中间文件，编译选项--gpu&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.ptx&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;类似汇编代码，编译选项--ptx&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.o/.obj&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;目标文件，编译选项--compile/-c&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.a/.lib&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;库文件，编译选项--lib/-lib&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.res&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;资源文件&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.so&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;共享目标文件，编译选项--shared/-shared&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;.cubin&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;cuda的二进制文件，编译选项-cubin&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;此外编译的常用选项还有--link，编译并链接所有的输入文件，--run编译链接文件后直接执行，-m指定32位还是64位机器。&lt;/p&gt;

&lt;p&gt;有时还会见到--gpu-architecture/-arch和--gpu-code/-code，它们的目标是为了让gpu代码能够兼容多种架构的gpu，它们的取值范围是一样的包括compute_10/compute_11/…/compute_30/compute_35/sm_10/sm_11/…/sm_30/sm_35，这些取值范围的意义实际上是指gpu的计算能力，或者是SM的版本，一般表示的是1.0/2.0/3.5这样的，具体体现在之前讲过的kepler/fermi的不同架构，所以两者实际含义是一样的(这里看[5]上表达应该是两者等价，但是为什么要搞两个，统一叫计算能力不行吗…)。而这里两个编译选项都要选择计算能力/sm版本，这是因为nvcc最后生成的可执行文件可以同时存在多个版本(对应compute_10/…/compute_35等)的kernel函数，这多个版本就通过这两个编译选项确定。这两个编译选项使得中间会生成的ptx文本文件和cubin二进制文件，它们指定了最后生成的可执行文件中可以满足的版本要求，即通过-arch指定ptx将来可以生成怎么样的版本(可以看成针对一个虚拟的gpu)，而-code参数是指当前就要生成的二进制的版本(可以想象成一个真实的GPU)，当前和将来的意思是指，最后生成的可执行文件中一个部分是马上就能在gpu上执行，而如果gpu硬件版本不支持这个能够马上执行的部分，那么显卡驱动会重新根据ptx指定的版本再生成一个能够执行的可执行版本，来满足这个gpu的硬件需求。它们的版本信息会先嵌入fatbin文件中，再通过fatbin与host代码编译生成的中间结果链接成最后的目标文件，这是针对一个.cu源文件生成一个.o文件，再将不同.o链接成可执行文件，那么这个可执行文件中就包含了多个版本的信息。假如取–arch=compute_10 –code=sm_13，在最后的可执行文件中，就有一个可以直接执行1.3的版本，假如此时gpu计算能力只有1.0，那么驱动会再次编译生成1.0版本的可执行文件，这个新的可执行文件就能在计算能力只有1.0的机器上运行了，从而通过这样的方式可以兼容不同计算能力的gpu。需要注意的是code的版本要高于arch。&lt;/p&gt;

&lt;p&gt;显然这里有一个问题，一般而言生成的c++可执行程序就直接执行了，不会再有编译的过程，但是cuda不一样，它有一个机制叫做just-in-time(JIT，运行时编译)，为了满足两种执行cuda程序的方式。第一种就是直接执行cubin版本，第二种就是显卡驱动通过JIT在运行的时候根据ptx版本再次编译生成可执行文件。&lt;/p&gt;

&lt;p&gt;NVCC实际上调用了很多工具来完成编译步骤(建议先看看[5]中完善的流程图)。在编译一个.cu源文件时，当输入下面的指令后，执行程序就会将整个编译过程都打印出来。cuda的整个编译流程分成两个分支，分支1预处理device代码并进行编译生成cubin或者ptx，然后整合到二进制文件fatbin中，分支2预处理host代码，再和fatbin一起生成目标文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nvcc --cuda test.cu -keep --dryrun
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;针对打印出来的每个过程中生成的文件做一个简单的分析，test.cu初始化一个runtime的硬件查询对象cudaDeviceProp，然后打印共享内存的大小。这里打印出来的内容只截取了每条命令的部分，主要给出中间的生成文件。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//打印信息
gcc -D__CUDA_ARCH__=200 -E -x c++ ... -o &quot;test.cpp1.ii&quot; &quot;test.cu&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这一步是device代码预处理，它将一些定义好的枚举变量(例如cudaError)、struct(例如cuda的数据类型float4)、静态内联函数、extern “c++”和extern的函数、还重新定义了std命名空间、函数模板等内容写在main函数之前。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cudafe ... --gen_c_file_name &quot;test.cudafe1.c&quot; --gen_device_file_name &quot;test.cudafe1.gpu&quot; ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一步test.cpp1.ii被cudafe切分成了c/c++ host代码和.gpu结尾的device代码，其中main函数还是在.c结尾的文件中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gcc -D__CUDA_ARCH__=200 -E -x c ... -o &quot;test.cpp2.i&quot; &quot;test.cudafe1.gpu&quot;
cudafe ... --gen_c_file_name &quot;test.cudafe2.c&quot; ... --gen_device_file_name &quot;test.cudafe2.gpu&quot;
gcc -D__CUDA_ARCH__=200 -E -x c ... -o &quot;test.cpp3.i&quot; &quot;test.cudafe2.gpu&quot;
filehash -s &quot;test.cpp3.i&quot; &amp;gt; &quot;test.hash&quot;
gcc -E -x c++ ... -o &quot;test.cpp4.ii&quot; &quot;test.cu&quot;
cudafe++ ... --gen_c_file_name &quot;test.cudafe1.cpp&quot; --stub_file_name &quot;test.cudafe1.stub.c&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这段生成的test.cpp4.ii是在对host代码进行预处理，前面几行内容直接看文件有点看不出来，希望以后能够把这段补充起来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cicc  -arch compute_20 ... --orig_src_file_name &quot;test.cu&quot; &quot;test.cpp3.i&quot; -o &quot;test.ptx&quot;
----------------
test.ptx:
	.version 4.1
	.target sm_20
	.address_size 64
----------------
test.cpp1.ii
	&quot;.../include/sm_11_atomic_functions.h&quot;
	...
	&quot;.../include/sm_12_atomic_functions.h&quot;
	...
	&quot;.../include/sm_35_atomic_functions.h&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;test.ptx文件中只记录了三行编译信息，可以看出对应了上面提到指定ptx的版本，以后可以根据这个版本再进行编译。实际上在host c++代码即每一个test.cpp*文件中，都包含了所有版本的SM头文件，从而可以调用每一种版本的函数进行编译。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ptxas -arch=sm_20 -m64  &quot;test.ptx&quot; -o &quot;test.sm_20.cubin&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一步叫做PTX离线编译，主要的目的是为了将代码编译成一个确定的计算能力和SM版本，对应的版本信息保存在cubin中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fatbinary --create=&quot;test.fatbin&quot; ... &quot;file=test.sm_20.cubin&quot; ... &quot;file=test.ptx&quot; --embedded-fatbin=&quot;test.fatbin.c&quot; --cuda
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这一步叫PTX在线编译，是将cubin和ptx中的版本信息保存在fatbin中。这里针对一个.cu源文件，调用系统的gcc/g++将host代码和fatbin编译成对应的目标文件。最后用c++编译器将目标文件链接起来生成可执行文件。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;5. 小结&lt;/h4&gt;

&lt;p&gt;上面谈到了关于cuda软件层面的一些东西，内容也不算多，主要是谈谈自己对这方面的理解，最后的编译流程，具体每一步到底做了什么还不够清晰，希望能有人共同探讨一下。&lt;/p&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://people.maths.ox.ac.uk/gilesm/cuda/lecs/lec1.pdf&quot;&gt;Lecture 1: an introduction to CUDA&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://chenrudan.github.io/blog/2015/07/22/cudastream.html&quot;&gt;【GPU编程系列之三】cuda stream和event相关内容&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://developer.nvidia.com/gpu-accelerated-libraries&quot;&gt;GPU-Accelerated Libraries&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://docs.nvidia.com/cuda/thrust/#axzz42f5Uevuz&quot;&gt;Thrust&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#axzz43KUwL7GV&quot;&gt;The CUDA Compiler Driver NVCC&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>2016-03-06 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2016/03/06/introductionofgpusoftware.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2016/03/06/introductionofgpusoftware.html</guid>
        
        <category>programming languages</category>
        
      </item>
    
      <item>
        <title>#Yoshua Bengio#1月20日Yoshua Bengio在Quora上的问答记录</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2016年1月20日Bengio在Quora上做了一次面向网友的问答，回答了83个问题，这里记录一下感兴趣的几个问题。因为不是全部翻译过来而是选取了一些我觉得有价值的内容，所以有的地方可能会失去原来的感觉，每个问题都附上了原文链接可以点击查看。目录如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. 什么是深度学习？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. 深度学习在沿着什么样的方向发展？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. 2015年读过最好的机器学习paper？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;4. 距离了解为什么深度学习有效还有多远？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5&quot;&gt;5. 深度学习研究的领域有哪些？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6&quot;&gt;6. 为什么bengio认为当前机器学习算法限制在于它们需要足够多的数据来学习？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7&quot;&gt;7. 为什么无监督重要？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#8&quot;&gt;8. 怎么看待Are ML and Statistics Complementary?这篇论文，由于深度学习机器学习是不是离统计学变远了？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#9&quot;&gt;9. 神经网络是否有概率解释？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#10&quot;&gt;10. 除了重构输入，其他的无监督学习目标还有什么?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#11&quot;&gt;11. 算法是否可能从噪声中提取有用信息？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quora原文地址: &lt;a href=&quot;https://www.quora.com/profile/Yoshua-Bengio/session/37/?__snids__=1517801181&amp;amp;__nsrc__=1&amp;amp;__filter__=all&quot;&gt;Session with Yoshua Bengio&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;1&quot;&gt;1. 什么是深度学习？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-is-Deep-Learning-3&quot;&gt;Yoshua Bengio: What is Deep Learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;深度学习是在学习多层表达来帮助学习者完成感兴趣的任务，较高级别的表达会通过深层组合计算来获取更抽象的概念。&lt;/p&gt;

&lt;h4 id=&quot;2&quot;&gt;2. 深度学习在沿着什么样的方向发展？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Where-is-deep-learning-research-headed&quot;&gt;Yoshua Bengio: Where is deep learning research headed?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;研究即探索，并不知道什么能够成功而是需要探索很多条路，因此以下是一些比较有挑战的方向。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无监督学习非常重要，而我们现在做的并不正确&lt;/li&gt;
  &lt;li&gt;深度学习会继续从传统模式识别的任务扩增到全面的人工智能任务，包括symbolic manipulation, memory, planning and reasoning，从而能够更好的理解人类的自然语言和对话（通过图灵测试）。此外还扩增到了强化学习、控制学、机器人学等领域。&lt;/li&gt;
  &lt;li&gt;人工智能方面，需要更加深入的理解人类大脑并尝试找到通过机器学习来解释大脑运作的方法&lt;/li&gt;
  &lt;li&gt;改进极大似然方法，在复杂的高维空间中，并不是绝对需要学习最优的目标&lt;/li&gt;
  &lt;li&gt;计算能力（特别是硬件）的提升会让基于深度学习的AI获利，因为AI需要特别多关于这个世界的数据和知识，并基于这些来进行推理，然后需要大型网络来训练大量数据集。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;3&quot;&gt;3. 2015年读过最好的机器学习paper？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-most-exciting-machine-learning-research-paper-you-read-in-2015&quot;&gt;Yoshua Bengio: What is the most exciting machine learning research paper you read in 2015?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1502.03167v3.pdf&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/a&gt;，因为对训练大型结构有效，并且被人们认为是标准方法&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1507.02672.pdf&quot;&gt;Semi-Supervised Learning with Ladder Networks&lt;/a&gt;,让半监督重回人们视野，特别是去噪的自编码网络很有意思&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1406.2661v1.pdf&quot;&gt;Generative Adversarial Nets&lt;/a&gt;、&lt;a href=&quot;http://arxiv.org/pdf/1511.06434v2.pdf&quot;&gt;nsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks&lt;/a&gt;等关于generative adversarial networks (GAN)，LAPGAN，DCGAN网络的论文，因为它们提出了图片生成模型，使得无监督在去年有快速的发展&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://arxiv.org/pdf/1506.03134.pdf&quot;&gt;Pointer Networks&lt;/a&gt;、&lt;a href=&quot;http://arxiv.org/pdf/1506.07503.pdf&quot;&gt;Attention-Based Models for Speech Recognition&lt;/a&gt;等基于内容的关注机制content-based attention mechanisms的文章，研究了机器翻译、神经图灵机和端到端的记忆网络等。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4&quot;&gt;4. 距离了解为什么深度学习有效还有多远？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/How-far-along-are-we-in-understanding-why-deep-learning-works&quot;&gt;Yoshua Bengio: How far along are we in understanding why deep learning works?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;bengio认为我们已经有一定的了解基础了。我们知道表示学习、深层结构、卷积结构和递归结构的每个元素对应到某个函数的参数选择（或者说某种先验信息）。我们已经有理论解释为什么这些参数选择能够起到一个重要(指数)统计优势（即能够用更少的数据获得更高的正确性）&lt;a href=&quot;http://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf&quot;&gt;On the Number of Linear Regions of Deep Neural Networks&lt;/a&gt;。我们知道为什么在训练深度网络中的优化问题并不像以前认为的那样难以解决，即绝大多数的局部最小值也是很好的解。&lt;a href=&quot;https://ganguli-gang.stanford.edu/pdf/14.SaddlePoint.NIPS.pdf&quot;&gt;Identifying and attacking the saddle point problem in high-dimensional non-convex optimization&lt;/a&gt;、&lt;a href=&quot;http://cims.nyu.edu/~achoroma/NonFlash/Papers/PAPER_AMMGY.pdf&quot;&gt;The Loss Surfaces of Multilayer Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;此外，在另外一个问题中，bengio认为当前的深度学习算法还有东西无法学到无法解决，但随着深度学习的逐渐发展，以后都能学到。&lt;/p&gt;

&lt;h4 id=&quot;5&quot;&gt;5. 深度学习研究的领域有哪些？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/What-are-the-open-research-areas-in-Deep-Learning&quot;&gt;Yoshua Bengio: What are the open  research areas in Deep Learning?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;无监督学习会有很了不起的发展，其中包括
    &lt;ul&gt;
      &lt;li&gt;从自然图片和声音中生成清晰图像和语音的生成模型&lt;/li&gt;
      &lt;li&gt;当有label的数据集不干净时半监督学习能够发挥作用&lt;/li&gt;
      &lt;li&gt;学习从数据到空间独立变量的双向变换&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;在模型中引入推理能力&lt;/li&gt;
  &lt;li&gt;大规模的自然语言理解和知识表达&lt;/li&gt;
  &lt;li&gt;多时间尺度的分层表达的模型&lt;/li&gt;
  &lt;li&gt;更好的理解某些优化问题，例如无监督学习或者有长期依赖的递归网络中产生的一些问题&lt;/li&gt;
  &lt;li&gt;训练模型将planning（能够执行what-if的情景）纳入学习过程，并且能够做决策&lt;/li&gt;
  &lt;li&gt;提升强化学习的规模&lt;/li&gt;
  &lt;li&gt;最大似然有一些缺点需要解决，例如在训练和测试条件下有错误匹配的问题&lt;/li&gt;
  &lt;li&gt;连接深度学习与生物学&lt;/li&gt;
  &lt;li&gt;加大对深度学习的理论理解（优化问题，表达和统计理论）&lt;/li&gt;
  &lt;li&gt;制造特殊的硬件，不仅仅能够离线训练模型，而且能训练更大的模型，使得模型能容纳更多信息&lt;/li&gt;
  &lt;li&gt;健康领域，存在的特殊问题是缺失数据，通过迁移学习来从其他小任务中采集数据&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;6&quot;&gt;6. 为什么bengio认为当前机器学习算法限制在于它们需要足够多的数据来学习？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/You-have-said-that-the-main-limitation-of-current-machine-learning-algorithms-is-that-they-need-too-much-data-to-learn-Can-you-elaborate-on-that&quot;&gt;Yoshua Bengio: You have said that the main limitation of current machine learning algorithms is that they need too much data to learn. Can you elaborate on that?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;人一生下来的前两年，孩子们能看到的数据实际上是没有label的，在幼年时代孩子们所接触到的自然语言比我们用来训练系统的要少的多。这是因为人类能够更好的利用少量的数据，bengio认为人类建造了一个关于这个世界的内在模型并且能够获取一些构成因果关系因子。这样能够让我们在某种假设条件下预测会发生什么，即使这些假设条件跟我们经历过的完全不一样。我们可能从来没有经历过一次车祸，但是我们能够在脑子里将它模拟出来。&lt;/p&gt;

&lt;p&gt;(笔者：这个问题我保持怀疑态度，这里面举出来的例子个人认为并不合适，比如我们没有经历车祸但是我们看到过，所以我觉得模拟出来的也差不多是我们记忆中看到的车祸。而孩子们接触到的自然语言，也没有一个量化的标准说明它比网络用来训练的少)&lt;/p&gt;

&lt;h4 id=&quot;7&quot;&gt;7. 为什么无监督重要？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Why-is-unsupervised-learning-important-What-role-does-deep-learning-have-in-solving-it&quot;&gt;Yoshua Bengio: Why is unsupervised learning important? What role does deep learning have in solving it?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;深度学习是在学习表示，获取中间概念，特征和隐藏变量的统计依赖性。这些依赖是指监督学习中的输入到输出的依赖或者无监督学习中的观测变量之间的关系。监督学习会给电脑展示非常多的例子，这些例子中会出现某些概念，然后“教”电脑知道哪些概念对我们来说很重要。但是这并不是人们学习的方式，人们在接受新概念时不一定同时有label来告诉他们，例如成年人不会告诉孩子一张图中每个像素点是什么或者每张图中每个物体是什么，也不会告诉他们听到的句子中每个词的意思和语法结构。而从简单的观察中提取大量的信息是无监督正在做的。我们希望无监督能从少量的有lable数据发现所有的概念。&lt;/p&gt;

&lt;p&gt;而科学家们也会进行无监督学习，比如他们在观察这个世界，想出一些有解释能力的模型，通过观察现象来测试它们，然后持续尝试改进围绕着我们的世界的因果模型。&lt;/p&gt;

&lt;h4 id=&quot;8&quot;&gt;8. 怎么看待Are ML and Statistics Complementary?这篇论文，由于深度学习机器学习是不是拉开了与统计学的距离？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Whats-your-opinion-on-Max-Wellings-position-paper-Are-ML-and-Statistics-Complementary-Is-ML-is-moving-away-from-statistics-due-to-deep-learning&quot;&gt;Yoshua Bengio: What’s your opinion on Max Welling’s position paper “Are ML and Statistics Complementary”? Is ML is moving away from statistics due to deep learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Max总结了三个深度学习成功的原因：计算能力,大数据集,大模型。bengio加了第四条，powerful biases，它是指函数的参数选择，也可以认为是贝叶斯理论中的先验信息）。因为在深度学习中有很多的假设：假设有很多隐藏因子，假设有很多因子的组合，equivariance(?)和时间相干性假设（卷积网络），时间平稳的假设（递归网络）等。bengio同意max认为的解释数以亿计参数的意义是不现实的，但是能理解这些隐藏的或者显在的引入网络的先验信息。因此，仍然有很多关于深度学习的理论需要被挖掘，其中统计学会占有重要地位。&lt;/p&gt;

&lt;h4 id=&quot;9&quot;&gt;9. 神经网络有效是否有概率解释？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Can-neural-networks-have-a-purely-probabilistic-interpretation-for-why-they-work&quot;&gt;Yoshua Bengio: Can neural networks have a purely probabilistic interpretation for why they work?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;可以这样理解，有监督深度学习在学习条件概率，无监督学习方法在估计数据生成的联合分布。但是由于它既包括统计问题又包括优化问题，所以为什么深度学习有效不是一个概率问题。&lt;/p&gt;

&lt;h4 id=&quot;10&quot;&gt;10. 除了重构输入，其他的无监督学习目标还有什么?&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Apart-from-trying-to-reconstruct-the-input-as-in-Autoencoder-what-other-tasks-could-prove-useful-for-unsupervised-learning-of-deep-networks&quot;&gt;Yoshua Bengio: Apart from trying to reconstruct the input (as in Autoencoder), what other tasks could prove useful for unsupervised learning of deep networks?&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在给定其他变量情况下预测一个变量（pseudolikelihood）&lt;/li&gt;
  &lt;li&gt;在给定其他变量情况下预测一小组变量(generalized pseudolikelihood)&lt;/li&gt;
  &lt;li&gt;在给定前一组变量情况下预测这一组中的某个变量(fully-visible Bayes nets, autoregressive nets, NADE, generative RNNs)&lt;/li&gt;
  &lt;li&gt;在给定一个被损坏的观测点情况下将原始干净的点还原出来(denoising)&lt;/li&gt;
  &lt;li&gt;预测输入是否来自数据产生的分布还是其他分布，类似概率分类器(Noise-Constrastive Estimation)&lt;/li&gt;
  &lt;li&gt;学习一个逆转函数&lt;/li&gt;
  &lt;li&gt;学习一个能够多次使用的复杂变换并收敛到接近数据产生的分布(Generative Stochastic Networks, generative denoising autoencoders, diffusion inversion = nonequilibrium thermodynamics)&lt;/li&gt;
  &lt;li&gt;学习产生不能被分类器区分的样本(GAN = generative adversarial networks)&lt;/li&gt;
  &lt;li&gt;极大化某个概率模型的似然函数&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;11&quot;&gt;11. 算法是否可能从噪声中提取有用信息？&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Do-you-think-that-its-possible-for-algorithms-to-extract-useful-information-from-what-is-generally-disacarded-as-noise&quot;&gt;Yoshua Bengio: Do you think that it’s possible for algorithms to extract useful information from what is generally disacarded as noise?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;只有当噪声中真正存在某种潜在的统计结构时才可以。例如猫在听人讲话觉得是噪声，人听外语也会觉得是噪声。因此当构建合适的模型让噪声也变得结构化才能提取有用信息。&lt;/p&gt;
</description>
        <pubDate>2016-01-20 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2016/01/20/bengiosession.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2016/01/20/bengiosession.html</guid>
        
        <category>resource</category>
        
      </item>
    
      <item>
        <title>【机器学习算法系列之二】浅析Logistic Regression</title>
        <description>&lt;p&gt;【转载请注明出处】&lt;a href=&quot;http://chenrudan.github.io/&quot;&gt;chenrudan.github.io&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;本文是受rickjin老师的启发，谈谈关于logistic regression的一些内容，虽然已经有珠玉在前，但还是做一下自己的总结。在查找资料的过程中，越看越觉得lr实在是博大精深，囊括的内容太多太多了，本文只能浅显的提到某些方面。文章的内容如下:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#1&quot;&gt;1. 起源&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2&quot;&gt;2. 模型介绍与公式推导&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#2.1&quot;&gt;2.1 Logistic Distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2.2&quot;&gt;2.2 Binomial logistic regression model&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3&quot;&gt;3. 解法&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#3.1&quot;&gt;3.1 梯度下降法&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.2&quot;&gt;3.2 牛顿法&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3.3&quot;&gt;3.3 BFGS&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4&quot;&gt;4. 正则化&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#4.1&quot;&gt;4.1 过拟合&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4.2&quot;&gt;4.2 正则化的两种方法&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5&quot;&gt;5. 逻辑回归与其他模型关系&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#5.1&quot;&gt;5.1 逻辑回归与线性回归&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.2&quot;&gt;5.2 逻辑回归与最大熵&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.3&quot;&gt;5.3 逻辑回归与svm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.4&quot;&gt;5.4 逻辑回归与朴素贝叶斯&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5.5&quot;&gt;5.5 逻辑回归与能量函数&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#6&quot;&gt;6. 并行化&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#7&quot;&gt;7. 小结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#8&quot;&gt;8. 引用&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1&quot;&gt;1. 起源&lt;/h3&gt;

&lt;p&gt;logistic regression的起源主要分为几个阶段，从开始想到logistic这个词，到发现logistic function，再推导出logit function，最后才命名logistic regression。这些过程都是大量的研究者们共同努力发现的，只是在历史的长河中，很多人被渐渐遗忘了。&lt;/p&gt;

&lt;p&gt;logistic起源于对人口数量增长情况的研究，最重要的工作是Pierre François Verhulst在1838年提出了对人口增长的公式描述(这人是个比利时人，写的文章是法语的，一个字都看不懂，下面的内容都是看了一篇将研究人口数量增长发展历程的书[1]才知道的…)，他博士毕业于根特大学的数学系，是个数学教授和人口学家。在1835年Verhulst的同乡人Adolphe Quetelet发表了一篇关于讨论人口增长的文章，文中认为人口不可能一直是几何(指数)增长，而会被与增长速度平方成比例的一种阻力而影响，但是这篇论文只有猜想没有数学理论基础，却极大的启发了Verhulst。因此在1838年Verhulst发表了关于人口数量增长的论文，就是在这篇论文里面他推导出了logistic equation，文章中谈到一个重要观点，随着时间的增加，一个国家的大小（我理解为资源）和这个国家人们的生育能力限制了人口的增长，人口数量会渐渐趋近一个稳定值。厉害的是他将这个过程用公式给描述出来了，他从人口数量增长的速度公式入手，即人口数量$P(t)$对时间t的导数:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\frac{\partial P}{\partial t} = rP(1-\frac{P}{K})
&lt;/script&gt;

&lt;p&gt;其中$K$就是他认为人口数量稳定的值，当$P(t)$远小于$K$时，求导公式后一项约等于0，那么就变成了$\frac{\partial P}{\partial t} \simeq  rP$，这个阶段人口增长速度与人口数量和一个常数的乘积成正比，并且在渐渐变大。然后对这个式子求解一阶线性微分方程得到$P(t)\simeq P(0)e^{rt}$。当$P(t)$接近$K$时，人口增长速度开始渐渐变小，同样求解二阶微分方程(论文中是将二阶转化成一阶求解)，然后将二者整合在一起得到最初的形式。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(t) = \frac{P(0)e^{rt}}{1+P(0)(e^{rt}-1)/K}
&lt;/script&gt;

&lt;p&gt;他将法国英国等过十几年的人口实际数据拿来跟这个公式对比之后发现确实拟合的很不错。但他当时并没有那么多年的数据，下图1是在他过世以后人们总结的300年来的人口增长分布，可以看到非常漂亮的拟合了logisitc分布的累积分布函数走势。但是当时这个公式并没有名字，直到1845年他发表了另外一篇重要文章[2]，他给这个公式起了一个名字——”logistic”，此外在这篇文章中，他发现在$P(t)&amp;lt;K/2$的时候，$P(t)$呈凸增长趋势，在$P(t)&amp;gt;K/2$时$P(t)$呈凹增长(通过求二阶导来分析，这里略)。这个增长的趋势类似logistic分布的概率密度函数。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr1.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 比利时的人口增长数量图&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;然而在后来的几十年内人们都没有意识到这个工作的重要性，很多人都独立的研究出了这个增长现象，直到1922年一个叫做Raymond Pearl的人口学家注意到Verhulst在1838年就已经提出了这个现象和公式，并在他的文章中也使用了logistic function来称呼它，并且沿用至今。在1920年Pearl[3]在研究美国人口增长规律时提出了另外一种表示logistic function的方法。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
y = \frac{be^{ax}}{1+ce^{ax}}
&lt;/script&gt;

&lt;p&gt;基于这个表达式，Joseph Berkson在1944年提出了logit function，$logit = In(\frac{1-Q}{Q})$，假如$Q = \frac{1}{1+e^{a-bx}}$，结果就是$logit=a-bx$。&lt;/p&gt;

&lt;p&gt;后来，在1958年David Cox提出了logistic regression[4]。他的文章是为了解决这样一个问题，有一组取值为0，1的观测值，它们的取值$Y_i$依赖于一些独立变量$x_i$， 当$Y_i=1$时对应的概率为$\theta_i=pr(Y_i = 1)$。由于$\theta_i$限制在[0,1]之间，因此假设$\theta_i$与$x_i$的关系符合logit function，即$logit\theta_i \equiv log{\frac{\theta_i}{1-\theta_i}} = \alpha + \beta x_i$，文章主要在分析如何求解里面的参数$\beta$，这里就不提了。由于用到了logistic function，而这个问题本身又个回归问题(建立观测值与独立变量之间的关系)，因而它被称呼为logistic regression。&lt;/p&gt;

&lt;p&gt;貌似Cox在这篇文章中并不是刻意提出logistic regression，但确实这个词第一次出现就是在这篇文章中，虽然Cox之前已经有很多人做过这方面的研究了，但是他们没给个名字，因此Cox成了提出logistic regression的人。这个故事告诉我们一个道理，无论是发文章还是写软件一定要取一个言简意赅又好听又好记的名字…&lt;/p&gt;

&lt;p&gt;以上是逻辑回归的历史发展中比较有代表性的几件事(我认为的…还有好多论文没时间细看…)，J.S Cramer[5]在他的文章中有更加详细的讨论。它是由数学家对人口发展规律研究得出，后来又被应用到了微生物生长情况的研究，后来又被应用解决经济学相关问题，直到发展到今天作为一个非常重要的算法而存在于各行各业。逻辑回归作为Regression Analysis的一个分支，它实际上还受到很多Regression Analysis相关技术的启发，例如Berkson就是基于probit function提出的logit function。光它的起源到应用就能写一本书出来了，难怪rickjin老师说lr其实非常非常复杂…&lt;/p&gt;

&lt;h3 id=&quot;2&quot;&gt;2.模型介绍与公式推导&lt;/h3&gt;

&lt;p&gt;上面说过了逻辑斯蒂回归的起源，下面讨论一下完整的模型，首先介绍一下何为逻辑斯蒂分布，再由逻辑斯蒂分布推出逻辑回归。&lt;/p&gt;

&lt;h4 id=&quot;2.1&quot;&gt;2.1 Logistic Distribution&lt;/h4&gt;

&lt;p&gt;随机变量X服从逻辑斯蒂分布，即X的累积分布函数为上文提到过的logistic function。对分布函数求导得到了概率密度函数。公式如下，参数影响参考图2(图来自维基百科，它的参数s就是统计学习方法上的$\gamma$)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
F(x) = P(X \leqslant x) = \frac{1}{1+e^{-(x-\mu)/\gamma}}
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
f(x) = F'(x) = \frac{e^{-(x-\mu)/\gamma}} { \gamma (1+e^{-(x-\mu)/\gamma})^2 }
&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr3.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 不同参数对logistic分布的影响&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;可以看到$\mu$影响的是中心对称点的位置，$\gamma$越小中心点附近增长的速度越快。而常常在深度学习中用到的非线性变换sigmoid函数是逻辑斯蒂分布的$\gamma=1,\mu=0$的特殊形式。&lt;/p&gt;

&lt;h4 id=&quot;2.2&quot;&gt;2.2 Binomial logistic regression model&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr2.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;370&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图3 数据示例&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;逻辑回归是为了解决分类问题，根据一些已知的训练集训练好模型，再对新的数据进行预测属于哪个类。如图3所示，有一些属于两个类的数据，目标是判断圆圈属于哪一类。也就是说逻辑回归的目标是找到一个有足够好区分度的决策边界，从而能够将两类很好的分开。假设已经存在这样一个边界，针对于图中这种线性可分的情况，这条边界是
输入特征向量的线性组合，假设输入的特征向量为$x\in R^n$(图中输入向量为二维)，$Y$取值为0，1。那么决策边界可以表示为$w_1x_1+w_2x_2+b=0$，假如存在一个例子使得$h_w(x) = w_1x_1+w_2x_2+b &amp;gt; 0$，那么可以判断它类别为1，这个过程实际上是感知机，即只通过决策函数的符号来判断属于哪一类。而逻辑回归需要再进一步，它要找到分类概率$P(Y=1)$与输入向量$x$的直接关系，通过比较概率值来判断类别，也就是上文中的logit function，因此产生了逻辑回归，将决策函数的输出值映射到概率值上。最基础的二分类问题，对逻辑回归而言就是二项逻辑斯蒂回归，从而某组输入向量$x$下导致产生不同类的概率为:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(Y=1|x)=\frac{e^{w\cdot x+b}}{1+e^{w\cdot x+b}} \:\:\:\:\:\:\:\:\:(1)
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(Y=0|x)=\frac{1}{1+e^{w\cdot x+b}} \:\:\:\:\:\:\:\:\:(2)
&lt;/script&gt;

&lt;p&gt;其中$w$称为权重，$b$称为偏置，其中的$w\cdot x+b$看成对$x$的线性函数。然后对比上面两个概率值，概率值大的就是x对应的类。有时候为了书写方便，会将$b$写入$w$，即$w=(w_0, w_1, …, w_n)$其中$w_0=b$，并取$x_0 = 1$。又已知一个事件发生的几率odds是指该事件发生与不发生的概率比值，二分类情况下即$\frac {P(Y=1|x)}{P(Y=0|x)} = \frac {P(Y=1|x)}{1-P(Y=1|x)}$。取odds的对数就是上面提到的logit function，$logit(P(Y=1|x)) = log\frac {P(Y=1|x)}{1-P(Y=1|x)} = w\cdot x$。从而可以得到一种对逻辑回归的定义，&lt;strong&gt;&lt;em&gt;输出$Y=1$的对数几率是由输入$x$的线性函数表示的模型，即逻辑斯蒂回归模型&lt;/em&gt;&lt;/strong&gt;(李航.《统计机器学习》)。而直接考察公式1可以得到另一种对逻辑回归的定义，&lt;strong&gt;&lt;em&gt;线性函数的值越接近正无穷，概率值就越接近1；线性值越接近负无穷，概率值越接近0，这样的模型是逻辑斯蒂回归模型&lt;/em&gt;&lt;/strong&gt;(李航.《统计机器学习》)。因此逻辑回归的思路是，先拟合决策边界(这里的决策边界不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。这里有个非常棒的博文[6]推荐，阐述了逻辑回归的思路。&lt;/p&gt;

&lt;p&gt;有了上面的分类概率，就可以建立似然函数，通过极大似然估计法来确定模型的参数。设$P(Y=1|x)=h_w (x)$，似然函数为$\prod [h_w(x_i)]^{y_i}[1-h_w(x_i)]^{(1-y_i)}$，对数似然函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
L(w) = \sum _{i=1}^{N}logP(y_i|x_i;w) = \sum_{i=1}^{N}[y_ilog h_w (x_i) +(1-y_i)log(1-h_w(x_i))] \:\:\:\:\:\:\:\:\:(3)
&lt;/script&gt;

&lt;h3 id=&quot;3&quot;&gt;3.解法&lt;/h3&gt;

&lt;p&gt;优化逻辑回归的方法有非常多[7]，有python的不同实现[8]，这里只谈谈梯度下降，牛顿法和BFGS。优化的主要目标是找到一个方向，参数朝这个方向移动之后使得似然函数的值能够减小，这个方向往往由一阶偏导或者二阶偏导各种组合求得。逻辑回归的损失函数是&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
min J(w) = min \{-\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))]\} \:\:\:\:\:\:\:\:\:(4)
&lt;/script&gt;

&lt;p&gt;先把$J(w)$对$w_j$的一阶二阶偏导求出来，且分别用$g$和$H$表示。$g$是梯度向量，$H$是海森矩阵。这里只考虑一个实例$y_i$产生的似然函数对一个参数$w_j$的偏导。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
g_j = \frac{\partial J(w)} {\partial w_j} = \frac{y^{(i)}}{h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))(-x_{j}^{(i)})+(1-y^{(i)})\frac {1}{1-h_w(x^{(i)})}h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}=(y^{(i)}-h_w(x^{(i)}))x^{(i)}   \:\:\:\:\:\:\:\:\:(5)
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
H_{mn} = \frac {\partial^2 J(w)} {\partial w_m \partial w_n} =h_w(x^{(i)})(1-h_w(x^{(i)}))x^{(i)}_mx^{(i)}_n \:\:\:\:\:\:\:\:\:(6)
&lt;/script&gt;

&lt;p&gt;这几种方法一般都是采用迭代的方式来逐步逼近极小值，需要给定参数$w_0$作为起点，并且需要一个阈值$\epsilon$来判断迭代何时停止。&lt;/p&gt;

&lt;h4 id=&quot;3.1&quot;&gt;3.1 梯度下降法&lt;/h4&gt;

&lt;p&gt;梯度下降是通过$J(w)$对$w$的一阶导数来找下降方向，并且以迭代的方式来更新参数，更新方式为$w_j^{k+1} = w_j^k + \alpha g_j$，$k$为迭代次数。每次更新参数后，可以通过比较$||J(w^{k+1})-J(w^k)||$或者$||w^{k+1}-w^k||$与某个阈值$\epsilon $大小的方式来停止迭代，即比阈值小就停止。&lt;/p&gt;

&lt;h4 id=&quot;3.2&quot;&gt;3.2 牛顿法&lt;/h4&gt;

&lt;p&gt;牛顿法的基本思路是，&lt;strong&gt;&lt;em&gt;在现有极小点估计值的附近对f(x)做二阶泰勒展开，进而找到极小点的下一个估计值&lt;/em&gt;&lt;/strong&gt;[9]。假设$w^k$为当前的极小值估计值，那么有&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\varphi (w) = J(w^k) + J'(w^k)(w-w^k)+\frac{1}{2}J''(w^k)(w-w^k)^2  \:\:\:\:\:\:\:\:\:(7)
&lt;/script&gt;

&lt;p&gt;然后令$\varphi’(w)=0$，得到了$w=w^k-\frac{J’(w^k)}{J''(w^k)}$。因此有迭代更新式，&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
w^{k+1} = w^k - \frac{J'(w^k)}{J''(w^k)} = w^k - H_k^{-1}\cdot g_k \:\:\:\:\:\:\:\:\:(8)
&lt;/script&gt;

&lt;p&gt;此方法中也需要一个阈值$\epsilon$，当$||g_k|| &amp;lt; epsilon$时停止迭代。此外，这个方法需要目标函数是二阶连续可微的，本文中的$J(w)$是符合要求的。&lt;/p&gt;

&lt;h4 id=&quot;3.3&quot;&gt;3.3 BFGS&lt;/h4&gt;

&lt;p&gt;由于牛顿法中需要求解二阶偏导，这个计算量会比较大，而且有时目标函数求出的海森矩阵无法保持正定，因此提出了拟牛顿法。拟牛顿法是一些算法的总称，它们的目标是通过某种方式来近似表示森海矩阵(或者它的逆矩阵)。例如BFGS就是一种拟牛顿法，它是由四个发明人的首字母组合命名，是求解无约束非线性优化问题最常用的方法之一。目标是用迭代的方式逼近海森矩阵$H$，假设这个逼近值为$B^k\approx H^k$，那么希望通过计算$B^{k+1} = B^k + \Delta B^k$能够达到目的。并且假设$\Delta B^k = \alpha uu^T + \beta vv^T$，而由3.2可知，$\Delta w = w^{k+1}-w^k = (H^{-1})^{k+1}(g^{k+1}-g^k) = (H^{-1})^k\Delta g$，将$B^{k+1}$的更新式代入，可以得到&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\Delta g = B^k\Delta g + (\alpha u^T\Delta w)u + (\beta v^T\Delta w)v  \:\:\:\:\:\:\:\:\:(9)
&lt;/script&gt;

&lt;p&gt;此处，直接令$\alpha u^T\Delta w=1$、$\beta v^T\Delta w=-1$、$u=\Delta g$和$v=B^k\Delta w$，那么可以求得$\alpha = \frac {1}{(\Delta g)^T\Delta w}$和$\beta = -\frac{1}{(\Delta w)^TB^k\Delta w}$。从而再代入求$\Delta B^k$的式子就可以得到更新的式子&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\Delta B^k = \frac {\Delta g(\Delta g)^T}{(\Delta g)^T\Delta w} - \frac {B^k\Delta w(\Delta w)^TB^k}{(\Delta w)^TB^k\Delta w}  \:\:\:\:\:\:\:\:\:(10)
&lt;/script&gt;

&lt;p&gt;这里还会对(10)进行变换，通过Sherman-Morrison公式直接求出$(B^{-1})^{k+1}$与$(B^{-1})^k$，用$D^{k+1}$和$D^k$来表。更新公式变成了&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D^{k+1} = (I-\frac {\Delta w (\Delta g)^T}{(\Delta g)^T\Delta w}) D^k (I - \frac {\Delta g (\Delta w)^T}{(\Delta g)^T\Delta w}) + \frac {\Delta w (\Delta w)^T}{(\Delta g)^T\Delta w} \:\:\:\:\:\:\:\:\:(11)&lt;/script&gt;

&lt;p&gt;用BFGS来更新参数的流程如下:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;确定改变量，$(\Delta w)^k= -D^k\cdot g^k$&lt;/li&gt;
  &lt;li&gt;更新参数，$w^{k+1} = w^k + \lambda (\Delta w)^k$&lt;/li&gt;
  &lt;li&gt;求出$\Delta g = g^{k+1} -g^k$&lt;/li&gt;
  &lt;li&gt;由(11)求出$D^{k+1}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;式子的系数$\lambda=argmin J(w^k + \lambda (\Delta w)^k)$，即在求得下降方向上来从很多值中搜索最优的下降大小，这里我觉得可以用学习率替代。因此，这个更新方法跟牛顿法的区别是，它是在更新参数$w$之后更新一下近似森海矩阵的值，而牛顿法是在更新$w$之前完全的计算一遍森海矩阵。还有一种从计算上改进BFGS的方法称为L-BFGS，不直接存储森海矩阵，而是通过存储计算过程中产生的部分$\Delta w(g)_{k-m+1,k-m+2,…,k}$，从而减少了参数存储所需空间。&lt;/p&gt;

&lt;h3 id=&quot;4&quot;&gt;4.正则化&lt;/h3&gt;

&lt;p&gt;正则化不是只有逻辑回归存在，它是一个通用的算法和思想，所以会产生过拟合现象的算法都可以使用正则化来避免过拟合，在谈正则化之前先聊聊什么是过拟合。&lt;/p&gt;

&lt;h4 id=&quot;4.1&quot;&gt;4.1 过拟合&lt;/h4&gt;

&lt;p&gt;之前的模型介绍和算法求解可以通过训练数据集(图2中的三角形和星形)将分类模型训练好，从而可以预测一个新数据(例如图2中的粉色圆圈)的分类，这种对新数据进行预测的能力称为泛化能力。而对新数据预测的结果不好就是泛化能力差，一般来说泛化能力差都是由于发生了过拟合现象。过拟合现象是指对训练数据预测很好但是对未知数据预测不行的现象，通常都是因为模型过于复杂，或者训练数据太少。即当$\frac{complexity\: of \:the \:model}{training \:set \:size}$比值太大的情况下会发生过拟合。模型复杂体现在两个方面，一是参数过多，二是参数值过大。参数值过大会导致导数非常大，那么拟合的函数波动就会非常大，即下图所示，从左到右分别是欠拟合、拟合和过拟合。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr4.jpg&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图4 同样数据下欠拟合，拟合和过拟合&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;在模型过于复杂的情况下，模型会学习到很多特征，从而导致可能把所有训练样本都拟合到，就像上图中一样，拟合的曲线将每一个点都正确的分类了。举个例子，假如要预测一个房子是贵还是便宜，房子的面积和所属的地区是有用的特征，但假如训练集中刚好所有贵的房子都是开发商A开发，便宜的都是开发商B开发，那么当模型变复杂能学习到的特征变多之后，房子是哪个开发商的会被模型认为是个有用特征，但是实际上这点不能成为判断的标准，这个现象就是过拟合。因此在这个例子中可以看到，解决的方法有两个，一个是减少学习的特征不让模型学到开发商的特征，一是增加训练集，让训练集有贵房子是B开发的样本。&lt;/p&gt;

&lt;p&gt;从而，解决过拟合可以从两个方面入手，一是减少模型复杂度，一是增加训练集个数。而正则化就是减少模型复杂度的一个方法。&lt;/p&gt;

&lt;h4 id=&quot;4.2&quot;&gt;4.2 正则化的两种方法&lt;/h4&gt;

&lt;p&gt;由于模型的参数个数一般是由人为指定和调节的，所以正则化常常是用来限制模型参数值不要过大，也被称为惩罚项。一般是在目标函数(经验风险)中加上一个正则化项$\Phi(w)$即&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
J(w) = -\frac{1}{m}[\sum_{i=1}^{m}y_ilog h_w (x_i) + (1-y_i)log(1-h_w(x_i))] + \lambda \Phi(w) \:\:\:\:\:\:\:\:\:(12)
&lt;/script&gt;

&lt;p&gt;而这个正则化项一般会采用L1范数或者L2范数。其形式分别为$\Phi (w)=||x||_1$和$\Phi (w)=||x||_2 $。&lt;/p&gt;

&lt;p&gt;首先针对L1范数$\phi (w) = |w|$，当采用梯度下降方式来优化目标函数时，对目标函数进行求导，正则化项导致的梯度变化为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial \Phi(w)}{\partial w_j} = \left\{\begin{matrix} 1 &amp; w_j&gt;0\\  -1 &amp; w_j&lt;0  \end{matrix}\right. \:\:\:\:\:\:\:\:\:(13)
 %]]&gt;&lt;/script&gt;

&lt;p&gt;从而导致的参数$w_j$减去了学习率与(13)式的乘积，因此当$w_j$大于0的时候，$w_j$会减去一个正数，导致$w_j$减小，而当$w_j$小于0的时候，$w_j$会减去一个负数，导致$w_j$又变大，因此这个正则项会导致参数$w_j$取值趋近于0，也就是为什么L1正则能够使权重稀疏，这样参数值就受到控制会趋近于0。L1正则还被称为 Lasso regularization。&lt;/p&gt;

&lt;p&gt;然后针对L2范数$\phi(w) = \sum_{j=1}^{n}w_j^2$，同样对它求导，得到梯度变化为$\frac{\partial \Phi(w)}{\partial w_j} = 2w_j$(一般会用$\frac{\lambda}{2}$来把这个系数2给消掉)。同样的更新之后使得$w_j$的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。&lt;/p&gt;

&lt;p&gt;需要注意的是，L1正则化会导致参数值变为0，但是L2却只会使得参数值减小，这是因为L1的导数是固定的，参数值每次的改变量是固定的，而L2会由于自己变小改变量也变小。而(12)式中的$\lambda$也有着很重要的作用，它在权衡拟合能力和泛化能力对整个模型的影响，$\lambda$越大，对参数值惩罚越大，泛化能力越好。&lt;/p&gt;

&lt;p&gt;此外，从贝叶斯的角度而言，正则化项实际上是给了模型一个先验知识，L2正则相当于添加了一个均值为0协方差为$1/\lambda$的高斯分布先验(将L2正则表示为$\frac{\lambda}{2}w^Tw$)，当$\lambda$为0，即不添加正则项，那么可以看成协方差是无穷大，$w$可以不受控制变成任意大。当$\lambda$越大，即协方差越小，那么参数值的取值方差会变小，模型会趋向于稳定(参考[10]最高票答案)。&lt;/p&gt;

&lt;h3 id=&quot;5&quot;&gt;5. 逻辑回归与其他模型的关系&lt;/h3&gt;

&lt;h4 id=&quot;5.1&quot;&gt;5.1 逻辑回归与线性回归&lt;/h4&gt;

&lt;p&gt;在谈两者关系之前，需要讨论的是，逻辑回归中使用到的sigmoid函数到底起到了什么作用。下图的例子中，需要判断肿瘤是恶性还是良性，其中横轴是肿瘤大小，纵轴是线性函数$h_w(x)=w^Tx+b$的取值，因此在左图中可以根据训练集(图中的红叉)找到一条决策边界，并且以0.5作为阈值，将$h_w(x) \geqslant 0.5$情况预测为恶性肿瘤，这种方式在这种数据比较集中的情况下好用，但是一旦出现如右图中的离群点，它会导致学习到的线性函数偏离(它产生的权重改变量会比较大)，从而原先设定的0.5阈值就不好用了，此时要么调整阈值要么调整线性函数。如果我们调节阈值，在这个图里线性函数取值看起来是0～1，但是在其他情况下可能就是从$-\infty$到$\infty$，所以阈值的大小很难确定，假如能够把$w^Tx+b$的值变换到一个能控制的范围那么阈值就好确定了，所以找到了sigmoid函数，将$w^Tx+b$值映射到了(0,1)，并且解释成概率。而如果调节线性函数，那么最需要的是减少离群点的影响，离群点往往会导致比较大的$|w^Tx+b|$值，通过sigmoid函数刚好能够削弱这种类型值的影响，这种值经过sigmoid之后接近0或者1，从而对$w_j$的偏导数为$h_w(x^{(i)})(1-h_w(x^{(i)}))x_j^{(i)}$，无论接近0还是1这个导数都是非常小的。因此可以说sigmoid在逻辑回归中起到了两个作用，一是将线性函数的结果映射到了(0,1)，一是减少了离群点的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr5.jpg&quot; alt=&quot;1&quot; height=&quot;55%&quot; width=&quot;55%&quot; hspace=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图5 良性恶性肿瘤分类&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;有了上面的分析基础，再来看看逻辑回归和线性回归的关系(线性回归我这里就不展开说了，不清楚的可以看看[11])，有的人觉得逻辑回归本质上就是线性回归，它们俩都要学习一个线性函数，逻辑回归无非是多加了一层函数映射，但是我对线性回归的理解是在拟合输入向量x的分布，而逻辑回归中的线性函数是在拟合决策边界，它们的目标是不一样的。所以我不觉得逻辑回归比线性回归好，它们俩要解决的问题不一样。但它们都可以用一个东西来概括，那就是广义线性模型GLM(Generalized linear models)[12]。先介绍何为指数簇(exponential family)，当某个随机变量的概率分布可以表示为$p(y;\eta )=b(y)exp(\eta^TT(y)-a(\eta))$时就可以说它属于指数簇，通过调整$\eta$可以获得不同的分布。对应于线性回归与逻辑回归的高斯分布与伯努利分布就是属于指数簇的，例如取$T(y)=y$、$a(\eta)=-log(1-\phi) = log(1+e^\eta)$以及$b(y)=1$代入上式就可以得到逻辑回归的损失函数$J(w) = \frac {1}{2} \sum_{i=1}^{m} (h_w(x^{(i)})-y^{(i)})^2$。&lt;/p&gt;

&lt;p&gt;GLM需要满足下面三个条件。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;在给定观测值x和参数w情况下，输出y服从参数为$\eta$的指数簇分布&lt;/li&gt;
  &lt;li&gt;预测的值$h_w(x) = E[y|x]$&lt;/li&gt;
  &lt;li&gt;$\eta = w^Tx$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;因此，选择合适的参数就能分析出线性回归和逻辑回归都是GLM的一种特例，有时会看到有的人会从GLM出发将逻辑回归的公式给推导出来。总之，线性回归和逻辑回归是属于同一种模型，但是它们要解决的问题不一样，前者解决的是regression问题，后者解决的是classification问题，前者的输出是连续值，后者的输出是离散值，而且前者的损失函数是输出y的高斯分布，后者损失函数是输出的伯努利分布。&lt;/p&gt;

&lt;h4 id=&quot;5.2&quot;&gt;5.2 逻辑回归与最大熵&lt;/h4&gt;

&lt;p&gt;最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。为了证明最大熵模型跟逻辑回归的关系，那么就要证明两者求出来的模型是一样的，即求出来的h(x)的形式应该是一致的。由于最大熵是通过将有约束条件的条件极值问题转变成拉格朗日对偶问题来求解，模型的熵为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\sum_{v=1}^k\sum_{i=1}^m h(x^{(i)})_v log(h(x^{(i)})_v) \:\:\:\:\:\:\:\:\:(14)&lt;/script&gt;

&lt;p&gt;并假设约束条件如下，其中$v,u$是输出类别的index，$j$是对应输入向量$x$的index，$A(u,y^{(i)})$是指示函数，两个值相等输出1，其他输出0[13]。而第三个约束是通过令公式(5)等于0得来的，它的意义是参数$w_{u,j}$最好的取值是让每一个样本i对应$h(x^{(i)})_u$的行为接近指示函数$A(u,y^{(i)})$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[

\begin{cases}
h(x)_v\geqslant 0 &amp; \text{ always } \\ 
\sum_{v=1}^k h(x)_v = 1 &amp; \text{ always } \\ 
\sum_{i=1}^m h(x^{(i)})_u x^{(i)}_j = \sum_{i=1}^m A(u,y^{(i)})x^{(i)}_j &amp; \text{ for all } \: u, j
\end{cases} \:\:\:\:\:\:\:\:\:(15)
 %]]&gt;&lt;/script&gt;

&lt;p&gt;通过约束条件(15)可以直接推导出softmax的公式。基于这一点，再回过头来看《统计学习方法》上的约束条件，如果假设$P(y|x) = h(x)$，公式左边的$f(x,y)$实际上取值一直为1，那么这两个约束条件实际上是一样的。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\sum_{x,y} \widetilde{P(x)}P(y&lt;/td&gt;
      &lt;td&gt;x)f(x,y) = \sum_{x,y} \widetilde{P(x,y)}f(x,y) :::::::::(16) $$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;因此，可以这样说，最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。此外，最大熵与逻辑回归都称为对数线性模型(log linear model)。&lt;/p&gt;

&lt;h4 id=&quot;5.3&quot;&gt;5.3 逻辑回归与svm&lt;/h4&gt;

&lt;p&gt;逻辑回归和svm作为经典的分类算法，被放在一起讨论的次数特别多，知乎和Quora上每种意见都非常有意思都从不同角度有分析，建议都可以看看[14][15][16]。这里只讨论一些我赞同的观点。要是不清楚svm的由来，建议看JerryLead的系列博客[17]，我这里就不提了。&lt;/p&gt;

&lt;p&gt;相同点:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;都是分类算法&lt;/li&gt;
  &lt;li&gt;都是监督学习算法&lt;/li&gt;
  &lt;li&gt;都是判别模型&lt;/li&gt;
  &lt;li&gt;都能通过核函数方法针对非线性情况分类&lt;/li&gt;
  &lt;li&gt;目标都是找一个分类超平面&lt;/li&gt;
  &lt;li&gt;都能减少离群点的影响&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不同点:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss&lt;/li&gt;
  &lt;li&gt;逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。&lt;/li&gt;
  &lt;li&gt;逻辑回归对概率建模，svm对分类超平面建模&lt;/li&gt;
  &lt;li&gt;逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有&lt;/li&gt;
  &lt;li&gt;逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响&lt;/li&gt;
  &lt;li&gt;逻辑回归是统计方法，svm是几何方法&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;5.4&quot;&gt;5.4 逻辑回归与朴素贝叶斯&lt;/h4&gt;

&lt;p&gt;这两个算法有一些相似之处，并且在对比判别模型和生成模型，它们作为典型的分类算法经常被提及，因此这里也做一个小小的总结。&lt;/p&gt;

&lt;p&gt;相同点是，它们都能解决分类问题和都是监督学习算法。此外，有意思的是，当假设朴素贝叶斯的条件概率$P(X|Y=c_k)$服从高斯分布时Gaussian Naive Bayes，它计算出来的$P(Y=1|X)$形式跟逻辑回归是一样的[18]。&lt;/p&gt;

&lt;p&gt;不同的地方在于，逻辑回归为判别模型求的是$p(y|x)$，朴素贝叶斯为生成模型求的是$p(x,y)$。前者需要迭代优化，后者不需要。在数据量少的情况下后者比前者好，数据量足够的情况下前者比后者好。由于朴素贝叶斯假设了条件概率$P(X|Y=c_k)$是条件独立的，也就是每个特征权重是独立的，如果数据不符合这个情况，朴素贝叶斯的分类表现就没有逻辑回归好。&lt;/p&gt;

&lt;h4 id=&quot;5.5&quot;&gt;5.5 逻辑回归与能量模型&lt;/h4&gt;

&lt;p&gt;(3月3日补充)&lt;/p&gt;

&lt;p&gt;基于能量的模型不是一个具体的算法，而是一种框架思想，它认为输入输出变量之间的依赖关系用一个值表示$E(x,y)$，这个值称为能量，对关系建模的这个函数叫能量函数 。如果在保持输入变量不变的情况下，对应正确输出时能量低，对应错误输出时能量高，那么这个模型就是有用的。&lt;/p&gt;

&lt;p&gt;因此当给定了训练集S，能量模型的构造和训练由四部分组成[19]:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;有合适的能量函数$E(W,Y,X)$&lt;/li&gt;
  &lt;li&gt;inference算法，针对一个给定的输入变量X和能量函数形式，找到一个Y值使得能量最小，即$Y^* = argmin_{Y\in y}E(W,Y,X) $&lt;/li&gt;
  &lt;li&gt;有loss函数$L(W,S)$，用来衡量在训练集S下能量函数的好坏&lt;/li&gt;
  &lt;li&gt;learning算法，用来找合适的参数W，在一系列能量函数中选择让损失函数最小化的能量函数。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看出13步通过选择不同的能量函数和损失函数来构造不同的模型，24步是如何训练这样的一个模型。&lt;/p&gt;

&lt;p&gt;因此当我们假设要解决二分类问题时，y取值为-1和1，如果假设能量函数为$E(W,Y,X)=-YG_w(X)=W^TX$，损失函数采用negative log-likelihood loss，那么可以求得损失函数具体形式为$L&lt;em&gt;{nll}(W,S)=\frac{1}{P}\sum&lt;/em&gt;{i=1}^{P}log(1+exp(-2Y^iW^TX))$，这个形式与把(1)(2)代入公式(3)后得到的损失函数公式是一致的，说明这种组合下，产生的算法就是逻辑回归。因此逻辑回归是能量模型的一种特例。&lt;/p&gt;

&lt;p&gt;而此处，同样针对二分类问题，假如能量函数保持不变，损失函数采用hinge loss，并加上一个正则化项，那么就能够推导出SVM的损失函数表达式(SVM的核函数体现在能量函数中，这里为了方便解释没有具体展开说)。这也是为什么说逻辑回归与svm最本质的区别就是损失函数不同。&lt;/p&gt;

&lt;h3 id=&quot;6&quot;&gt;6. 并行化&lt;/h3&gt;

&lt;p&gt;由于找不到特别多的并行化资料，这里就分析一下博主冯扬给出的实现[20]。实际上逻辑回归的并行化最主要的目标就是计算梯度。将目标的label变为-1和1，那么梯度公式可以整合在一起变成$\sum_{i=1}^M(\frac{1}{1+exp(y^{(i)}w^Tx^{(i)})}-1)y^{(i)}x^{(i)}$，梯度计算里面最主要的就是矩阵乘法，一般的做法都是想办法将矩阵切割成大小合适的块。针对二分类，现在有M个样本，N个特征，假如有m*n个计算节点，并且将计算节点排列成m行n列，那么每个节点分配M/m个样本，N/n个特征，如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr6.jpg&quot; alt=&quot;1&quot; height=&quot;55%&quot; width=&quot;55%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图6 并行LR的数据分割&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;原文的标示我不太习惯，下面都改成了ij，并画出了矩阵运算的过程图。其中$X_{(i,j)}, i\in[1, m],j\in[1,n]$表示输入数据被分块后的第i行第j列的块。&lt;/p&gt;

&lt;p&gt;$X_{(i,j),k}$表示这个块中的第k行，$W_j$表示参数的第j块。&lt;/p&gt;

&lt;p&gt;第一步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_{(i,j),k}=W^T_jX_{(i,j),k}&lt;/script&gt;

&lt;p&gt;第二步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;d_{i,k} = \sum_{j=1}{n}d_{(i,j),k}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr7.png&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图7 并行LR的求梯度12步&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;第三步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{(i,j)} = \sum_{k=1}^{M/m}(\frac {1}{1+exp(y_{i,k} d_{i,k})} - 1) y_{i,k}X_{(i,j),k}&lt;/script&gt;

&lt;p&gt;第四步计算&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;G_{j} = \sum_{i=1}^mG_{(i,j)}&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/lr8.png&quot; alt=&quot;1&quot; height=&quot;65%&quot; width=&quot;65%&quot; hspace=&quot;150&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图6 并行LR的求梯度34步&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;从而，经过上面的分解步骤可以将逻辑回归来做并行化计算。&lt;/p&gt;

&lt;h3 id=&quot;7&quot;&gt;7. 总结&lt;/h3&gt;

&lt;p&gt;这篇文章写了好几天，有时候写着写着就把自己绕进去了，因为可以展开说的地方太多了，写完这些内容，我又找了一些面试题看了看，理论部分基本上都能覆盖到了，但是涉及到真正的应用还是要再花时间去了解，最后的并行化理解还不够透彻，矩阵乘法我用gpu实现过，但是并没有接触过海量的数据，也不知道真正的问题会发生在什么地方。逻辑回归可以从很多方面来解释来理解，确实是个很美丽的算法。&lt;/p&gt;

&lt;h3 id=&quot;8&quot;&gt;8. 引用&lt;/h3&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://download.springer.com/static/pdf/925/chp%253A10.1007%252F978-0-85729-115-8_6.pdf?originUrl=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-0-85729-115-8_6&amp;amp;token2=exp=1452256751~acl=%2Fstatic%2Fpdf%2F925%2Fchp%25253A10.1007%25252F978-0-85729-115-8_6.pdf%3ForiginUrl%3Dhttp%253A%252F%252Flink.springer.com%252Fchapter%252F10.1007%252F978-0-85729-115-8_6*~hmac=7d6eefedcc9f47275d89e6b094bf3900beea7c9a52e1ee42d99ea3d4d4a03064&quot;&gt;Verhulst and the logistic equation (1838)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN129323640_0018&amp;amp;DMDID=dmdlog7&quot;&gt;Mathematical enquiries on the law of population growth&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://math.bu.edu/people/mak/MA565/Pearl_Reed_PNAS_1920.pdf&quot;&gt;Proceedings of the national academy of sciences&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://www.jstor.org/stable/pdf/2983890.pdf?acceptTC=true&quot;&gt;The regression analysis of binary sequences&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;http://papers.tinbergen.nl/02119.pdf&quot;&gt;The Origins of Logistic Regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;http://blog.csdn.net/han_xiaoyang/article/details/49332321&quot;&gt;机器学习系列(2)用初等数学视角解读逻辑回归&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;http://research.microsoft.com/en-us/um/people/minka/papers/logreg/minka-logreg.pdf&quot;&gt;A comparison of numerical optimizers for logistic regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&quot;http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/&quot;&gt;Numerical optimizers for Logistic Regression&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[9] &lt;a href=&quot;http://blog.csdn.net/itplus/article/details/21896453&quot;&gt;牛顿法与拟牛顿法学习笔记（一）牛顿法&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] &lt;a href=&quot;https://www.zhihu.com/question/20700829&quot;&gt;知乎:机器学习中使用「正则化来防止过拟合」到底是一个什么原理&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[11] &lt;a href=&quot;http://blog.csdn.net/abcjennifer/article/details/7700772&quot;&gt;多变量线性回归 Linear Regression with multiple variable&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[12] &lt;a href=&quot;http://cs229.stanford.edu/notes/cs229-notes1.pdf&quot;&gt;CS229 Lecture notes&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[13] &lt;a href=&quot;http://www.win-vector.com/dfiles/LogisticRegressionMaxEnt.pdf&quot;&gt;The equivalence of logistic regression and maximum entropy models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[14] &lt;a href=&quot;https://www.zhihu.com/question/26768865&quot;&gt;Linear SVM 和 LR 有什么异同？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[15] &lt;a href=&quot;https://www.zhihu.com/question/21704547&quot;&gt;SVM和logistic回归分别在什么情况下使用？&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[16] &lt;a href=&quot;https://www.quora.com/Support-Vector-Machines/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression&quot;&gt;Support Vector Machines: What is the difference between Linear SVMs and Logistic Regression?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[17] &lt;a href=&quot;http://www.cnblogs.com/jerrylead/archive/2011/03/13/1982639.html&quot;&gt;支持向量机svm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[18] &lt;a href=&quot;https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf&quot;&gt;GENERATIVE AND DISCRIMINATIVE CLASSIFIERS: NAIVE BAYES AND LOGISTIC REGRESSION&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[19] &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf&quot;&gt;A Tutorial on Energy-Based Models&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[20] &lt;a href=&quot;http://blog.sina.com.cn/s/blog_6cb8e53d0101oetv.html&quot;&gt;并行逻辑回归&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>2016-01-09 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2016/01/09/logisticregression.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2016/01/09/logisticregression.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【GPU编程系列之一】从深度学习选择什么样的gpu来谈谈gpu的硬件架构</title>
        <description>&lt;p&gt;从深度学习在2012年大放异彩，gpu计算也走入了人们的视线之中，它使得大规模计算神经网络成为可能。人们可以通过07年推出的CUDA(Compute Unified Device Architecture)用代码来控制gpu进行并行计算。本文首先根据显卡一些参数来推荐何种情况下选择何种gpu显卡，然后谈谈跟cuda编程比较相关的硬件架构。&lt;/p&gt;

&lt;h4 id=&quot;gpu&quot;&gt;1.选择怎样的GPU型号&lt;/h4&gt;

&lt;p&gt;这几年主要有AMD和NVIDIA在做显卡，到目前为止，NVIDIA公司推出过的GeForce系列卡就有几百张[1]，虽然不少都已经被淘汰了，但如何选择适合的卡来做算法也是一个值得思考的问题，Tim Dettmers[2]的文章给出了很多有用的建议，根据自己的理解和使用经历(其实只用过GTX 970…)我也给出一些建议。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/choose-gpu.png&quot; alt=&quot;1&quot; height=&quot;80%&quot; width=&quot;80%&quot; hspace=&quot;100&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 GPU选择&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;上面并没有考虑笔记本的显卡，做算法加速的话还是选台式机的比较好。性价比最高的我觉得是GTX 980ti，从参数或者一些用户测评来看，性能并没有输给TITAN X多少，但价格却便宜不少。从图1可以看出，价位差不多的显卡都会有自己擅长的地方，根据自己的需求选择即可。要处理的数据量比较小就选择频率高的，要处理的数据量大就选显存大core数比较多的，有double的精度要求就最好选择kepler架构的。tesla的M40是专门为深度学习制作的，如果只有深度学习的训练，这张卡虽然贵，企业或者机构购买还是比较合适的(百度的深度学习研究院就用的这一款[3])，相对于K40单精度浮点运算性能是4.29Tflops，M40可以达到7Tflops。QUADRO系列比较少被人提起，它的M6000价格比K80还贵，性能参数上也并没有好多少。&lt;/p&gt;

&lt;p&gt;在挑选的时候要注意的几个参数是处理器核心(core)、工作频率、显存位宽、单卡or双卡。有的人觉得位宽最重要，也有人觉得核心数量最重要，我觉得对深度学习计算而言处理器核心数和显存大小比较重要。这些参数越多越高是好，但是程序相应的也要写好，如果无法让所有的core都工作，资源就被浪费了。而且在购入显卡的时候，如果一台主机插多张显卡，要注意电源的选择。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;2.一些常见的名称含义&lt;/h4&gt;

&lt;p&gt;上面聊过了选择什么样的gpu，这一部分介绍一些常见名词。随着一代一代的显卡性能的更新，从硬件设计上或者命名方式上有很多的变化与更新，其中比较常见的有以下一些内容。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;gpu架构：Tesla、Fermi、Kepler、Maxwell、Pascal&lt;/li&gt;
  &lt;li&gt;芯片型号：GT200、GK210、GM104、GF104等&lt;/li&gt;
  &lt;li&gt;显卡系列：GeForce、Quadro、Tesla&lt;/li&gt;
  &lt;li&gt;GeForce显卡型号：G/GS、GT、GTS、GTX&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;gpu架构指的是硬件的设计方式，例如流处理器簇中有多少个core、是否有L1 or L2缓存、是否有双精度计算单元等等。每一代的架构是一种思想，如何去更好完成并行的思想，而芯片就是对上述思想的实现，芯片型号GT200中第二个字母代表是哪一代架构，有时会有100和200代的芯片，它们基本设计思路是跟这一代的架构一致，只是在细节上做了一些改变，例如GK210比GK110的寄存器就多一倍。有时候一张显卡里面可能有两张芯片，Tesla k80用了两块GK210芯片。这里第一代的gpu架构的命名也是Tesla，但现在基本已经没有这种设计的卡了，下文如果提到了会用Tesla架构和Tesla系列来进行区分。&lt;/p&gt;

&lt;p&gt;而显卡系列在本质上并没有什么区别，只是NVIDIA希望区分成三种选择，GeFore用于家庭娱乐，Quadro用于工作站，而Tesla系列用于服务器。Tesla的k型号卡为了高性能科学计算而设计，比较突出的优点是双精度浮点运算能力高并且支持ECC内存，但是双精度能力好在深度学习训练上并没有什么卵用，所以Tesla系列又推出了M型号来做专门的训练深度学习网络的显卡。需要注意的是Tesla系列没有显示输出接口，它专注于数据计算而不是图形显示。&lt;/p&gt;

&lt;p&gt;最后一个GeForce的显卡型号是不同的硬件定制，越往后性能越好，时钟频率越高显存越大，即G/GS&amp;lt;GT&amp;lt;GTS&amp;lt;GTX。&lt;/p&gt;

&lt;h4 id=&quot;gpu-1&quot;&gt;3.gpu的部分硬件&lt;/h4&gt;

&lt;p&gt;这一部分以下面的GM204硬件图做例子介绍一下GPU的几个主要硬件(图片可以点击查看大图，不想图片占太多篇幅)[4]。这块芯片它是随着GTX 980和970一起出现的。一般而言，gpu的架构的不同体现在流处理器簇的不同设计上(从Fermi架构开始加入了L1、L2缓存硬件)，其他的结构大体上相似。主要包括主机接口(host interface)、复制引擎(copy engine)、流处理器簇(Streaming Multiprocessors)、图形处理簇GPC(graphics processing clusters)、内存等等。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/gm204hardware.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;390&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 GM204芯片结构&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;主机接口，它连接了gpu卡和PCI Express，它主要的功能是读取程序指令并分配到对应的硬件单元，例如某块程序如果在进行内存复制，那么主机接口会将任务分配到复制引擎上。&lt;/p&gt;

&lt;p&gt;复制引擎(图中没有表示出来)，它完成gpu内存和cpu内存之间的复制传递。当gpu上有复制引擎时，复制的过程是可以与核函数的计算同步进行的。随着gpu卡的性能变得强劲，现在深度学习的瓶颈已经不在计算速度慢，而是数据的读入，如何合理的调用复制引擎是一个值得思考的问题。&lt;/p&gt;

&lt;p&gt;流处理器簇SM是gpu最核心的部分，这个翻译参考的是GPU编程指南，SM由一系列硬件组成，包括warp调度器、寄存器、Core、共享内存等。它的设计和个数决定了gpu的计算能力，一个SM有多个core，每个core上执行线程，core是实现具体计算的处理器，如果core多同时能够执行的线程就多，但是并不是说core越多计算速度一定更快，最重要的是让core全部处于工作状态，而不是空闲。不同的架构可能对它命名不同，kepler叫SMX，maxwell叫SMM，实际上都是SM。而GPC只是将几个sm组合起来，在做图形显示时有调度，一般在写gpu程序不需要考虑这个东西，只要掌握SM的结构合理的分配SM的工作即可。&lt;/p&gt;

&lt;p&gt;图中的内存控制器控制的是L2内存，每个大小为512KB。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;4.流处理器簇的结构&lt;/h4&gt;

&lt;p&gt;上面介绍的是gpu的整个硬件结构，这一部分专门针对流处理器簇SM来分析它内部的构造是怎样的。首先要明白的是，gpu的设计是为了执行大量简单任务，不像cpu需要处理的是复杂的任务，gpu面对的问题能够分解成很多可同时独立解决的部分，在代码层面就是很多个线程同时执行相同的代码，所以它相应的设计了大量的简单处理器，也就是stream process，在这些处理器上进行整形、浮点型的运算。下图给出了GK110的SM结构图。它属于kepler架构，与之前的架构比较大的不同是加入了双精度浮点运算单元，即图中的DP Unit。所以用kepler架构的显卡进行双精度计算是比较好的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/keplersmx.png&quot; alt=&quot;1&quot; height=&quot;30%&quot; width=&quot;30%&quot; hspace=&quot;390&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 GK110的SMX结构图&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;上面提到过的一个SM有多个core或者叫流处理器，它是gpu的运算单元，做整形、浮点型计算。可以认为在一个core上一次执行一个线程，GK110的一个SM有192个core，因此一次可以同时执行192个线程。core的内部结构可以查看[5]，实现算法一般不会深究到core的结构层面。SFU是特殊函数单元，用来计算log/exp/sin/cos等。DL/ST是指Load/Store，它在读写线程执行所需的全局内存、局部内存等。&lt;/p&gt;

&lt;p&gt;一个SM有192个core，8个SM有1536个core，这么多的线程并行执行需要有统一的管理，假如gpu每次在1536个core上执行相同的指令，而需要计算这一指令的线程不足1536个，那么就有core空闲，这对资源就是浪费，因此不能对所有的core做统一的调度，从而设计了warp(线程束)调度器。32个线程一组称为线程束，32个线程一组执行相同的指令，其中的每个thread称为lane。一个线程束接受同一个指令，里面的32个线程同时执行，不同的线程束可执行不同指令，那么就不会出现大量线程空闲的问题了。但是在线程束调度上还是存在一些问题，假如某段代码中有if…else…，在调度一整个线程束32个线程的时候不可能做到给thread0~15分配分支1的指令，给thread16~31分配分支2的指令(实际上gpu对分支的控制是，所有该执行分支1的线程执行完再轮到该执行分支2的线程执行)，它们获得的都是一样的指令，所以如果thread16~31是在分支2中它们就需要等待thread0~15一起完成分支1中的计算之后，再获得分支2的指令，而这个过程中，thread0～15又在等待thread16~31的工作完成，从而导致了线程空闲资源浪费。因此在真正的调度中，是半个warp执行相同指令，即16个线程执行相同指令，那么给thread0~15分配分支1的指令，给thread16~31分配分支2的指令，那么一个warp就能够同时执行两个分支。这就是图中Warp Scheduler下为什么会出现两个dispatch的原因。&lt;/p&gt;

&lt;p&gt;另外一个比较重要的结构是共享内存shared memory。它存储的内容在一个block(暂时认为是比线程束32还要大的一些线程个数集合)中共享，一个block中的线程都可以访问这块内存，它的读写速度比全局内存要快，所以线程之间需要通信或者重复访问的数据往往都会放在这个地方。在kepler架构中，一共有64kb的空间大小，供共享内存和L1缓存分配，共享内存实际上也可看成是L1缓存，只是它能够被用户控制。假如共享内存占48kb那么L1缓存就占16kb等。在maxwell架构中共享内存和L1缓存分开了，共享内存大小是96kb。而寄存器的读写速度又比共享内存要快，数量也非常多，像GK110有65536个。&lt;/p&gt;

&lt;p&gt;此外，每一个SM都设置了独立访问全局内存、常量内存的总线。常量内存并不是一块内存硬件，而是全局内存的一种虚拟形式，它跟全局内存不同的是能够高速缓存和在线程束中广播数据，因此在SM中有一块常量内存的缓存，用于缓存常量内存。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;5.小结&lt;/h4&gt;

&lt;p&gt;本文谈了谈gpu的一些重要的硬件组成，就深度学习而言，我觉得对内存的需求还是比较大的，core多也并不是能够全部用上，但现在开源的库实在完整，想做卷积运算有cudnn，想做卷积神经网络caffe、torch，想做rnn有mxnet、tensorflow等等，这些库内部对gpu的调用做的非常好并不需用户操心，但了解gpu的一些内部结构也是很有意思的。&lt;/p&gt;

&lt;p&gt;另，一开始接触GPU并不知道是做图形渲染的…所以有些地方可能理解有误，主要基于计算来讨论GPU的构造。&lt;/p&gt;

&lt;p&gt;参考：&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_Nvidia_graphics_processing_units&quot;&gt;List of Nvidia graphics processing units&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://timdettmers.com/2014/08/14/which-gpu-for-deep-learning/&quot;&gt;Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://www.nextplatform.com/2015/12/11/inside-the-gpu-clusters-that-power-baidus-neural-networks/&quot;&gt;Inside the GPU Clusters that Power Baidu’s Neural Networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://international.download.nvidia.com/geforce-com/international/pdfs/GeForce_GTX_980_Whitepaper_FINAL.PDF&quot;&gt;Whitepaper NVIDIA GeForce GTX 980&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://developer.nvidia.com/content/life-triangle-nvidias-logical-pipeline&quot;&gt;Life of a triangle - NVIDIA’s logical pipeline&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>2015-12-20 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2015/12/20/introductionofgpuhardware.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2015/12/20/introductionofgpuhardware.html</guid>
        
        <category>programming languages</category>
        
      </item>
    
      <item>
        <title>【机器学习算法系列之一】EM算法实例分析</title>
        <description>&lt;p&gt;最近两天研究了一下EM算法，主要是基于《统计学习方法》和论文《What is the expectation maximization algorithm?》[1]，但是对两个文章里面给的实例求解过程都比较的困惑，搜索网上的一些博客也没有找到对应的求解过程，自己就仔细研究了一下，中间也遇到了一些坑，现在把解题思路给出来。因为书上和网上的博客[2]对EM算法的推导和证明解释的非常清楚，本文就不做解释了，如果对EM算法原理不清楚的建议先看看《统计学习方法》第9章或者博客[2][3]。本文只给出两个文章中的例子的求解过程。&lt;/p&gt;

&lt;p&gt;(题目我会列出来，如果不是看这个两个文章而了解EM算法的也不要紧，题目是通用的)&lt;/p&gt;

&lt;p&gt;本文中观测数据记为Y(因为两个例子都是输出是观测数据)，隐藏变量(未观测变量)记为z，模型参数记为$\theta$。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;1.三硬币模型&lt;/h4&gt;

&lt;p&gt;假设有三枚硬币A、B、C，每个硬币正面出现的概率是$\pi、p、q$。进行如下的掷硬币实验：先掷硬币A，正面向上选B，反面选C；然后掷选择的硬币，正面记1，反面记0。独立的进行10次实验，结果如下：1，1，0，1，0，0，1，0，1，1。假设只能观察最终的结果(0 or 1)，而不能观测掷硬币的过程(不知道选的是B or C)，问如何估计三硬币的正面出现的概率？&lt;/p&gt;

&lt;p&gt;首先针对某个输出y值，它在参数$\theta (\theta=(\pi, p, q))$下的概率分布为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(y|\theta )=\sum_{z}P(y,z|\theta)=\sum_{z}P(z|\theta)P(y|z, \theta) = \pi p^y (1-p)^{1-y} + (1-\pi) q^y (1-q)^{1-y}
&lt;/script&gt;

&lt;p&gt;从而针对观测数据$Y=(y_1, y_2, \cdot\cdot\cdot, y_n)^T$的似然函数为&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(Y|\theta ) =\sum_{z}P(Y,z|\theta)=\sum_{z}P(z|\theta)P(Y|z, \theta) = \prod _{j=1} ^{n} \pi p^y_j (1-p)^{1-y_j} + (1-\pi) q^y_j (1-q)^{1-y_j}
&lt;/script&gt;

&lt;p&gt;因此本题的目标是求解参数$\theta$的极大似然估计，即$\hat{\theta} = \underset{\theta }{argmax}logP(Y|\theta)$。直接对连乘的似然函数求导太复杂，所以一般用极大似然估计都会转化成对数似然函数，但是就算转化成了求和，如果这个式子对某个参数(例如$\pi$)求导，由于这个式子中有“和的对数”，求导非常复杂。因此这个问题需要用EM算法来求解。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E步&lt;/strong&gt;:根据EM算法，在这一步需要计算的是未观测数据的条件概率分布，也就是每一个$P(z|y_j, \theta)$，$\mu^{i+1}$表示在已知的模型参数$\theta^i$下观测数据$y_j$来自掷硬币B的概率，相应的来自掷C的概率就是$1-\mu^{i+1}$。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu ^{i+1} = \frac {\pi^i ({p^i})^{y_j}(1-p^i)^{1-y_j}} {\pi^i ({p^i})^{y_j}(1-p^i)^{1-y_j} + (1-\pi^i) ({q^i})^{y_j} (1-q^i)^{1-y_j}}&lt;/script&gt;

&lt;p&gt;这里的分子就是z取掷硬币B和y的联合概率分布，需要注意的是，这里的$\mu^{i+1}$通过E步的计算就已经是一个常数了，后面的求导不需要把这个式子代入。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;M步&lt;/strong&gt;:针对Q函数求导，Q函数的表达式是&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$Q(\theta, \theta^i) = \sum&lt;em&gt;{j=1}^N \sum&lt;/em&gt;{z} P(z&lt;/td&gt;
      &lt;td&gt;y_j, \theta^i)logP(y_j, z&lt;/td&gt;
      &lt;td&gt;\theta)=\sum_{j=1}^N \mu_jlog(\pi p^{y_j}(1-p)^{1-y_j}) + (1-\mu_j)log((1-\pi) q^{y_j} (1-q)^{1-y_j})] $$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;最开始求导犯了一个大错，没有将表达式展开来求，这样就直接默认$\mu_j$是一个系数，求导将它给约去了，这样就得不到最后的结果。&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q}{\partial \pi} = (\frac{\mu_1}{\pi} - \frac{1-\mu_1}{1-\pi})+\cdot \cdot \cdot + (\frac{\mu_N}{\pi} - \frac{1-\mu_N}{1-\pi}) = \frac{\mu_1-\pi}{\pi(1-\pi)} + \cdot \cdot \cdot + \frac{\mu_N-\pi}{\pi(1-\pi)} = \frac{\sum _{j=1} ^N\mu_j-N\pi}{\pi(1-\pi)}&lt;/script&gt;

&lt;p&gt;再令这个结果等于0，即获得$\pi^{i+1} = \frac{1}{N}\sum_{j=1}^{N}\mu_j^{i+1}$，其他两个也同理。&lt;/p&gt;

&lt;h4 id=&quot;section-1&quot;&gt;2.两硬币模型&lt;/h4&gt;

&lt;p&gt;假设有两枚硬币A、B，以相同的概率随机选择一个硬币，进行如下的掷硬币实验：共做5次实验，每次实验独立的掷十次，结果如图中a所示，例如某次实验产生了H、T、T、T、H、H、T、H、T、H，H代表证明朝上。a是在知道每次选择的是A还是B的情况下进行，b是在不知道选择的硬币情况下进行，问如何估计两个硬币正面出现的概率？&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/em1.png&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;针对a情况，已知选择的A or B，重点是如何计算输出的概率分布，论文中直接统计了5次实验中A正面向上的次数再除以总次数作为A的$\hat{\theta_A}$，这其实也是极大似然求导求出来的。 &lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$\underset{\theta }{argmax}logP(Y&lt;/td&gt;
      &lt;td&gt;\theta) = log((\theta_B^5(1-\theta_B)^5) (\theta_A^9(1-\theta_A))(\theta_A^8(1-\theta_A)^2) (\theta_B^4(1-\theta_B)^6) (\theta_A^7(1-\theta_A)^3) ) = log(   (\theta_A^{24}(1-\theta_A)^6) (\theta_B^9(1-\theta_B)^{11})  )$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;上面这个式子求导之后就能得出$\hat{\theta_A} = \frac{24}{24 + 6} = 0.80$以及$\hat{\theta_B} = \frac{9}{9 + 11} = 0.45$。&lt;/p&gt;

&lt;p&gt;针对b情况，由于并不知道选择的是A还是B，因此采用EM算法。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;E步&lt;/strong&gt;:计算在给定的$\hat{\theta_A^{(0)}}$和$\hat{\theta_B^{(0)}}$下，选择的硬币可能是A or B的概率，例如第一个实验中选择A的概率为(由于选择A、B的过程是等概率的，这个系数被我省略掉了)&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$P(z=A&lt;/td&gt;
      &lt;td&gt;y_1, \theta) = \frac {P(z=A, y_1&lt;/td&gt;
      &lt;td&gt;\theta)}{P(z=A,y_1&lt;/td&gt;
      &lt;td&gt;\theta) + P(z=B,y_1&lt;/td&gt;
      &lt;td&gt;\theta)} = \frac{(0.6)^5&lt;em&gt;(0.4)^5}{(0.6)^5&lt;/em&gt;(0.4)^5+(0.5)^{10}} = 0.45$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;M步&lt;/strong&gt;:针对Q函数求导，在本题中Q函数形式如下，参数设置参照例1，只是这里的$y_j$代表的是每次正面朝上的个数。&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$$Q(\theta, \theta^i) = \sum&lt;em&gt;{j=1}^N \sum&lt;/em&gt;{z} P(z&lt;/td&gt;
      &lt;td&gt;y_j, \theta^i)logP(y_j, z&lt;/td&gt;
      &lt;td&gt;\theta)=\sum_{j=1}^N \mu_jlog(\theta_A^{y_j}(1-\theta_A)^{10-y_j}) + (1-\mu_j)log(\theta_B^{y_j}(1-\theta_B)^{10-y_j})]$$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;从而针对这个式子来对参数求导，例如对$\theta_A$求导&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial Q}{\partial \theta_A} = \mu_1(\frac{y_1}{\theta_A}-\frac{10-y_1}{1-\theta_A}) + \cdot \cdot \cdot  + \mu_5(\frac{y_5}{\theta_A}-\frac{10-y_5}{1-\theta_A}) 
= \mu_1(\frac{y_1 - 10\theta_A} {\theta_A(1-\theta_A)}) + \cdot \cdot \cdot +\mu_5(\frac{y_5 - 10\theta_A} {\theta_A(1-\theta_A)})  = \frac{\sum_{j=1}^5 \mu_jy_j - \sum_{j=1}^510\mu_j\theta_A} {\theta_A(1-\theta_A)}&lt;/script&gt;

&lt;p&gt;求导等于0之后就可得到图中的第一次迭代之后的参数值$\hat{\theta_A^{(1)}} = 0.71$和$\hat{\theta_B^{(1)}} = 0.58$。&lt;/p&gt;

&lt;p&gt;这个例子可以非常直观的看出来，EM算法在求解M步是将每次实验硬币取A或B的情况都考虑进去了。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;3.小结&lt;/h4&gt;

&lt;p&gt;EM算法将不完全数据补全成完全数据，而E步并不是只取最可能补全的未观测数据，而是将未观测的数据的所有补全可能都计算出对应的概率值，从而对这些所有可能的补全计算出它们的期望值，作为下一步的未观测数据。至于为什么取期望，一是因为这个未观测数据本身就是基于一组不完全正确的参数估计出来的，例如三硬币例子假如每次在进行maximization之前都只取某一个值(极端一点，每次结果都是认为B是最可能的观测数据，而不算C)，那么在更新参数时，也只有B的参数在更新。二是这种情况下JENSEN不等式不成立，那么对$\theta$的似然函数变换形式就不成立，收敛也不成立。&lt;/p&gt;

&lt;p&gt;这两个例子想明白之后求解实际上非常简单，所以很多博主并没把它们列出来，但如果一开始思考的方向不对就会浪费很多时间，当我把上面的过程想清楚之后再去求解别的例子，发现很轻松就能解出来。当然EM算法的核心还是证明和推导，这点别的文章讲的非常清晰了我就不赘述了。这也是数学上常用的思路，当无法直接对某个含参式子求极大值时，考虑对它的下界求极大值，当确定下界取极大值的参数时也能让含参式子值变大，也就是&lt;strong&gt;&lt;em&gt;不断求解下界的极大值逼近求解对数似然函数极大化(李航.《统计学习方法》)&lt;/em&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;如果本文有错误，请一定要指出来，感谢～&lt;/p&gt;

&lt;h4 id=&quot;section-3&quot;&gt;4.参考：&lt;/h4&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf&quot;&gt;What is the expectation maximization
algorithm?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html&quot;&gt;（EM算法）The EM Algorithm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://blog.csdn.net/zouxy09/article/details/8537620&quot;&gt;从最大似然到EM算法浅解&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>2015-12-02 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2015/12/02/emexample.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2015/12/02/emexample.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>Caffe、TensorFlow、MXnet三个开源库对比</title>
        <description>&lt;p&gt;最近Google开源了他们内部使用的深度学习框架TensorFlow[1]，结合之前开源的MXNet[2]和Caffe[3]，对三个开源库做了一些讨论，其中只有Caffe比较仔细的看过源代码，其他的两个库仅阅读官方文档和一些研究者的评论博客有感，本文首先对三个库有个整体的比较，再针对一些三者设计的不同数据结构、计算方式、gpu的选择方式等方面做了比较详细的讨论。表格1是三者的一些基本情况的记录和比较。其中示例指的是官方给出的example是否易读易理解，因为TensorFlow直接安装python包，所以一开始没有去下源代码，从文档中找example不如另外两个下源码直接。实际上TensorFlow更加像一套独立的python接口，它不止能够完成CNN/RNN的功能，还见到过有人用它做Kmeans聚类。这个表主观因素比较明显，仅供参考。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;库名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;开发语言&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;支持接口&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;安装难度(ubuntu)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;文档风格&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;示例&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;支持模型&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;上手难易&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Caffe&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++/cuda&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++/python/matlab&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;***&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;*&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;***&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CNN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;**&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MXNet&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++/cuda&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;python/R/Julia&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;**&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;***&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;**&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CNN/RNN&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;*&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TensorFlow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++/cuda/python&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;c++/python&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;*&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;**&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;*&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;CNN/RNN/…&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;***&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;font size=&quot;1.5&quot;&gt;安装难度: *(简单) --&amp;gt; ***(复杂)&lt;/font&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;font size=&quot;1.5&quot;&gt;文档风格: *(一般) --&amp;gt; ***(好看、全面)&lt;/font&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;font size=&quot;1.5&quot;&gt;示例: *(给的少) --&amp;gt; ***(给的多、全)&lt;/font&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;font size=&quot;1.5&quot;&gt;上手难易: *(易) --&amp;gt; ***(难)&lt;/font&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section&quot;&gt;1.基本数据结构&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;库名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;数据结构名称&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;设计方式&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Caffe&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Blob&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;存储的数据可以看成N维的c数组，有(n,k,h,w)四个维数，一个blob里面有两块数据空间保存前向和后向求导数据&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;MXNet&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;NDArray&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;提供cpu/gpu的矩阵和矢量计算，能够自动并行&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TensorFlow&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;tensor&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;相当于N维的array或者list，维数可变，数据类型一旦定义不能改变&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;caffe的数据存储类blob，当把数据可以看成是一个N维的c数组，它们的存储空间连续。例如存储图片是4维(num, channel, height, width),变量(n,k,h,w)在数组中存储位置为((n*K+k)*H+h)*W+w。blob有以下三个特征[4]:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;两块数据，一个是原始data，一个是求导值diff&lt;/li&gt;
  &lt;li&gt;两种内存分配方式，一种是分配在cpu上，一种是分配在gpu上，通过前缀cpu、gpu来区分&lt;/li&gt;
  &lt;li&gt;两种访问方式，一种是不能改变数据，一种能改变数据&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Caffe最让我觉得精妙的地方在于一个blob保存前向和后向的数据。虽然就代码本身而言，前向数据是因为输入数据不同而改变，后向求导是因为求导不同而改变，根据SRP原则，在一个类里面因为两个原因而改变了数据这种是不合适的设计。但是从逻辑层面，前向数据的改变引起了反向求导的不同，它们实际上是一起在改变，本身应该是一个整体。所以我很喜欢这个设计，虽然基本上其他框架中都是将两个数据给分离出来，caffe2也不知是否保留。&lt;/p&gt;

&lt;p&gt;MXNet的NDArray类似numpy.ndarray，也支持把数据分配在gpu或者cpu上进行运算。但是与numpy和caffe不同的是，当在操作NDArray，它能自动的将需要执行的数据分配到多台gpu和cpu上进行计算，从而完成高速并行。在调用者的眼中代码可能只是一个单线程的，数据只是分配到了一块内存中，但是背后执行的过程实际上是并行的。将指令(加减等)放入中间引擎，然后引擎来评估哪些数据有依赖关系，哪些能并行处理。定义好数据之后将它绑定到网络中就能处理它了。&lt;/p&gt;

&lt;p&gt;TensorFlow的tensor，它相当于N维的array或者list，与MXNet类似，都是采用了以python调用的形式展现出来。某个定义好的tensor的数据类型是不变的，但是维数可以动态改变。用tensor rank和TensorShape来表示它的维数（例如rank为2可以看成矩阵，rank为1可以看成向量）。tensor是个比较中规中矩的类型。唯一特别的地方在于在TensorFlow构成的网络中，tensor是唯一能够传递的类型，而类似于array、list这种不能当成输入。&lt;/p&gt;

&lt;p&gt;值得一提的是cuda-convnet采用的数据结构是NVMatrix，NV表示数据分配在gpu上，即将所有变量都当成矩阵来处理，它只有两维，它算是最早用cuda实现的深度学习框架，而上面三种框架都采用了多维可变维的思想，这种可变维在用矩阵做卷积运算的时候是很有效的。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2.网络实现方式&lt;/h3&gt;

&lt;p&gt;Caffe是典型的功能（过程）计算方式，它首先按照每一个大功能（可视化、损失函数、非线性激励、数据层）将功能分类并针对部分功能实现相应的父类，再将具体的功能实现成子类，或者直接继承Layer类，从而形成了XXXLayer的形式。然后将不同的layer组合起来就成了net。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/caffe5.png&quot; alt=&quot;1&quot; height=&quot;25%&quot; width=&quot;25%&quot; hspace=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图1 caffe的网络结构&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;MXNet是符号计算和过程计算混合[5]，它设计了Symbol大类，提供了很多符号运算的接口，每个symbol定义了对数据进行怎样的处理，symbol只是定义处理的方式，这步还并未真正的执行运算。其中一个需要注意的是symbol里面有Variable，它作为承载数据的符号，定义了需要传递什么样的数据给某个Variable，并在后续的操作中将数据绑定到Variable上。下面的代码是一个使用示例，它实现了将激励函数连接到前面定义好的net后面，并给出了这一个symbol的名字和激励函数类型，从而构造出net。下图左边部分是定义symbol的合集，中间将数据绑定到Variable上之后变成了右边真正的执行流程图。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;net = mx.symbol.Activation(data=net, name='relu1', act_type=&quot;relu&quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/compare2.png&quot; alt=&quot;1&quot; height=&quot;50%&quot; width=&quot;50%&quot; hspace=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图2 MXNet的网络结构&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;TensorFlow选择的是符号计算方式，它的程序分为计算构造阶段和执行阶段，构造阶段是构造出computation graph，computation graph就是包含一系列符号操作Operation和Tensor数据对象的流程图，跟mxnet的symbol类似，它定义好了如何进行计算（加减乘除等）、数据通过不同计算的顺序（也就是flow，数据在符号操作之间流动的感觉）。但是暂时并不读取输入来计算获得输出，而是由后面的执行阶段启动session的run来执行已经定义好的graph。这样的方式跟mxnet很相似，应该都是借鉴了theano的想法。其中TensorFlow还引入了Variable类型，它不像mxnet的Variable属于symbol（tf的operation类似mxnet的symbol），而是一个单独的类型，主要作用是存储网络权重参数，从而能够在运行过程中动态改变。tf将每一个操作抽象成了一个符号Operation，它能够读取0个或者多个Tensor对象作为输入(输出)，操作内容包括基本的数学运算、支持reduce、segment（对tensor中部分进行运算。例如tensor长度为10，可以同时计算前5个，中间2个，后面三个的和）、对image的resize、pad、crop、filpping、transposing等。tf没有像mxnet那样给出很好的图形解释或者实例(可能因为我没找到。。)，按照自己的理解画了一部分流程图。有点疑惑的是，为什么要设计Variable，tf给出的一个alexnet的example源码中，输入数据和权重都设置成了Variable，每一层的输出并未直接定义，按照tf的说法，只有tensor类型能够在网络中传递，输出的类型应该是tensor，但是由于输入和权重改变了，输出应该也在随着改变，既然如此，为何不只设计一个tensor，让tensor也能动态改变。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/compare3.png&quot; alt=&quot;1&quot; height=&quot;20%&quot; width=&quot;20%&quot; hspace=&quot;450&quot; /&gt;&lt;/p&gt;

&lt;font size=&quot;2&quot;&gt;&lt;center&gt;图3 TensorFlow的computation graph&lt;/center&gt;&lt;/font&gt;

&lt;p&gt;就设计而言，TensorFlow相对于其他两个更像是一种通用的机器学习框架，而不是只针对cnn或rnn，但就现在的性能而言，tf的速度比很多开源框架都要差一点[6]。&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;3.分布式训练&lt;/h3&gt;

&lt;p&gt;Caffe和TensorFlow没有给出分布式的版本，MXNet提供了多机分布式，因而前两者只有如何控制使用多gpu。Caffe通过直接在执行指令后面加上&lt;strong&gt;&lt;em&gt;-gpu 0,1&lt;/em&gt;&lt;/strong&gt;来表示调用两个gpu0和1，只实现了数据并行，也就是在不同的gpu上执行相同网络和不同数据，caffe会实例化多个solver和net让每次处理的batch_size加倍。TensorFlow则能够自己定义某个操作执行在哪个gpu上，通过调用&lt;strong&gt;&lt;em&gt;with tf.device(‘/gpu:2’)&lt;/em&gt;&lt;/strong&gt;表示接下来的操作要在gpu2上处理，它也是数据并行。MXNet通过执行脚本时指定多机节点个数来确定在几台主机上运行，也是数据并行。MXNet的多gpu分配和它们之间数据同步是通过MXNet的数据同步控制KVStore来完成的。&lt;/p&gt;

&lt;p&gt;KVStore的使用首先要创建一个kv空间，这个空间用来在不同gpu不同主机间分享数据，最基本的操作是push和pull，push是把数据放入这个空间，pull是从这个空间取数据。这个空间内保存的是key-value([int, NDArray])，在push/pull的时候来指定到哪个key。下面的代码将不同的设备上分配的b[i]通过key3在kv空间累加再输出到a，从而完成了对多gpu的处理。这个是个非常棒的设计，提供了很大的自由度，并且为开发者减少了控制底层数据传输的麻烦。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gpus = [mx.gpu(i) for i in range(4)]	
b = [mx.nd.ones(shape, gpu) for gpu in gpus]
kv.push(3, b)
kv.pull(3, out = a)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之前有看过一篇论文，如何将卷积网络放在多gpu上训练，论文中有两种方法，一种是常用的数据并行，另一种是模型并行。模型并行指的是将一个完整的网络切分成不同块放在不同gpu上执行，每个gpu可能只处理某一张图的四分之一。采用模型并行很大程度上是因为显存不够放不下整个网络的数据，而现在gpu的功能性能提高，一个gpu已经能够很好的解决显存不够的问题，再加上模型并行会有额外的通信开销，因此开源框架采用了数据并行，用来提高并行度。&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;4.小结&lt;/h3&gt;

&lt;p&gt;上面针对三个框架的不同方面进行了一些分析与比较，可以看出TensorFlow和MXNet有一些相似的地方，都是想做成更加通用的深度学习框架，貌似caffe2也会采用符号计算[5]，说明以后的框架会更加的偏向通用性和高效，个人最喜欢的是caffe，也仿造它和cuda-convnet的结构写过卷积网络，如果是想提高编程能力可以多看看这两个框架的源码。而MXNet给人的感觉是非常用心，更加注重高效，文档也非常的详细，不仅上手很容易，运用也非常的灵活。TensorFlow则是功能很齐全，能够搭建的网络更丰富而不是像caffe仅仅局限在CNN。总之框架都是各有千秋，如何选择也仅凭个人的喜好，然而google这个大杀器一出现引起的关注度还是最大的，虽然现在单机性能还不够好，但是看着长长的开发人员名单，也只能说大牛多就是任性。&lt;/p&gt;

&lt;p&gt;参考:&lt;/p&gt;

&lt;p&gt;[1] &lt;a href=&quot;http://tensorflow.org/&quot;&gt;http://tensorflow.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;http://mxnet.readthedocs.org/en/latest/index.html&quot;&gt;http://mxnet.readthedocs.org/en/latest/index.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;http://caffe.berkeleyvision.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;http://chenrudan.github.io/blog/2015/05/07/cafferead.html&quot;&gt;[caffe]的项目架构和源码解析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;http://weibo.com/p/1001603907610737775666&quot;&gt;如何评价Tensorflow和其它深度学习系统&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://github.com/soumith/convnet-benchmarks&quot;&gt;Imagenet Winners Benchmarking&lt;/a&gt;&lt;/p&gt;

</description>
        <pubDate>2015-11-18 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2015/11/18/comparethreeopenlib.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2015/11/18/comparethreeopenlib.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>---Ubuntu 14.04下配置caffe---</title>
        <description>&lt;p&gt;&lt;strong&gt;1.从github上下载源码&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/BVLC/caffe.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2.安装BLAS库&lt;/strong&gt;&lt;/p&gt;

&lt;font size=&quot;3&quot;&gt;	选择安装mkl，在官网上下载学生版，解压到存放目录。先对解压后的文件授权&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;chmod a+x parallel_studio_xe_2015 -R
&lt;/code&gt;&lt;/pre&gt;

&lt;font size=&quot;3&quot;&gt;然后用root权限执行&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;sudo ./install.sh（一般都选择默认的选项）
sudo vim /etc/ld.so.conf.d/intel_mkl.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;font size=&quot;3&quot;&gt;配置环境，加入下面内容&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;/opt/intel/lib/intel64
/opt/intel/mkl/lib/intel64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3.安装python依赖包&lt;/strong&gt;&lt;/p&gt;

&lt;font size=&quot;3&quot;&gt;先安装python-pip&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install python-pip
&lt;/code&gt;&lt;/pre&gt;

&lt;font size=&quot;3&quot;&gt;然后进入caffe下的python文件夹，执行&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;for req in $(cat requirements.txt); do pip install $req; done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4.安装cmake&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository ppa:george-edison55/cmake-3.x
sudo apt-get update
sudo apt-get install cmake
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;5.安装glog&lt;/strong&gt;&lt;/p&gt;

&lt;font size=&quot;3&quot;&gt;https://github.com/google/glog.git(这个地址的安装包会报错，下载&lt;/font&gt;
&lt;font size=&quot;3&quot; color=&quot;#FA8072&quot;&gt;0.3.3&lt;/font&gt;
&lt;font size=&quot;3&quot;&gt;的)&lt;/font&gt;

&lt;font size=&quot;3&quot;&gt;进入文件夹，执行&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;sudo ./configure
sudo make 
sudo make install
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;6.可以用apt-get安装的&lt;/strong&gt;&lt;/p&gt;

&lt;font size=&quot;3&quot;&gt;sudo apt-get install libboost-all-dev libprotobuf-dev libsnappy-dev libleveldb-dev libhdf5-serial-dev libgflags-dev libgoogle-glog-dev liblmdb-dev protobuf-compiler libopencv-dev&lt;/font&gt;

&lt;p&gt;&lt;strong&gt;7.安装cudnn&lt;/strong&gt;&lt;/p&gt;

&lt;font size=&quot;3&quot;&gt;从官网上下载，然后解压&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;sudo cp cudnn.h /usr/local/include
sudo cp libcudnn.* /usr/local/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;font size=&quot;3&quot;&gt;复制过去之后，软连接就不见了，要自己再链接一次&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;sudo ln -sf libcudnn.so.7.0.64 libcudnn.so.7.0
sudo ln -sf libcudnn.so.7.0 libcudnn.so
sudo ldconfig 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;8.安装caffe&lt;/strong&gt;&lt;/p&gt;

&lt;font size=&quot;3&quot;&gt;执行cp Makefile.config.example Makefile.config，修改部分内容&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;BLAS := mkl
USE_CUDNN := 1前面注释去掉
DEBUG := 1 //便于后面调试
&lt;/code&gt;&lt;/pre&gt;

&lt;font size=&quot;3&quot;&gt;编译&lt;/font&gt;

&lt;pre&gt;&lt;code&gt;make all -j8
make test -j8
make runtest -j8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;错误：&lt;/p&gt;

&lt;p&gt;1./bin/bash: aclocal-1.14: command not found&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install autotools-dev
sudo apt-get install automake
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2.src/demangle.h:80:27: error: expected initializer before ‘Demangle’.换成版本0.3.3就好了&lt;/p&gt;

&lt;p&gt;3./sbin/ldconfig.real: /usr/local/lib/libcudnn.so.7.0 is not a symbolic link.重新建立软链接&lt;/p&gt;
</description>
        <pubDate>2015-10-26 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2015/10/26/installcaffe.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2015/10/26/installcaffe.html</guid>
        
        <category>configure</category>
        
      </item>
    
      <item>
        <title>[DeepLearning]深度学习之五常见tricks</title>
        <description>&lt;p&gt;本文主要给出了在实现网络或者调节代码过程使用的以及平时看一些文章记录下来的一些小技巧，主要针对卷积网络和图像处理。就个人感受，有些技巧还是非常有效的，而且通常可以通过看开源库的一些文档或者源代码来发掘这些内容，最后能够称为自己所用。&lt;/p&gt;

&lt;h4 id=&quot;validation-set&quot;&gt;1.构造validation set&lt;/h4&gt;

&lt;p&gt;一般数据集可能不会给出验证集，所以自己会从给的训练集中按照一定比例（9：1）分离出验证集。&lt;/p&gt;

&lt;h4 id=&quot;section&quot;&gt;2.增加训练数据&lt;/h4&gt;

&lt;p&gt;为了更好的训练网络，有时候需要增加原始数据集，一般有以下方法[1]：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;沿着x轴将图片左右翻转&lt;/li&gt;
  &lt;li&gt;随机的剪切、缩放、旋转&lt;/li&gt;
  &lt;li&gt;用pca来改变RGB的强度值，产生分别对应的特征值和特征向量，然后用均值为0方差为0.1的随机数与特征值和特征向量相乘得到新的数据[2]&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-1&quot;&gt;3.预处理&lt;/h4&gt;

&lt;p&gt;常见的是减均值、除方差，还有变化到-1～1，主要针对不同尺度的特征，例如房价预测的例子中，每个房子的房屋大小和卧室数量就不在一个数量级上，这种情况就需要对每一维特征进行尺度变换normalize，还有的方法是使用pca白化。但是就图像处理领域，通常就减去一个均值就可以直接拿来计算。&lt;/p&gt;

&lt;h4 id=&quot;section-2&quot;&gt;4.权重初始化&lt;/h4&gt;

&lt;p&gt;不要全部初始化为0，这样会导致大部分的deltaw都一样，一般用高斯分布或者uniform分布。但是这样的分布会导致输出的方差随着输入单元个数而变大，因此需要除以fan in（输入个数的平方根）。&lt;/p&gt;

&lt;h4 id=&quot;tricks&quot;&gt;5.卷积tricks&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;图片输入是2的幂次方，例如32、64、96、224等。&lt;/li&gt;
  &lt;li&gt;卷积核大小是3*3或者5*5。&lt;/li&gt;
  &lt;li&gt;输入图片上下左右需要用0补充，即padding，且假如卷积核大小是5那么padding就是2（图片左右上下都补充2），卷积核大小是3padding大小就是1。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;poolingtricks&quot;&gt;5.pooling层tricks&lt;/h4&gt;

&lt;p&gt;poolin层也能防止过拟合，使用overlapped pooling，即用来池化的数据有重叠，但是pooling的大小不要超过3。&lt;/p&gt;

&lt;p&gt;max pooling比avg pooling效果会好一些。&lt;/p&gt;

&lt;h4 id=&quot;overfitting&quot;&gt;6.避免overfitting&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;drop out能够避免过拟[1]，一般在加在全连接层后面[3]，但是会导致收敛速度变慢。&lt;/li&gt;
  &lt;li&gt;正则化也能避免过拟合，L2正则l2正则惩罚了峰值权重，l1正则会导致稀疏权重，趋近于0，l1会趋向选择有用的输入。又或者可以给给权重矢量的模加上上边界（3 or 4），更新时对delta w进行归一化。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-3&quot;&gt;7.调参&lt;/h4&gt;

&lt;p&gt;使用pretrain好的网络参数作为初始值。然后fine-tuning，此处可以保持前面层数的参数不变，只调节后面的参数。但是finetuning要考虑的是图片大小和跟原数据集的相关程度，如果相关性很高，那么只用取最后一层的输出，不相关数据多就要finetuning比较多的层。&lt;/p&gt;

&lt;p&gt;初始值设置为0.1，然后训练到一定的阶段除以2，除以5，依次减小。&lt;/p&gt;

&lt;p&gt;加入momentum项[2]，可以让网络更快的收敛。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;节点数增加，learning rate要降低&lt;/li&gt;
  &lt;li&gt;层数增加，后面的层数learning rate要降低&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-4&quot;&gt;9.激励函数&lt;/h4&gt;

&lt;p&gt;Sigmoid作为激励函数有饱和和梯度消失的现象，在接近输出值0和1的地方梯度接近于0（可通过sigmoid的分布曲线观察出）。因而可采用Rectified Linear Units(ReLUs)作为激励函数，这样会训练的快一些，但是relu比较脆弱，如果某次某个点梯度下降的非常多，权重被改变的特别多，那么这个点的激励可能永远都是0了，还有带参的prelu、产生随机值的rrelu等改进版本。但是leaky版本（0换成0.01）的效果并不是很稳定。&lt;/p&gt;

&lt;h4 id=&quot;section-5&quot;&gt;10.通过做图来观察网络训练的情况&lt;/h4&gt;

&lt;p&gt;可以画出随着不同参数训练集测试集的改变情况，观察它们的走势图来分析到底什么时候的参数比较合适。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;不同学习率与loss的曲线图，横坐标是epoch，纵坐标是loss或者正确率&lt;/li&gt;
  &lt;li&gt;不同的batchsize与loss的曲线图，坐标同上&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;section-6&quot;&gt;11.数据集不均衡&lt;/h4&gt;

&lt;p&gt;这种情况如果数据集跟imagenet的比较相近，可以直接用finetuning的方法，假如不相近，首先考虑重新构造数据集的每一类个数，再次可以减少数据量比较大的类（减采样），并且复制数据量比较小的类（增采样）。&lt;/p&gt;

&lt;p&gt;以上是在实现卷积神经网络中使用过的和平时看文章中提及到的一些技巧，大多数现在的开源软件都是已经实现好了，直接调用使用即可。&lt;/p&gt;

&lt;p&gt;参考：[1] &lt;a href=&quot;http://cs231n.stanford.edu/&quot;&gt;http://cs231n.stanford.edu/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;a href=&quot;http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html&quot;&gt;http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&quot;&gt;http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>2015-08-04 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2015/08/04/dl5tricks.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2015/08/04/dl5tricks.html</guid>
        
        <category>project experience</category>
        
      </item>
    
      <item>
        <title>【GPU编程系列之三】cuda stream和event相关内容</title>
        <description>&lt;p&gt;本文主要理解CUDA streams和相关的概念，有的概念翻译成中文反而无法体现它的意思，因此基本上还是用英文。主要参考了《The CUDA Handbook》这本书。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;contexts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先介绍contexts，它类似于cpu中的进程，它作为一个容器，管理了所有对象的生命周期，大多数的CUDA函数调用需要context。这些对象如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;所有分配内存
Modules，类似于动态链接库，以.cubin和.ptx结尾
CUDA streams，管理执行单元的并发性
CUDA events
texture和surface引用
kernel里面使用到的本地内存（设备内存）
用于调试、分析和同步的内部资源
用于分页复制的固定缓冲区
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;cuda runtime（软件层的库）不提供API直接接入CUDA context，而是通过&lt;strong&gt;&lt;em&gt;延迟初始化&lt;/em&gt;&lt;/strong&gt;（deferred initialization）来创建context，也就是lazy initialization。具体意思是在调用每一个CUDART库函数时，它会检查当前是否有context存在，假如需要context，那么才自动创建。也就是说需要创建上面这些对象的时候就会创建context。创建的这个context的特性跟程序之间给的要求有关，例如cudaSetDevice()（线程设置当前的context，这个函数并不需要context存在），cudaSetDeviceFlags()等。此处，可以显式的控制初始化，即调用cudaFree(0)，强制的初始化。cuda runtime将context和device的概念合并了，即在一个gpu上操作可看成在一个context下。因而cuda runtime提供的函数形式类似cudaDeviceSynchronize()而不是与driver API 对应的cuCtxSynchronize()。应用可以通过驱动API来访问当前context的栈。与context相关的操作，都是以cuCtxXXXX()的形式作为driver API实现。&lt;/p&gt;

&lt;p&gt;当context被销毁，里面分配的资源也都被销毁，一个context内分配的资源其他的context不能使用。在driver API中，每一个cpu线程都有一个&lt;strong&gt;&lt;em&gt;current context&lt;/em&gt;&lt;/strong&gt;的栈，新建新的context就入栈。针对每一个线程只能有一个出栈变成可使用的current context，而这个游离的context可以转移到另一个cpu线程，通过函数cuCtxPushCurrent/cuCtxPopCurrent来实现。&lt;/p&gt;

&lt;h4 id=&quot;cuda-streams&quot;&gt;1.CUDA streams&lt;/h4&gt;

&lt;p&gt;CUDA streams用来管理执行单元的并发，在一个流中，操作是串行的按序执行的，但是在不同的流中操作就可以同时执行，从而完成并发操作。其中包括如下一些操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;管理GPU、CPU的并发
当流处理器在执行kernel时可以调用内存复制引擎同时进行内存复制
（不同？）核函数的并发
多GPU的并发
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;并发的例子如下，摘自&lt;a href=&quot;http://on-demand.gputechconf.com/gtc-express/2011/presentations/StreamsAndConcurrencyWebinar.pdf&quot;&gt;http://on-demand.gputechconf.com/gtc-express/2011/presentations/StreamsAndConcurrencyWebinar.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;代码1下面的操作就是同步的，没有异步的过程&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cudaMalloc ( &amp;amp;dev1, size ) ;
double* host1 = (double*) malloc ( &amp;amp;host1, size ) ;
…
cudaMemcpy ( dev1, host1, size, H2D ) ;
kernel2 &amp;lt;&amp;lt;&amp;lt; grid, block, 0 &amp;gt;&amp;gt;&amp;gt; ( …, dev2, … ) ;
kernel3 &amp;lt;&amp;lt;&amp;lt; grid, block, 0 &amp;gt;&amp;gt;&amp;gt; ( …, dev3, … ) ;
cudaMemcpy ( host4, dev4, size, D2H ) ;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;而代码2的操作是异步的，全并发的，在不同的四个流中完成不同的操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cudaStream_t stream1, stream2, stream3, stream4 ;
cudaStreamCreate ( &amp;amp;stream1) ;
...
cudaMalloc ( &amp;amp;dev1, size ) ;
cudaMallocHost ( &amp;amp;host1, size ) ; 
…
cudaMemcpyAsync ( dev1, host1, size, H2D, stream1 ) ;
kernel2 &amp;lt;&amp;lt;&amp;lt; grid, block, 0, stream2 &amp;gt;&amp;gt;&amp;gt; ( …, dev2, … ) ;
kernel3 &amp;lt;&amp;lt;&amp;lt; grid, block, 0, stream3 &amp;gt;&amp;gt;&amp;gt; ( …, dev3, … ) ;
cudaMemcpyAsync ( host4, dev4, size, D2H, stream4 ) ;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同步异步的过程可以参考下图，其中的数字就代表了流编号。在第一张示例中，在一个stream 0上三个操作按序执行，第二张图中，第二个时间段，stream 1的kernel执行操作就和stream 2的内存复制操作时间重叠（overlap）了，从而做到了并发。&lt;/p&gt;

&lt;p&gt;摘自&lt;a href=&quot;http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/&quot;&gt;http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://7xkmdr.com1.z0.glb.clouddn.com/cuda-stream.png&quot; alt=&quot;1&quot; height=&quot;60%&quot; width=&quot;60%&quot; hspace=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GPU上的一些操作是异步进行的，异步的意思就是gpu在它执行完任务之前就将控制全返回给主机线程，那么就能保证后面的cpu程序在执行的时候gpu的函数也在执行。也就是说在GPU上执行的一些操作和CPU上执行的函数能够异步进行。这些操作大致如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;启动核函数
在同一个设备上的内存复制
小于64KB内存从主机复制到设备
后缀带有Async的复制函数
调用内存设置函数（设置共享内存、L1缓存大小等）
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在cuda7版本之前，没有显式指定流，空流（默认流）会被隐式指定，它要同步设备上的所有操作。一个设备会产生一个空流。其它流的工作完成之后空流的工作才能开始，空流工作完成后其它流才能开始。cuda7版本增加了新的特性，可以选择每一个主机线程使用独立的空流，即一个线程一个空流，避免了原来空流的按序执行。&lt;a href=&quot;http://devblogs.nvidia.com/parallelforall/gpu-pro-tip-cuda-7-streams-simplify-concurrency/&quot;&gt;http://devblogs.nvidia.com/parallelforall/gpu-pro-tip-cuda-7-streams-simplify-concurrency/&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;//启动每个线程一个空流的方法
//方法1
nvcc --default-stream per-thread
//方法2，在include CUDA头文件之前
#define CUDA_API_PER_THREAD_DEFAULT_STREAM
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;cuda-events&quot;&gt;2.CUDA events&lt;/h4&gt;

&lt;p&gt;CUDA events可以用来控制同步，包括cpu/gpu的同步、gpu上不同engine的同步和gpu之间的同步。此外可以用来检查gpu的操作时长。它能够向CUDA stream进行记录（record），cpu会等待event记录的这个地方完成才能执行下一步。&lt;/p&gt;

&lt;p&gt;例如用来计算程序运行时间的例子，省略掉了初始化的过程。cudaEventRecord的第二个参数是cudaStream_t stream = 0 。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cudaEventRecord(start, 0);
for (int i = 0; i &amp;lt; 2; ++i) {
    cudaMemcpyAsync(inputDev + i * size, inputHost + i * size,
                 size, cudaMemcpyHostToDevice, stream[i]);
    MyKernel&amp;lt;&amp;lt;&amp;lt;100, 512, 0, stream[i]&amp;gt;&amp;gt;&amp;gt;
                  (outputDev + i * size, inputDev + i * size, size);
    cudaMemcpyAsync(outputHost + i * size, outputDev + i * size,
                 size, cudaMemcpyDeviceToHost, stream[i]);
}
cudaEventRecord(stop, 0);
cudaEventSynchronize(stop);
float elapsedTime;
cudaEventElapsedTime(&amp;amp;elapsedTime, start, stop);
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;不同的同步函数原型&lt;/th&gt;
      &lt;th&gt;函数意义&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;__host__ ​cudaError_t cudaEventSynchronize(cudaEvent_t event)&lt;/td&gt;
      &lt;td&gt;等待event完成，才执行下一段&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CUresult cuEventSynchronize(CUevent hEvent)&lt;/td&gt;
      &lt;td&gt;等待event完成，才执行下一段&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;__host__ ​ __device__ ​cudaError_t cudaDeviceSynchronize(void)&lt;/td&gt;
      &lt;td&gt;等待device上所有操作完成&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CUresult cuCtxSynchronize(void)&lt;/td&gt;
      &lt;td&gt;等待context中所有操作完成，driver API对应cudaDeviceSynchronize()&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;__host__ ​cudaError_t cudaThreadSynchronize(void)&lt;/td&gt;
      &lt;td&gt;是cudaDeviceSynchronize的一个弃用版本，意义一样但是现在不用这个了&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;__host__ ​cudaError_t cudaStreamSynchronize(cudaStream_t stream)&lt;/td&gt;
      &lt;td&gt;等待传入的流中的操作完成&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CUresult cuStreamSynchronize(CUstream hStream)&lt;/td&gt;
      &lt;td&gt;等待传入的流中的操作完成&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

</description>
        <pubDate>2015-07-22 00:00:00 +0800</pubDate>
        <link>chenrudan.github.ioblog/2015/07/22/cudastream.html</link>
        <guid isPermaLink="true">chenrudan.github.ioblog/2015/07/22/cudastream.html</guid>
        
        <category>programming languages</category>
        
      </item>
    
  </channel>
</rss>
